{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf6bf0b",
   "metadata": {},
   "source": [
    "### Recap\n",
    "- how to best describe the feature\n",
    "- can we get away with feature engineering\n",
    "- automatically learn the features - Representation Learning\n",
    "- no manual feature engineering is needed\n",
    "- Goal - efficient task-independent feature learnign for ML with graphs\n",
    "- represent mapping automatically in the form of $f:u \\rightarrow \\mathbb R ^{d}$, where u is the node\n",
    "- the vector is called __feature representation__ or __embedding__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a612e0",
   "metadata": {},
   "source": [
    "### Why embedding\n",
    "- task is to map nodes into embedding\n",
    "- there is similarity between nodes and embedding\n",
    "- if both nodes are close in the network, they must be close in the embedding as well\n",
    "- automatically encode network structure information\n",
    "- this can be used for different type of downstream tasks\n",
    "  - classify nodes\n",
    "  - predict links\n",
    "  - classify graphs\n",
    "  - anomalous node detection\n",
    "  - clustering\n",
    "- in 2014, DeepWalk paper represented social representation of karate club network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c919e",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "- how to formulate this as a task using encoder and decoder\n",
    "- represent graph as adjacency matrix\n",
    "  - do not make assumptions about the features or extra information\n",
    "  - binary\n",
    "  - simplicity assume it as undirected graph\n",
    "- goal is to encode nodes in the form of embeddings\n",
    "  - some notion of similarity in network is approximated in embedding space\n",
    "    - in the form of space\n",
    "    - the dot product of two nodes in a coordinate system approximates similarity in the embedding space\n",
    "  - goal is to define similarity in original network and to map nodes in the embedding space\n",
    "    - dot product is the angle between two vectors\n",
    "    - if two points are close together or in same direction, will have high dot product\n",
    "    - if two points are orthogonal, they will have zero dot product or represent disimilarity\n",
    "    > similarity(u,v) $ \\approx z_{v}^{T}z_{u}$\n",
    "\n",
    "<img src=\"./images/03_encoderEmbedding.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS224W-Jure Leskovec}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7696cea",
   "metadata": {},
   "source": [
    "### Learning node embeddings\n",
    "- Encoder maps from node to embedding\n",
    "- Define node similarity function \n",
    "- Decoder maps from embeddings to similarity score\n",
    "- Optimize parameters of encoder so that similarity in original network is as approximate to similarity of embedding\n",
    "  - Decoder on right side is simply the dot product\n",
    "- Encoder\n",
    "  - maps each node to low-dimensional vector\n",
    "  - ENC(v) = $z_{v}$, where z is in d-dimensional embedding\n",
    "  - d is generally between 64 and 1000\n",
    "  - d also depends on size of network and other factors\n",
    "  - Similarity function specifies the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb5d19",
   "metadata": {},
   "source": [
    "#### Shallow Encoder \n",
    "- Encoder is just an embedding lookup - so its called shallow\n",
    "- encoding of a given node is simply a vector of numbers, which is a lookup in a big matrix\n",
    "> ENC($v) = z_{v} = Z.v$\n",
    "- goal is to learn/estimate the matrix $Z \\in \\mathbb R ^{dx|v|}$\n",
    "  - matrix Z has embedding dimension d times the number of nodes\n",
    "  - in this matrix, each column is a node embedding\n",
    "  - $v$ is an indicator vector that has all zeros except one in column indicating node $v \\in \\mathbb I^{|v|}$  \n",
    "  - this method is not very scalable, you can estimate upto say million nodes\n",
    "  - for every node we have estimate d parameters\n",
    "- Methods used: DeepWalk, node2vec\n",
    "  \n",
    "<img src=\"./images/03_encoderEmbeddingMatrix.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS224W-Jure Leskovec}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cadf10",
   "metadata": {},
   "source": [
    "### Framework summary\n",
    "- Shallow encoder\n",
    "- Parameters to optimize Z\n",
    "- Deep encoders in GNN is another variation \n",
    "  - it does not use node embeddings\n",
    "- Decoder \n",
    "  - will be very simple\n",
    "  - it will be based on node similarity based on the dot product\n",
    "  - objective will be to maximize the dot product $z_{v}^{T}z_{u}$ for node pairs (u,v), which are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba8e3a",
   "metadata": {},
   "source": [
    "### How to define node similarity\n",
    "- we will define similarity based on random walks\n",
    "- then optimize embeddings for such similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41824431",
   "metadata": {},
   "source": [
    "### Node embedding\n",
    "- this method is called unsupervised/self-supervised when we learn node embeddings\n",
    "- the node labels are not utilized\n",
    "- the node features/attributes are not utilized/captured\n",
    "- the goal is only to capture some notion of network similarity, \n",
    "- the notion of labels of nodes are not needed/captured because if the nodes are human, then the features such as location, gender, age are attached to the node \n",
    "- goal is to directly estimate a set of coordinates of node, so that some aspect of network structure is preserved\n",
    "- in this sense, these embeddings are task independent because they are not trained on a given prediction task or the labeling of node or given subset of links - it is trained given the network itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda6340",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23068c8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
