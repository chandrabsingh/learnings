{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5deb3d3",
   "metadata": {},
   "source": [
    "## Node embedding\n",
    "\n",
    "### Recap\n",
    "- how to best describe the feature\n",
    "- can we get away with feature engineering\n",
    "- automatically learn the features - Representation Learning\n",
    "- no manual feature engineering is needed\n",
    "- Goal - efficient task-independent feature learnign for ML with graphs\n",
    "- represent mapping automatically in the form of $f:u \\rightarrow \\mathbb R ^{d}$, where u is the node\n",
    "- the vector is called __feature representation__ or __embedding__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e184af2",
   "metadata": {},
   "source": [
    "### Why embedding\n",
    "- task is to map nodes into embedding\n",
    "- there is similarity between nodes and embedding\n",
    "- if both nodes are close in the network, they must be close in the embedding as well\n",
    "- automatically encode network structure information\n",
    "- this can be used for different type of downstream tasks\n",
    "  - classify nodes\n",
    "  - predict links\n",
    "  - classify graphs\n",
    "  - anomalous node detection\n",
    "  - clustering\n",
    "- in 2014, DeepWalk paper represented social representation of karate club network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20ebc5",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "- how to formulate this as a task using encoder and decoder\n",
    "- represent graph as adjacency matrix\n",
    "  - do not make assumptions about the features or extra information\n",
    "  - binary\n",
    "  - simplicity assume it as undirected graph\n",
    "- goal is to encode nodes in the form of embeddings\n",
    "  - some notion of similarity in network is approximated in embedding space\n",
    "    - in the form of space\n",
    "    - the dot product of two nodes in a coordinate system approximates similarity in the embedding space\n",
    "  - goal is to define similarity in original network and to map nodes in the embedding space\n",
    "    - dot product is the angle between two vectors\n",
    "    - if two points are close together or in same direction, will have high dot product\n",
    "    - if two points are orthogonal, they will have zero dot product or represent disimilarity\n",
    "    > similarity(u,v) $ \\approx z_{v}^{T}z_{u}$\n",
    "\n",
    "<img src=\"./images/03_encoderEmbedding.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS224W-Jure Leskovec}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744570c0",
   "metadata": {},
   "source": [
    "### Learning node embeddings\n",
    "- Encoder maps from node to embedding\n",
    "- Define node similarity function \n",
    "- Decoder maps from embeddings to similarity score\n",
    "- Optimize parameters of encoder so that similarity in original network is as approximate to similarity of embedding\n",
    "  - Decoder on right side is simply the dot product\n",
    "- Encoder\n",
    "  - maps each node to low-dimensional vector\n",
    "  - ENC(v) = $z_{v}$, where z is in d-dimensional embedding\n",
    "  - d is generally between 64 and 1000\n",
    "  - d also depends on size of network and other factors\n",
    "  - Similarity function specifies the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685002a",
   "metadata": {},
   "source": [
    "#### Shallow Encoder \n",
    "- Encoder is just an embedding lookup - so its called shallow\n",
    "- encoding of a given node is simply a vector of numbers, which is a lookup in a big matrix\n",
    "> ENC($v) = z_{v} = Z.v$\n",
    "- goal is to learn/estimate the matrix $Z \\in \\mathbb R ^{dx|v|}$\n",
    "  - matrix Z has embedding dimension d times the number of nodes\n",
    "  - in this matrix, each column is a node embedding\n",
    "  - $v$ is an indicator vector that has all zeros except one in column indicating node $v \\in \\mathbb I^{|v|}$  \n",
    "  - this method is not very scalable, you can estimate upto say million nodes\n",
    "  - for every node we have estimate d parameters\n",
    "- Methods used: DeepWalk, node2vec\n",
    "  \n",
    "<img src=\"./images/03_encoderEmbeddingMatrix.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS224W-Jure Leskovec}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7848103",
   "metadata": {},
   "source": [
    "### Framework summary\n",
    "- Shallow encoder\n",
    "- Parameters to optimize Z\n",
    "- Deep encoders in GNN is another variation \n",
    "  - it does not use node embeddings\n",
    "- Decoder \n",
    "  - will be very simple\n",
    "  - it will be based on node similarity based on the dot product\n",
    "  - objective will be to maximize the dot product $z_{v}^{T}z_{u}$ for node pairs (u,v), which are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf2d13",
   "metadata": {},
   "source": [
    "### How to define node similarity\n",
    "- we will define similarity based on random walks\n",
    "- then optimize embeddings for such similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698697c",
   "metadata": {},
   "source": [
    "### Node embedding\n",
    "- this method is called unsupervised/self-supervised when we learn node embeddings\n",
    "- the node labels are not utilized\n",
    "- the node features/attributes are not utilized/captured\n",
    "- the goal is only to capture some notion of network similarity, \n",
    "- the notion of labels of nodes are not needed/captured because if the nodes are human, then the features such as location, gender, age are attached to the node \n",
    "- goal is to directly estimate a set of coordinates of node, so that some aspect of network structure is preserved\n",
    "- in this sense, these embeddings are task independent because they are not trained on a given prediction task or the labeling of node or given subset of links - it is trained given the network itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc90ee",
   "metadata": {},
   "source": [
    "## Random walk approaches for node embedding\n",
    "\n",
    "- Vector\n",
    "- probability\n",
    "- softmax function\n",
    "- sigmoid funcion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eaa690",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945c2137",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3dbce0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa31d9dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e1e0b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a900b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc611b47",
   "metadata": {},
   "source": [
    "## Embedding entire graphs\n",
    "\n",
    "- want to embed a subgraph or entire graph G\n",
    "- used for molecule classification - toxic/nontoxic\n",
    "\n",
    "### Strategy 1\n",
    "- use standard node embedding like node2vec or deepwalk on a (sub)graph G\n",
    "- then sum(average) over the node embedding in G\n",
    "- used in Duvenaud 2016 paper to classify molecules\n",
    "\n",
    "### Strategy 2\n",
    "- introduce a virtual node to represent a subgraph or entire graph and run standard graph embedding technique\n",
    "- used in Li 2016 paper\n",
    "\n",
    "### Strategy 3 - Anonymous walk\n",
    "- anonymous walks correspond to index of first time we visited the node in a random walk\n",
    "- no of anonymous walk grows exponentially\n",
    "\n",
    "<img src=\"./images/03_embeddingGraphAnonymous.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS224W-Jure Leskovec}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ee1e1",
   "metadata": {},
   "source": [
    "#### How to design anonymous walk\n",
    "- simulate anonymous walk of length l, record their count\n",
    "- represent graph as probability distribution over these walks\n",
    "- for length l = 3, it can be represented as 5-dimensional vector \n",
    "- length 4 - can be represented as 53, 5 as 203, and likewise \n",
    "- using probability $Z_{G}[i]$ of ith coordinate of embedding as the probability of anonymous walk $w_{i}$ of type i has occured in graph G\n",
    "- to embed graphs of higher dimension increase the length of walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e57f9d",
   "metadata": {},
   "source": [
    "#### How many anonymous walks do you need to sample\n",
    "- how many anonymous walks do you want to sample such that the estimate in the probability of occurence are accurate\n",
    "- to quantify accuracy, we use $\\epsilon$ to represent that we dont want error more than $\\epsilon$ \n",
    "- and $\\delta$ to represent the probability if the error\n",
    "- distribution to have error of more than $\\epsilon$ with prob less than $\\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fa6a8",
   "metadata": {},
   "source": [
    "### Hierarchical embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed37ea5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a74e3db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "debe423f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dff6803",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932d592d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "276a863b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "841f79ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2a37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d9eb55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
