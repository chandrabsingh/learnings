{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7091025",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Fei-Fei Li/Prof Justin Johnson/Prof Serena Yeung - CS231n - Stanford. This is my interpretation of their excellent teaching and I take full responsibility of any misinterpretation or misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452b818",
   "metadata": {},
   "source": [
    "### Lecture 3: Loss Function and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4c6b3",
   "metadata": {},
   "source": [
    "#### Outline:\n",
    "- Loss functions\n",
    "  - A loss function quantifies the unhappiness with the scores across the training data\n",
    "    - a function that takes in a W and tells us how bad quantitatively is that W\n",
    "    - minimize the loss on training example\n",
    "    - different types of loss \n",
    "- Optimization\n",
    "  - Come up with a way of efficient procedure to calculate W\n",
    "    - efficiently come up with the procedure of searching through the space of all possible Ws and come up with what is the correct value of W that is the least bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483f8db",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "- Given a dataset $\\{(x_{i}, y_{i})\\}_{i=1}^{N}$, where $x_{i}$ is image and $y_{i}$ is (integer) label\n",
    "- Loss over the dataset is sum of loss over examples:\n",
    "> $L(W) = \\frac{1}{N}\\sum\\limits_{i}L_{i}(f(x_{i},W),y_{i}) + \\lambda R(W)$\n",
    "  - where 1st term is the data loss\n",
    "  - 2nd term is the regularization correction - making the model simple\n",
    "- binary SVM - has 2 classes - each example will be classified as positive or negative example\n",
    "- multinomial SVM - handle multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225e8a9",
   "metadata": {},
   "source": [
    "#### Multiclass SVM loss\n",
    "- Given a dataset $\\{(x_{i}, y_{i})\\}_{i=1}^{N}$, where $x_{i}$ is image and $y_{i}$ is (integer) label\n",
    "- and scores vector $s = f(x_{i}, W)$\n",
    "  - predicted scores that are coming from the classifier\n",
    "  - $y_{i}$ is the ground truth label\n",
    "  - $s_{y_{i}}$ denotes score of the true class for the ith example in training set\n",
    "  - $s_{1}$ and $s_{2}$ will be cat and dog score respectively\n",
    "\n",
    "- SVM loss has the form - **Hinge Loss**:  \n",
    "\n",
    "> \\begin{equation}\\\\\n",
    "\\begin{aligned}\\\\\n",
    "  L_{i} &= \\sum\\limits_{j \\neq y_{i}}\n",
    "    \\begin{cases}\n",
    "      0 & \\text{if $s_{y_{i}} \\geq s_{j} + 1$}\\\\\n",
    "      s_{j} - s_{y_{i}} + 1 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\\\\       \n",
    "    &= \\sum\\limits_{j \\neq y_{i}} max(0, s_{j} - s_{y_{i}} + 1)\n",
    "\\end{aligned}\\\\\n",
    "\\end{equation}\\\\\n",
    "\n",
    "<img src=\"images/03_Ws.png\" height=400 width=400>\n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "- If the true score is high, that is good. Otherwise, we will have to incur some loss and that would be bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40217701",
   "metadata": {},
   "source": [
    "<img src=\"images/03_Ls1.png\" height=400 width=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "<img src=\"images/03_Ls2.png\" height=400 width=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "<img src=\"images/03_Ls3.png\" height=400 width=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "\n",
    "- Why +1?\n",
    "  - We care about the relative scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea46ab",
   "metadata": {},
   "source": [
    "#### Regularization - 2nd term\n",
    "> $L(W) = \\frac{1}{N}\\sum\\limits_{i}L_{i}(f(x_{i},W),y_{i}) + \\lambda R(W)$  \n",
    "> where $\\lambda$ is the regularization strength (hyperparameter)  \n",
    "\n",
    "- Types of regularization:\n",
    "  - L2 regularization - weight decay - Euclidean norm or squared norm - penalize the Euclidean norm of this weight vector\n",
    "    > $R(W) = \\sum_{k}\\sum_{l}W^{2}_{k,l}$\n",
    "  - L1 regularization - nice property of encouraging sparsity in matrix W\n",
    "    > $R(W) = \\sum_{k}\\sum_{l}|W_{k,l}|$\n",
    "  - Elastic net (L1 + L2) regularization - combination of L1 and L2\n",
    "    > $R(W) = \\sum_{k}\\sum_{l}\\beta W^{2}_{k,l} + |W_{k,l}|$\n",
    "  - Max norm regularization - penalizes the max norm rather than L1 and L2 norm\n",
    "  - Dropout regularization - specific to deep learning\n",
    "  - Fancier regularization: Batch normalization, stochastic depth  \n",
    "\n",
    "- Goal of regularization term is that it penalizes the complexity of the model rather than explicitly trying to fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625d931",
   "metadata": {},
   "source": [
    "#### Softmax Classifier (Multinomial Logistic Regression)\n",
    "\n",
    "- Multiclass SVM\n",
    "  - there was no interpretation of loss function\n",
    "  - the model f spits out scores for the classes, which didn't actually had much interpretation\n",
    "  - all we cared about was the score of correct class must be greater than score of incorrect class\n",
    "- Multinomial Logistic Regression\n",
    "  - in this case, the scores will have meaning\n",
    "  > Softmax function $P(Y=k|X=x_{i}) = \\frac{e^{s}k}{\\sum_{j}e^{s_{j}}}$  \n",
    "  > where scores $s = f(x_{i}; W)$ = unnormalized log probabilities of the classes\n",
    "  - the probability of softmax function sum to 1\n",
    "  - To maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class:\n",
    "  > $L_{i} = -$log $P(Y=y_{i}|X=x_{i}) $\n",
    "  - more weight (i.e., probability of 1) should be on the cat and 0 probability for all other classes\n",
    "  - computed probability distribution coming out of the softmax function should match this target probability distribution that has all the mass on the correct class\n",
    "    - use KL divergence\n",
    "    - maximum likelihood estimate\n",
    "  - Goal is the probability of true class is high and as close to 1\n",
    "- Loss function will be the -log of the probability of true class\n",
    "  > $L_{i} = -$log $\\frac{e^{s_{y_{i}}}}{\\sum_{j}e^{s_{j}}} $\n",
    "- Calculation steps:\n",
    "  - Calculate unnormalized log probabilities, as above\n",
    "  - calculate exponent of it(unnormalized probabilities)\n",
    "  - calculate normalized value (probabilities)\n",
    "  - calculate negative log (softmax loss function value) (or multinomial logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fabe5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe904a7",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "- find bottom of the valley\n",
    "- use types of iterative method\n",
    "- types\n",
    "  - random search\n",
    "    - depends on luck\n",
    "  - follow the slope. \n",
    "    - use local geometry, which way will take me little bit down\n",
    "    - gradient is the vector of (partial derivatives) along each dimension\n",
    "    - slope in any direction is the dot product of the direction with the gradient\n",
    "    - direction of steepest descent is negative gradient\n",
    "    - use finite differences\n",
    "      - adv\n",
    "        - easy to write\n",
    "      - disadv\n",
    "        - approximate\n",
    "        - can be very slow if size is large\n",
    "        - in practice, it is never used\n",
    "    - instead compute analytic gradient\n",
    "      - calculate dW in one step instead of looping over iteratively\n",
    "        - adv\n",
    "          - exact, fast\n",
    "        - disadv\n",
    "          - error prone\n",
    "    - in practice, always use analytic gradient, but check implementation with numerical gradient - gradient check\n",
    "    \n",
    "<img src = \"images/03_finiteMethod1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "<img src = \"images/03_finiteMethod2.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS231n-Justin Johnson}}$   \n",
    "\n",
    "  - gradient descent\n",
    "    - most used\n",
    "    - initialize W random \n",
    "    - compute loss and gradient\n",
    "    - update the weights in opposite of the gradient direction\n",
    "      - gradient points to the direction of greatest increase\n",
    "      - minus gradient points in the direction of greatest decrease\n",
    "      - take small step in the direction of minus gradient\n",
    "      - repeat till it converges\n",
    "    - step size or learning rate is a hyperparameter\n",
    "      - tells us how far we step in the direction of gradient\n",
    "      - step size is the first hyperparameter we check\n",
    "      - model size and regularization can be dealt later, but step size should be the primary focus\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b81e6",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "> $L(W) = \\frac{1}{N}\\sum\\limits_{i}L_{i}(f(x_{i},y_{i},W) + \\lambda R(W)$  \n",
    "> $\\nabla_{W}L(W) = \\frac{1}{N}\\sum\\limits_{i}\\nabla_{W} L_{i}(f(x_{i},y_{i},W) + \\lambda\\nabla_{W} R(W)$\n",
    "- Vanilla Minibatch Gradient Descent\n",
    "  - minibatch of size 32/64/128\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0363d97",
   "metadata": {},
   "source": [
    "#### Image features\n",
    "- Instead of feeding raw pixels into linear classifiers doesnot work too well\n",
    "- Prior to deep neural network popularity, two stage approach was used\n",
    "- first, take your image, compute various feature representations\n",
    "- then concatenate these feature vectors to give some feature representation of image\n",
    "- trick is to use right feature transform for the problem statement\n",
    "- example\n",
    "  - color histogram\n",
    "    - count how many pixels fall into each bucket\n",
    "    - tells us what type of color exist in image\n",
    "  - histogram of oriented gradients (HoG)\n",
    "    - dominant edge direction of each pixel\n",
    "    - compute histogram over these different edge orientation in bucket \n",
    "    - tells us what type of edge exist in image\n",
    "    - was used for object recognition in the past\n",
    "  - bag of words (comes from NLP)\n",
    "    - in NLP, number of words in a paragraph are counted\n",
    "    - apply same concept in images\n",
    "    - no straightforward analogy of words and images\n",
    "    - create your own version of vocabulary of visual words\n",
    "    - get sample \n",
    "    \n",
    "- ConvNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06da64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
