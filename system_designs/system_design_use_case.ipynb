{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19451697",
   "metadata": {},
   "source": [
    "### Segment - Monolithic to Microservices to Monolithic Architecture\n",
    "\n",
    "**Core services**\n",
    "- it delivers messages to third-party endpoints\n",
    "- retries messages upon failure\n",
    "- archives any undelivered messages\n",
    "\n",
    "**why not just use a queue here? Building a fully distributed job scheduler seems a bit overdone**\n",
    "- At segment, Kafka was already used extensively \n",
    "- it passes nearly 1M messages per second through it\n",
    "- it’s been the core building block of all of our streaming pipelines\n",
    "\n",
    "**When does a queue break down**\n",
    "- The problem with using any sort of queue is that you are fundamentally limited in terms of how you access data. \n",
    "- a queue only supports two operations (push and pop)\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/i8bfc4nr05rq/526JE4CsskIje2tIoBxmbr/c8108e96a2f99d5d8f1499e08a43e635/asset_sJlQSUXb.png\" width=600 height=400>\n",
    "     \n",
    "**a single queue architecture**\n",
    "<img src=\"https://images.ctfassets.net/i8bfc4nr05rq/1TMg75tMuAumjmYHu5UvDP/a59d8315041ceb7a48da16c6cdfb16d9/asset_VdRJbhUe.png\" width=600 height=400>\n",
    "\n",
    "\n",
    "**What happens when single endpoint gets slow**\n",
    "<img src=\"https://images.ctfassets.net/i8bfc4nr05rq/3wQyQCVvbIwDOS3roQmaUv/ec97341d39dabd25098e627f47028cbc/asset_QVaGLpel.png\" width=600 height=400>\n",
    "\n",
    "\n",
    "**queues per destination architecture**\n",
    "- updated queueing topology to route events into separate queues based upon the downstream endpoints they would hit\n",
    "- router was added in front of each queue, which would only publish messages to a queue destined for a specific API endpoint\n",
    "- segment is a large, multi-tenant system, so some sources of data will generate substantially more load than others\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/i8bfc4nr05rq/3mwwfjAAWWVLMmqZhKcEEH/97e2569dfc3ca1fedc9f5da233ce94f3/asset_DLWTCwfw.png\" width=600 height=400>\n",
    "\n",
    "\n",
    "- https://segment.com/docs/guides/\n",
    "- https://segment.com/blog/goodbye-microservices/\n",
    "- https://segment.com/blog/introducing-centrifuge/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac467ad",
   "metadata": {},
   "source": [
    "### Istio\n",
    "https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8dbca",
   "metadata": {},
   "source": [
    "### LinkedIn\n",
    "\n",
    "#### Starting days\n",
    "\n",
    "https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin\n",
    "\n",
    "Leo - single monolithic application doing it all. That single app was called Leo. It hosted web servlets for all the various pages, handled business logic, and connected to a handful of LinkedIn databases\n",
    "\n",
    "Cloud - manage member to member connections. We needed a system that queried connection data using graph traversals and lived in-memory for top efficiency and performance. to scale independently of Leo, so a separate system for our member graph called Cloud was born - LinkedIn’s first service\n",
    "\n",
    "Lucene - member graph service started feeding data into a new search service running Lucene.\n",
    "\n",
    "![Master Slave Model]()\n",
    "<img src=\"https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/arch_master_slave_0.png\" width=600 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45306c67",
   "metadata": {},
   "source": [
    "### How do Pokemon Go scale to millions of request\n",
    "- https://www.scaleyourapp.com/distributed-systems-scalability-part1-heroku-client-rate-throttling/\n",
    "- https://cloud.google.com/blog/topics/developers-practitioners/how-pok%C3%A9mon-go-scales-millions-requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b2a00",
   "metadata": {},
   "source": [
    "### How does Uber scale to millions of concurrent requests\n",
    "\n",
    "- Fulfillment platform\n",
    "  - goes through series of checks and balances and then triggers fulfillment to create new order for consumer\n",
    "  - order is the intent of consumer\n",
    "  - intent is then translated into jobs that needs to be processed\n",
    "  - this information is stored in spanner\n",
    "  - these jobs are read by matching engine\n",
    "  - offered to any open provider or supplier nearby\n",
    "  - earlier all of this data was saved in **on-prem database** and recently moved to **Spanner**\n",
    "  \n",
    "<img src=\"./images/uberRequestFlow.png\">\n",
    "\n",
    "- Challenges with on-prem database\n",
    "  - No-SQL storage engine **Cassandra** was used to save all real time fulfillment entities\n",
    "  - to maintain serializability on top of Cassandra, **Ringpop** was used to provide individual **entity-level serialization**\n",
    "  - Challenges with NoSQL\n",
    "    - Cassandra follows the principle of **eventual consistency**\n",
    "    - there is **no guarantee of low-latency quorum writes with Cassandra**\n",
    "    - witnessed complex storage interactions that required multi-row and multi-table writes.\n",
    "    - to tackle this, an application layer framework was build to orchestrate this operation using saga pattern\n",
    "    - these inconsistencies had to be manually corrected\n",
    "    - in real world - two different drivers are dispatched to a customer\n",
    "  - Horizontal scaling bottlenecks were observed in overall architecture\n",
    "    - this was primarily because how application layer was sharded with Ringpop\n",
    "    - properties of traditional SQL-based storage ACID guarantees were observed\n",
    "    - consistency was the primary requirement\n",
    "    - NoSQL requirements with strong ACID guarantees were needed\n",
    "- Google Spanner was the solution\n",
    "  - what were the explorations for transitioning from NoSQL to NewSQL\n",
    "    - Consistency with high resilency and availability were the requirement criterias\n",
    "    - CockroachDB, FoundationDB and Cloud Spanner were considered\n",
    "    - functional requirements were fulfilled\n",
    "      - scaled horizontally with our benchmarks, and provided a managed solution for cluster management and maintenance\n",
    "    - what is the new architecture\n",
    "      - every user request results in a transaction against one or more rows and across one or more tables in cloud Spanner\n",
    "      - consistent view of data is available both to internal and external customers\n",
    "\n",
    "<img src=\"./images/uberNewFulfillmentArchitecture.png\">\n",
    "\n",
    "- latency is the biggest factor, what is the networking architecture looks like \n",
    "  - resilient and highly reliable infrastructure to support Uber's workload\n",
    "  - networking infrastructure was divided into 2 major components\n",
    "    - physical layer consisting of interconnections between Uber and Cloud vendors\n",
    "    - logical layer consisting of virtual connections on top of physical layer to achieve redundancy\n",
    "      - this is achieved by making multiple routers and having local access points at each physical network route\n",
    "      - leverage Google's private Google access to route using cloud interconnect VLAN attachments\n",
    "      - reduced the need of routing traffic through public internet and provided additional reliability\n",
    "      - benchmarking tests were designed     \n",
    "  \n",
    "<img src=\"./images/uberLowLatencyNetwork.png\">\n",
    "\n",
    "- how was the migration and transition made seemless\n",
    "  - database topology was significant different from old platform, any kind of live migration of data was ruled out\n",
    "  - most of the data is ephemeral and changing continuously, backups would have resulted in loss of data\n",
    "  - systems were built that will intercept request from user session and attach to the new user session \n",
    "  - till the past order is not complete, this migration is not done\n",
    "  - only open user session with no active orders were migrated to new architecture\n",
    "  - tooling were tested rigorously in testing, staging and shadow environments\n",
    "  - test cities were developed to check migrations with one city at a time\n",
    "  - continuously migrate over 6 month period\n",
    "\n",
    "  \n",
    "- Performance Optimization\n",
    "  - networking stack\n",
    "    - TCP Reset\n",
    "      - the spanner session needs to be re-established for the forwarded requests\n",
    "      - by optimizing gRPC channel pool whenever a channel is affected by TCP resets, requests are automatically forwarded temporarily to backup empty gRPC channel\n",
    "    - Intermittent packet loss\n",
    "      - with frequent health checks, broken TCP connections can be detected within 5 sec and graceful connections can be triggered\n",
    "  \n",
    "- To take advantage of cloud Spanner elasticity, autoscaler was built\n",
    "  - constantly tunes the number of nodes based on CPU utilization target\n",
    "  - as traffic is variable based on constant CPU utilization target, maximum elasticity is achieved from Cloud spanner cluster \n",
    "  \n",
    "<img src=\"./images/uberAutoscaler.png\">\n",
    "\n",
    "- On-prem cache was made to handle workload which is very read-heavy\n",
    "  - improves latency and cost\n",
    "  - only route stale reads to Spanner \n",
    "  - allow cache to serve snapshot isolation based on Spanner's queue time\n",
    "\n",
    "<img src=\"./images/uberOnpremcache.png\">\n",
    "\n",
    "\n",
    "\n",
    "- READ:\n",
    "  - \"How Ringpop from Uber Engineering helps distribute your application\"\n",
    "  - Saga pattern - in microservices architecture it tracks all events of distributed transactions as a sequence and decides the rollbacks events in case of a failure\n",
    "  - Planet scale\n",
    "  - Strong read and Stale read\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8d058",
   "metadata": {},
   "source": [
    "### What database does Facebook use\n",
    "- MySQL is the primary database used for storing all social data\n",
    "- Memcache is used in addition to MySQL as caching layer\n",
    "- Cassandra for search\n",
    "\n",
    "\n",
    "\n",
    "To manage BigData Facebook leverages Apache Hadoop, HBase, Hive, Apache Thrift and PrestoDB. All these are used for data ingestion, warehousing and running analytics.\n",
    "\n",
    "Apache Cassandra is used for the inbox search\n",
    "\n",
    "Beringei & Gorilla, high-performance time-series storage engines are used for infrastructure monitoring. \n",
    "\n",
    "LogDevice, a distributed data store is used for storing logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7109da",
   "metadata": {},
   "source": [
    "### How is Git different from any other source systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df67f83",
   "metadata": {},
   "source": [
    "### How does Google Ad business model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c86df",
   "metadata": {},
   "source": [
    "### How to create a microservice architecture with Google Cloud - Nylas\n",
    "\n",
    "- Workflow Automation \n",
    "  - by collecting data from multiple applications \n",
    "  - compiling insights\n",
    "  - create end-to-end automation workflow\n",
    "\n",
    "- handle communication data driving business process automation\n",
    "\n",
    "- Problems to tackle \n",
    "  - security\n",
    "  - scalability\n",
    "  - performance\n",
    "  - cost\n",
    "  \n",
    "- Why Google Cloud\n",
    "  - best distributed technology\n",
    "  - GKE - Google Kubernetes Engine is the best cloud provider to run Kubernetes workload securely at scale \n",
    "  - one of the most performant data stores in Spanner\n",
    "  \n",
    "- Architecture  \n",
    "  - the advantage of using **GKE** is its ability to **handle the container orchestration**, **Cloud Pub/Sub for message bus** and **Cloud Spanner for relational data store**   \n",
    "  - Spanner processes 1B requests per second at peak\n",
    "  - latency of less than 7 milliseconds\n",
    "  - GCP services are very robust\n",
    "\n",
    "<img src=\"./images/nylasOldArchitecture.png\">\n",
    "\n",
    "- Transition \n",
    "  - from Python based application to Go - code rewrite\n",
    "    - services were rewritten in Go \n",
    "    - 10x throughput improvement was observed\n",
    "  - move from virtual machine instances on AWS EC2 to GKE \n",
    "    - move the orchestration from **AWS autoscaler group to GKE**\n",
    "    - GKE was used to orchestrate containers \n",
    "    - due to security reasons, nodes have to be cycled very quickly\n",
    "    - were able to have upto 15000 nodes in Kubernetes cluster \n",
    "    - other cloud providers dont provide this level of support\n",
    "    - **gVisor** is used to run containers to create strong isolation between application and operating system\n",
    "    - helps lock down host memory and storage access and enforce least permission principle at operating system level\n",
    "    - **gVisor is an application kernel**, written in Go, that implements substantial portion of Linux system call interface. It provides an additional layer of isolation between running applications and host operating system    \n",
    "    \n",
    "- 20TB of data processing in MySQL shards\n",
    "  - Cloud spanner was used to keep application states \n",
    "  - store keys for each accounts that needs to be connected\n",
    "  - fast read/write is extremely critical for large number of calls\n",
    "  - high performance to have predictable end-to-end latency\n",
    "\n",
    "<img src=\"./images/nylasNewArchitecture.png\">\n",
    "\n",
    "- PII(Personally Identifiable Information)\n",
    "  - Nylas does not have to hold PII\n",
    "  - for security services\n",
    "  \n",
    "- Migrated from database centric design to event based design\n",
    "  - message bus is the heart of all application\n",
    "  - to make this transition, Google Pub/Sub was choosen\n",
    "  - Google Pub/Sub allows services to communicate asynchronously with latencies on the order of 100 milliseconds\n",
    "  - in legacy AWS architecture which was more of data driven architecture\n",
    "    - the data is written into **Dynamo**\n",
    "    - then gets converted by **lambda function**\n",
    "    - then goes to **Kinesis event stream**\n",
    "  - in new architecture, this is simplified\n",
    "    - event it first\n",
    "    - rewrote it directly on Google Pub/Sub\n",
    "    - puts lot of strain on system, with trust that this will get scale and perform reliably and consistently\n",
    "    - on **Google Pub/Sub, the performance latency is minimal**\n",
    "    - event bus can act as data store\n",
    "    \n",
    "- Benefits of migrating\n",
    "  - **30 times performance on each node** was observed\n",
    "  - **Elasticity of architecture** is amazing\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5c6b1",
   "metadata": {},
   "source": [
    "### How to architect an AI/ML powered healthcare platform on Google Cloud - Vida Health\n",
    "- provides comprehensive virtual care solution tailored to personalized health goals and preferences\n",
    "- what are the challenges in building such application\n",
    "  - data coming from different sources of different types \n",
    "  - needs to be integrated seemlessly\n",
    "  - both for mobile and web users\n",
    "  \n",
    "- Why Google cloud\n",
    "  - 50% cost reduction\n",
    "  - developer productivity, one single place to look at\n",
    "  - focus on AI/ML\n",
    "  \n",
    "- Challenges with ML and AI\n",
    "  - make provider more efficient at their jobs\n",
    "  - use NLP and speech to text - take down notes for doctors/assistants\n",
    "  - how to get more patients\n",
    "    - depending on severity and acuity of patients \n",
    "    - give signals to providers\n",
    "    - prioritize such cases \n",
    "    \n",
    "- Vidapedia - major recommendation platform - using Workspace and BigQuery. How does it work?\n",
    "  - Vidapedia \n",
    "    - set of protocols, as Google docs, written to help providers/clinicians \n",
    "    - 200 different protocols are created, not feasible to remember/memorize\n",
    "    - how to get such information at high speed\n",
    "  - Vidapedia recommendation system does on machine learning\n",
    "    - look into patient-provider interaction going on\n",
    "    - surface right protocol for such interaction\n",
    "    - ML model detects and triggers particular protocol during or after the session\n",
    "    - provider is able to give right care to patient\n",
    "  - this was possible using Google Doc protocols interaction with Google Workspace \n",
    "    - use this data, index them\n",
    "    - store it in BigQuery\n",
    "    - build ML tools on top of that\n",
    "    \n",
    "- Data pipeline architecture\n",
    "  - data comes from different sources\n",
    "  - providers provide consultation notes\n",
    "  - derive insights \n",
    "  - data gets ingested both in relational and unstructured format\n",
    "  - all these data is saved in BigQuery\n",
    "  - which is then used in analytics application deriving insights\n",
    "  - visualization tools such as data studio and looker\n",
    "  - ml models are built which is then sourced into visualization tools\n",
    "  - exploration in terms of VertexAI for quickly building these models and deploying them\n",
    "  \n",
    "<img src=\"./images/vidaDataAnalyticsArchitecture.png\">\n",
    "\n",
    "- Transformations\n",
    "  - in past custom ETL pipelines were designed, which is a challenge\n",
    "  - currently **dbt** is used for transformations\n",
    "  \n",
    "- how is the orchestration done\n",
    "  - **Cloud Composer** is used for orchestration the pipeline, scheduling all of this ingest, processing data, and running all the stuff\n",
    "\n",
    "- how is security requirement resolved\n",
    "  - patient data is very sensitive\n",
    "  - stringent privacy laws\n",
    "  - use **Google Data Loss Protection(DLP)**\n",
    "  - **mask PII data**\n",
    "  - reduce surface area\n",
    "  \n",
    "- what is the Web Architecture designed\n",
    "  - SADA practices - well known in the industry\n",
    "  - standard 3-tier architecture\n",
    "  - load balancers at the front\n",
    "  - deployed in Google Kubernetes engine\n",
    "  - middle API layer that returns all the request response\n",
    "  - at the end there is GCS Postgres database, scaling is easy \n",
    "  - easy to scale\n",
    "  \n",
    "<img src=\"./images/vidaWebArchitecture.png\">\n",
    "\n",
    "- What next\n",
    "  - Google Cloud Healthcare API\n",
    "    - provides managed solution for storing and accessing healthcare data in Google cloud\n",
    "  - Google FHIR - Fast Healthcare Interoperability Resources\n",
    "    - Google fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065bb26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f258b572",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c236dfa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21fc157c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee3c7d57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22ddeb7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232665aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9236de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b49925",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97225e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b196905",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d2fa42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aacf489e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
