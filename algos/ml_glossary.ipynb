{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f603013",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "\n",
    "#### Supervised Learning\n",
    "Supervised Learning is a machine learning approach that uses labeled data to train algorithms into classifying or predicting outcomes accurately.\n",
    "> - Types of Supervised Learning algorithms\n",
    "\n",
    "#### Unsupervised Learning\n",
    "Unsupervised Learning is a machine learning approach that uses unlabeled data to analyze and cluster datasets.\n",
    "> - Types of Unsupervised Learning algorithms\n",
    "\n",
    "#### Reinforcement Learning\n",
    "Reinforcement learning is a machine learning training method based on rewarding desired behaviors and punishing the undesirable ones, thereby learning about the environment by trial and error\n",
    "> - Types of Reinforcement Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd462e06",
   "metadata": {},
   "source": [
    "### Classification algorithms\n",
    "Classification algorithms use input training data to predict the likelihood that subsequent data will fall into one of predetermined categories\n",
    "> - Types of Classification algorithms \n",
    "\n",
    "#### Probabilistic \n",
    "Probabilistic classification models classify, given an observation of an input, a probability distribution over a set of classes \n",
    "> - Types of Probabilistic Classification algorithms\n",
    ">   - Naive Bayes\n",
    ">   - Logistic regression \n",
    ">   - Multilayer perceptrons \n",
    "\n",
    "<img src = \"https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-15.png\">\n",
    "$\\tiny{\\text{www.ismiletechnologies.com}}$   \n",
    "\n",
    "##### Naive Bayes\n",
    "Bayesian classification helps us find the probability of a label given some observed features, using Bayes theorem, which describes the relationship of conditional probabilities of statistical quantities\n",
    "> - READ: https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "> - Types of Bayesian Classification\n",
    ">   - Multinomial Naïve Bayes Classifier\n",
    ">   - Bernoulli Naïve Bayes Classifier\n",
    ">   - Gaussian Naïve Bayes Classifier\n",
    "\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png\">\n",
    "$\\tiny{\\text{www.analyticsvidhya.com}}$   \n",
    "\n",
    "#### Rule based\n",
    "Rule based classification helps in classifying datasets by using a collection of \"if.. else..\" rules. The classifier may contain mutually exclusive rules, exhaustive rules, not mutually exclusive rules, or not exhaustive rules\n",
    "> - READ: http://jcsites.juniata.edu/faculty/rhodes/ml/rulebasedClass.htm\n",
    "\n",
    "\n",
    "##### Decision Tree\n",
    "A decision tree is a data mining/machine learning method of predicting/classifying the value of a target variable based on several input variables. In this classification tree, each internal node is labeled with an input feature and each leaf of the tree is labeled with a class or a probability distribution over the classes of either class or into a particular probability distribution\n",
    "\n",
    "> - Decision Tree Types\n",
    ">   - CART - Classification and Regression Trees\n",
    ">   - Ensemble methods, construct more than one decision tree\n",
    ">     - Boosted trees \n",
    ">     - Bootstrap aggregated (or bagged/bagging) decision trees\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*ZkQXt7mqI7MXuXhHrfvgtQ.png\" width=400 height=400> \n",
    "$\\tiny{\\text{miro.medium.com}}$   \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png\">\n",
    "$\\tiny{\\text{Wikipedia}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513e678",
   "metadata": {},
   "source": [
    "### Most common\n",
    "\n",
    "#### Linear Regression\n",
    "Linear regression helps us model the relationship between two variables by fitting a linear equation to observed data. The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values.\n",
    "\n",
    "We assume here that $y|x; \\theta \\sim \\mathbb N(\\mu, \\sigma^{2})$\n",
    "\n",
    "The closed form solution for the $\\theta$ that minimizes the cost function is\n",
    "> $$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "#### Logistic Regression\n",
    "The logistic regression is used to model the relationship between a set of independent and dependent variables. The dependent variables are categorical in nature, which is predicted based on the probabilities given some characteristics of class variables. The logistic regression uses sigmoid function to assign class labels.\n",
    "\n",
    "The logit function is defined as the logarithm of the log odds\n",
    "$$ \\text{logit(p)} = \\ln\\frac{p}{1-p}$$\n",
    "\n",
    "A standard logistic sigmoid function is defined as the \n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "The linear part of the model predicts the log-odds of the dataset example in the form of probability using logistic sigmoid function.\n",
    "\n",
    "It tries to learn a function that approximates P(Y|X), by assuming that P(Y|X) can be approximated as a sigmoid\n",
    "function when applied to the linear combination of input features.  \n",
    "$$ P(Y = 1|X = x) = \\sigma(z) = \\sigma(\\theta^{T}x) $$,\n",
    "where $z = \\theta_{0} + \\sum\\limits_{i=1}^{m}\\theta_{i}x_{i} $\n",
    "\n",
    "Similarly,\n",
    "$$ P(Y = 0|X = x) = 1 - \\sigma(\\theta^{T}x) $$\n",
    "\n",
    "The gradient descent is calculated as the partial derivative of logistic cost function wrt weight, which is used to maximize the logistic cost function.\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Tree\n",
    "<a href=\"#Decision-Tree\">Link</a>\n",
    "\n",
    "\n",
    "#### SVM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### kNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### K-Means\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Dimensionality Reduction Algorithms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Gradient Boosting algorithms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### GBM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### XGBoost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### LightGBM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### CatBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351c20e",
   "metadata": {},
   "source": [
    "[Microsoft - ML Reference](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/component-reference)\n",
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet.png#lightbox\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3692df",
   "metadata": {},
   "source": [
    "### Regression - Predict a value\n",
    "#### Boosted Decision Tree Regression\n",
    "#### Decision Forest Regression\n",
    "#### Fast Forest Quantile Regression\n",
    "#### Linear Regression\n",
    "#### Local Linear Regression\n",
    "\n",
    "#### Locally Weighted Linear Regression (LWR)\n",
    "Any parametric model can be made local if the fitting method accommodates observation weights. This is a variant of linear regression where the weights of each training example in the cost function is defined as \n",
    "> $$ w^{(i)}(x) = \\exp \\left( -\\frac{(x^{(i)} - x)^{2}}{2\\tau^{2}} \\right) $$\n",
    "\n",
    "#### Neural Network Regression\n",
    "#### Poisson Regression\n",
    "#### Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b860e4",
   "metadata": {},
   "source": [
    "### Classification - Predict a class. Choose from binary (two-class) or multiclass algorithms.\t\n",
    "#### Multiclass Boosted Decision Tree\n",
    "#### Multiclass Decision Forest\n",
    "\n",
    "#### Multiclass Logistic Regression (Softmax Regression)\n",
    "Softmax Regression is a generalization of logistic regression where we want to handle multiple classes instead of two classes $(y^{(i)} \\in \\{0,1\\})$\n",
    "\n",
    "#### Multiclass Neural Network\n",
    "#### One vs. All Multiclass\n",
    "#### One vs. One Multiclass\n",
    "#### Two-Class Averaged Perceptron\n",
    "#### Two-Class Boosted Decision Tree\n",
    "#### Two-Class Decision Forest\n",
    "\n",
    "#### Two-Class Logistic Regression\n",
    "\n",
    "<a href=\"#Logistic-Regression\">Logistic Regression</a>\n",
    "\n",
    "The hypothesis looks like \n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^{T}x}}$$ \n",
    "and the model parameters $\\theta$ are trained to minimize the cost function\n",
    "$$ J(\\theta) = - \\left [ \\sum\\limits_{i=0}^{n}y^{(i)}\\log\\sigma(\\theta^{T}x^{(i)}) + (1-y^{(i)}) \\log[1-\\sigma(\\theta^{T}x^{(i)})] \\right]  $$ \n",
    "$$ = -\\left [ \\sum\\limits_{i=0}^{n}y^{(i)}\\log h_{\\theta}(x^{(i)}) + (1-y^{(i)}) \\log[1- h_{\\theta}(x^{(i)})) ]\\right] $$ \n",
    "\n",
    "\n",
    "#### Two-Class Neural Network\n",
    "#### Two Class Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821c1a0",
   "metadata": {},
   "source": [
    "### Supervised learning (Classification, Regression)\n",
    "#### Decision trees\n",
    "#### Ensembles\n",
    "##### Bagging\n",
    "##### Boosting\n",
    "##### Random forest\n",
    "#### k-NN\n",
    "#### Linear regression\n",
    "#### Naive Bayes\n",
    "#### Artificial neural networks\n",
    "#### Logistic regression\n",
    "#### Perceptron\n",
    "#### Relevance vector machine (RVM)\n",
    "#### Support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c5db2",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "#### BIRCH\n",
    "#### CURE\n",
    "#### Hierarchical\n",
    "#### k-means\n",
    "#### Expectation–maximization (EM)\n",
    "#### DBSCAN\n",
    "#### OPTICS\n",
    "#### Mean shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97d7a8",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "#### Factor analysis\n",
    "#### CCA\n",
    "#### ICA\n",
    "#### LDA\n",
    "#### NMF\n",
    "#### PCA\n",
    "#### PGD\n",
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cf3f0",
   "metadata": {},
   "source": [
    "### Structured Prediction - Graphical models \n",
    "#### Bayes net\n",
    "#### Conditional random field\n",
    "#### Hidden Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8f86a",
   "metadata": {},
   "source": [
    "### Anomaly detection\n",
    "#### k-NN\n",
    "#### Local outlier factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754017cb",
   "metadata": {},
   "source": [
    "### Artificial neural network\n",
    "#### Autoencoder\n",
    "#### Cognitive computing\n",
    "#### Deep learning\n",
    "#### DeepDream\n",
    "#### Multilayer perceptron\n",
    "#### RNN \n",
    "##### LSTM\n",
    "##### GRU\n",
    "##### ESN\n",
    "#### Restricted Boltzmann machine\n",
    "#### GAN\n",
    "#### SOM\n",
    "#### Convolutional neural network \n",
    "##### U-Net\n",
    "#### Transformer Vision\n",
    "#### Spiking neural network\n",
    "#### Memtransistor\n",
    "#### Electrochemical RAM (ECRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324d61c",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "#### Q-learning\n",
    "#### SARSA\n",
    "#### Temporal difference (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75079a82",
   "metadata": {},
   "source": [
    "### Machine Learning Theory\n",
    "#### Kernel machines\n",
    "#### Bias–variance tradeoff\n",
    "#### Computational learning theory\n",
    "#### Empirical risk minimization\n",
    "#### Occam learning\n",
    "#### PAC learning\n",
    "#### Statistical learning\n",
    "#### VC theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069766eb",
   "metadata": {},
   "source": [
    "### Recommender System - Methods and challenges\n",
    "#### Cold start\n",
    "#### Collaborative filtering\n",
    "#### Dimensionality reduction\n",
    "#### Implicit data collection\n",
    "#### Item-item collaborative filtering\n",
    "#### Matrix factorization\n",
    "#### Preference elicitation\n",
    "#### Similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63677e4",
   "metadata": {},
   "source": [
    "### 10 most popular deep learning algorithms\n",
    "[Link](https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-algorithm)\n",
    "\n",
    "#### Convolutional Neural Networks (CNNs)\n",
    "#### Long Short Term Memory Networks (LSTMs)\n",
    "#### Recurrent Neural Networks (RNNs)\n",
    "#### Generative Adversarial Networks (GANs)\n",
    "#### Radial Basis Function Networks (RBFNs)\n",
    "#### Multilayer Perceptrons (MLPs)\n",
    "#### Self Organizing Maps (SOMs)\n",
    "#### Deep Belief Networks (DBNs)\n",
    "#### Restricted Boltzmann Machines( RBMs)\n",
    "#### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1aad9",
   "metadata": {},
   "source": [
    "### Optimization algorithms - Deep Learning\n",
    "- [Optimizing GD](https://ruder.io/optimizing-gradient-descent/)\n",
    "- [Optimizer Visualization](https://github.com/Jaewan-Yun/optimizer-visualization)\n",
    "\n",
    "#### ASGD\n",
    "#### Adadelta\n",
    "#### Adagrad\n",
    "#### Adam\n",
    "#### AdamW\n",
    "#### Adamax\n",
    "#### LBFGS\n",
    "#### NAdam\n",
    "#### RAdam\n",
    "#### RMSprop\n",
    "#### Rprop\n",
    "#### SGD\n",
    "#### SparseAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc2f61",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "#### Confusion Matrix\n",
    "<img src=\"https://miro.medium.com/max/1400/1*-kFYBD6v7wv4qABsVtx9JA.png\" width=300 height=300>\n",
    "\n",
    "#### Log Loss or Cross Entropy Loss\n",
    "<img src=\"https://miro.medium.com/max/1400/1*s5AmzAfKxh06ymdw1zkNkA.png\" width=300 height=300>\n",
    "\n",
    "#### Area under the curve (AUC)\n",
    "<img src=\"https://www.ismiletechnologies.com/wp-content/uploads/2021/10/image-9.png\" width=300 height=300>\n",
    "\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faa48f",
   "metadata": {},
   "source": [
    "### Generative Models\n",
    "\n",
    "### Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e087bae",
   "metadata": {},
   "source": [
    "### Types of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e87fb",
   "metadata": {},
   "source": [
    "### Estimation methods\n",
    "\n",
    "[Estimation Techniques](https://towardsdatascience.com/essential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0)\n",
    "\n",
    "#### Maximum Likelihood (ML) Estimation\n",
    "\n",
    "\n",
    "#### Maximum a Posteriori (MAP) Estimation\n",
    "\n",
    "\n",
    "#### Minimum Mean Square Error (MMSE) Estimation\n",
    "\n",
    "\n",
    "#### Least Square (LS) Estimation\n",
    "The best fit estimation line in least-square minimizes the sum of squared residuals\n",
    "- it has a closed form solution\n",
    "\n",
    "#### Linear Least squares\n",
    "> Types of Linear least squares formulation:\n",
    "> - Ordinary Least squares (OLS)\n",
    ">   - The OLS method minimizes the sum of squared residuals, and leads to a closed-form expression for the estimated value of the unknown parameter vector\n",
    ">   - it has the assumption that the error terms have finite variance and are homoscedastic\n",
    "> - Weighted Least squares (WLS)\n",
    ">   - it has the assumption that the error terms are heteroscedasticity\n",
    "> - Generalized Least squares (GLS)\n",
    ">   - it has the assumption that the error terms are either heteroscedasticity or correlations or both are present among the error terms of the model\n",
    "\n",
    "#### Non-linear\n",
    "The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\n",
    "\n",
    "#### Ordinary\n",
    "\n",
    "#### Weighted\n",
    "\n",
    "#### Generalized\n",
    "\n",
    "#### Partial\n",
    "\n",
    "#### Total\n",
    "\n",
    "#### Non-negative\n",
    "\n",
    "#### Ridge regression\n",
    "\n",
    "#### Regularized\n",
    "\n",
    "#### Least absolute deviations\n",
    "\n",
    "#### Iteratively reweighted\n",
    "\n",
    "#### Bayesian\n",
    "\n",
    "#### Bayesian multivariate\n",
    "\n",
    "#### Bayes Estimator\n",
    "\n",
    "#### Probit\n",
    "\n",
    "#### Logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aee0be",
   "metadata": {},
   "source": [
    "### Frequentists vs Bayesian Approach\n",
    "- Frequentists define probability as a relative frequency of an event in the long run\n",
    "- Bayesians define probability as a measure of uncertainty and belief for any event.\n",
    "> TODO: READ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aebc7",
   "metadata": {},
   "source": [
    "### Parametric vs NonParametric Method\n",
    "- Parametric methods makes assumption in regards to the form of the function $f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p}$, where $f(X)$ is the unknown function to be estimated, $\\beta$ are the coefficients to learn, $X$'s are the corresponding inputs and $p$ is the number of independent variables. \n",
    "  - These assumptions may or may not be correct\n",
    "  - these methods are quite fast\n",
    "  - they require significantly less data\n",
    "  - they are more interpretable\n",
    "  - Examples \n",
    "    - Linear Regression\n",
    "    - Naive Bayes\n",
    "    - Perceptron\n",
    "- NonParametric methods donot make any underlying assumption wrt to the form of the function to be estimated.\n",
    "  - they tend to be more accurate\n",
    "  - they require lots of data\n",
    "  - Examples:\n",
    "    - Support Vector Machines\n",
    "    - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da96f1",
   "metadata": {},
   "source": [
    "### Prediction vs Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2dae6",
   "metadata": {},
   "source": [
    "### Mathematical functions\n",
    "\n",
    "#### Logistic/Sigmoid function\n",
    "$$ f(x) = \\frac{L}{1+e^{-k(x-x_{0})}} $$\n",
    "> L - the curve’s maximum value  \n",
    "> $x_{0}$ - the sigmoid’s midpoint  \n",
    "> k - the logistic growth rate or steepness of curve  \n",
    "\n",
    "#### Standard logistic sigmoid function\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "> L = 1 - the curve’s maximum value  \n",
    "> $x_{0}$ = 0 - the sigmoid’s midpoint  \n",
    "> k = 1 - the logistic growth rate or steepness of curve  \n",
    "\n",
    "\n",
    "\n",
    "#### Logit function\n",
    "The logit function is the logarithm of the odds ratio (log-odds)\n",
    "$$ \\text{logit(p)} = \\ln\\frac{p}{1-p}$$\n",
    "\n",
    "#### Probit function\n",
    "\n",
    "#### Softplus function\n",
    "\n",
    "#### Weighted sum\n",
    "$$ \\theta^{T}x = \\sum\\limits_{i=1}^{n}\\theta_{i}x_{i} = \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} $$\n",
    "\n",
    "#### Likelihood function\n",
    "The likelihood function defines the joint probability of observed data as a function of parameters of the model\n",
    "$$ LL(\\theta) = \\sum\\limits_{i=0}^{n}y^{(i)}\\log\\sigma(\\theta^{T}x^{(i)}) + (1-y^{(i)}) \\log[1-\\sigma(\\theta^{T}x^{(i)})]  $$\n",
    "\n",
    "#### Gradient of log likelihood function\n",
    "It tries to choose values of $\\theta$ that maximizes the function\n",
    "$$ \\frac{\\partial LL(\\theta)}{\\partial \\theta_{j}} = \\sum\\limits_{i=0}^{n}\\left[ y^{(i)} - \\sigma(\\theta^{T}x^{(i)}) \\right]x_{j}^{(i)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b8eff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa0046b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
