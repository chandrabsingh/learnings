{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1922675",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Percy Liang/Prof Dorsa Sadigh - CS221 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fc9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 4: Machine Learning 3 - Generalization, K-means | Stanford CS221"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d79f69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Review\n",
    "- Supervised Learning - Feature extractor\n",
    "- Prediction Score - for Linear predictor score or NN score\n",
    "- Loss function - to access the quality of linear classification\n",
    "- Training loss - the average over all the losses\n",
    "- Optimization - compute Stochastic gradient descent \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0f860",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### True Objective of ML\n",
    "- minimizing the training error\n",
    "- minimizing training error with regularization\n",
    "- minimizing error on test sets\n",
    "- __minimizing error on unseen future examples__(correct)\n",
    "- learning about the machines\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d664357",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Generalization\n",
    "- Unsupervised learning\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67beccb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalization\n",
    "- Is this training loss a good objective function?\n",
    "- Strawman algorithm (rote learning)\n",
    "  - overfitting \n",
    "- Evaluation\n",
    "  - how to access if predictor is good?\n",
    "  - how to minimize the error on unseen future examples\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57331f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximation and estimation error\n",
    "- Based on the defined feature extractor of \n",
    "    - Say $f^{*}$ - is the true predictor\n",
    "    - g - best predictor function based on this feature extractor class\n",
    "    - $\\hat{f}$ - is the prediction you made\n",
    "      - Approximation error - how good is your hypothesis class\n",
    "      - Estimation error - best thing in your hypothesis class and the learned function you actually find \n",
    "        > Err($\\hat{f}$) - Err($f^{*}$)  \n",
    "        > = [Err($\\hat{f}$) - Err(g)] + [Err(g) - Err($f^{*}$)]  \n",
    "        - 1st term is the estimation error  \n",
    "        - 2nd term is the approximation error  \n",
    "\n",
    "<img src=\"images/04_appEstErr.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef0a2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As the hypothesis class size increases\n",
    "  - Approximation error decreases\n",
    "    - bcoz taking min over larger set\n",
    "  - Estimation error increases\n",
    "    - bcoz its harder to estimate something complex\n",
    "- Learning\n",
    "  - Make your hypothesis class large, but dont make it too large that estimation becomes hard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976688bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to control hypothesis class size?\n",
    "  - Strategy 1: dimensionality\n",
    "    - Reduce dimensionality d of hypothesis class\n",
    "    - if you have 3 dimensions, reduce it to 2 or less\n",
    "      - Add features if they are helping\n",
    "      - Remove features if they don't help\n",
    "    - Automatic feature selection\n",
    "      - Forward selection\n",
    "      - Boosting\n",
    "      - L$_{1}$ regularization\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b207b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Strategy 2: Look at the norm or length of vector\n",
    "  - Reduce the norm(length) $\\left\\Vert  w \\right\\Vert$\n",
    "  - For example, for linear predictors, you need to see how long this weight vector is\n",
    "  - By reducing the weight vector slope, you are looking at a smaller zone which is very flat and constant functions\n",
    "  - total number of possible weight vector is shrinking and reducing\n",
    "\n",
    "<img src=\"images/04_hypothesisClassReduction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce25a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to control the norm\n",
    "- by reducing Regularization (second term)\n",
    "> $\\text{min}_{w} \\text{TrainLoss}(w) + \\frac{\\lambda}{2}\\left\\Vert  w \\right\\Vert^{2}$\n",
    "- try to minimize your original training loss and but also minimize the regularization term which is your penalty term\n",
    "  - in convex optimization, $\\lambda$ is called Lagrangian\n",
    "  - there's a duality in convex optimization where you penalize objective function by adding a penalty on weight vector and the constraint form where you minimize the training loss subject to the norm of w being less than some value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d44bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to optimize?\n",
    "- same as gradient descent, in addition shrink the weights towards zero by $\\lambda$(regularization strength)\n",
    "- the derivative and algorithm looks like this\n",
    "- in NN, $\\lambda$ is called weight decay\n",
    "- in statistics, $\\lambda$ is called L2 regularization because this is the Euclidean or 2-norm\n",
    "<img src=\"images/04_gdWithNorm.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bda0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Strategy - Early stopping\n",
    "- Why?\n",
    "- fewer the updates, then $\\left\\Vert  w \\right\\Vert$ wont get too big\n",
    "\n",
    "#### Summary\n",
    "- try to minimize the training error, but do it in a way that the hypothesis class is small\n",
    "- try to minimize the training set error, but dont try it too hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b6cf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to choose hyperparameters\n",
    "- features\n",
    "- regularization parameter\n",
    "- number of iterations\n",
    "- step size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ea674",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Validation\n",
    "- Split data into train, validate and test\n",
    "- tune hyperparameters using validation set rather than test set\n",
    "- how to choose size of validation set\n",
    "  - its a balance, where the validation set is large enough to give reliable estimates\n",
    "  - but you want to use most of data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68499f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problem Set - simplified named-entity recognition\n",
    "\n",
    "<img src=\"images/04_problemStatement1.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72230949",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised learning\n",
    "- In supervised learning\n",
    "  - training data $\\mathbb D_{\\text{train}}$ contains input-output(x,y) pairs\n",
    "  - Fully labeled data is expensive\n",
    "- In unsupervised learning\n",
    "  - unlabeled data is cheap\n",
    "  - training data $\\mathbb D_{\\text{train}}$ only contains input x \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca3845",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Text classification\n",
    "- In text classification, all the data is unlabeled\n",
    "- Same is with images\n",
    "- Word clustering\n",
    "  - input was raw text from news article\n",
    "  - returns bunch of clusters, which are very coherent\n",
    "- Word vectors \n",
    "  - do something similar\n",
    "  - embed words into clusters\n",
    "  - contextualized vector is very hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf4497",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Image classification\n",
    "  - clustering with deep embeddings\n",
    "  - clustering algorithm which is jointly learning the features\n",
    "  - identify different type of digits\n",
    "    - this is not classification, this is finding structure/visual similarity that are the same thing\n",
    "    - these clusters many a times, correspond to label  \n",
    "    \n",
    "<img src=\"images/04_imageCluster.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bfc9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of unsupervised learning\n",
    "- Clustering \n",
    "  - K-means\n",
    "- Dimensionality reduction\n",
    "  - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f66de9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Clustering\n",
    "- given set of points\n",
    "- output an assignment of each point to a cluster\n",
    "\n",
    "<img src=\"images/04_imageCluster2.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0bec8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### K-means objective\n",
    "- Every cluster will have a _centroid_, representing where the cluster is\n",
    "- _Associate_ each of the points with a particular centroid\n",
    "  - these two points will represent the clustering\n",
    "    - centroid\n",
    "    - assignment of points into cluster\n",
    "      - Optimize both\n",
    "- Each cluster k = 1,..K is represented by centroid $\\mu_{k} \\in \\mathbb R^{d}$\n",
    "- Goal: associate each point $\\phi(x_{i})$ close to its assigned centroid $\\mu_{z_{i}}$\n",
    "- Objective function:\n",
    "> $$\\text{Loss}_{kmeans}(z, \\mu) = \\sum\\limits_{i=1}^{n} \\left\\Vert \\phi(x_{i}) - \\mu_{z_{i}} \\right\\Vert^{2} $$  \n",
    "- for each point, measure the distance between that point and the associated centroid\n",
    "- $z_{i}$ is a number between 1 and K\n",
    "- we are interested in minimizing the squared distance between the points\n",
    "- Need to choose centroids $\\mu$ and assignments $z$ __jointly__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f621ed2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How does each point get associated to a centroid?\n",
    "- it will be specified by the z's which will be optimized overall\n",
    "- we don't know it apriori\n",
    "Do we know how many clusters can there be?\n",
    "- No, not in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402dddb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### K-means algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558de7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### K-means: Local minima\n",
    "- K-means is guaranteed to converge to local minima, but is not guaranteed to find the global minimum\n",
    "- One way is to randomly initialize using different random points, and take the best - this is not guaranteed to converge\n",
    "- Another way is to put down points which are farthest away as possible, this spreads out the centres, which does not interfere with each other - which generally works pretty well \n",
    "  - this method is called k-means plus plus\n",
    "  - initialize with a __heuristic(k-means++)__\n",
    "- How do you choose K\n",
    "  - the loss generally goes down\n",
    "  - we pick a cut-off point\n",
    "  - other way is - say you have validation steps and you choose a minimum based on that"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
