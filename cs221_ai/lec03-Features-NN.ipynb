{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d43458",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Percy Liang/Prof Dorsa Sadigh - CS221 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdc714",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 03 - Features and Neural Network, Stanford CS221: AI\n",
    "### Recap\n",
    "  - Loss function\n",
    "    - minimize the training loss which depends upon the weight vector\n",
    "      - find the w that minimizes the training loss\n",
    "    - In regression, for loss function, \n",
    "      - we look at minimizing the residual which is the difference between prediction and actual value\n",
    "      - if residual is 0, the loss is zero\n",
    "      - if residual increases, loss increases quadrically for squared loss or linearly for absolute deviations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dd2ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    - In classification, we look at margin \n",
    "      - Margin is the score times the label(+1/-1), which says how correct we are\n",
    "      - large margin is good\n",
    "        - we obtain either 0 or near 0 loss\n",
    "      - less than 0 margin\n",
    "        - means we are making mistake\n",
    "      - Hinge loss or logistic loss grow linearly or exponentially\n",
    "      \n",
    "<img src=\"images/03_lossFunctions02.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dc924",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Squared loss works more like mean where it penalizes the outlier\n",
    "- Absolute loss works more like median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc199381",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Regression example\n",
    "- How does the training set of this regression example looks like?\n",
    "  - training loss is the average over the losses on the individual examples\n",
    "  - the goal is to fit $w_{1}$ and $w_{2}$  \n",
    "  - the min point will be at the center of all losses\n",
    "  - for every possible weight vectors, the function coming out of the board is the loss\n",
    "\n",
    "<img src=\"images/03_regressionLoss.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef39a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- How do you optimize it?\n",
    "  - Gradient descent\n",
    "    - might be slow because we will have to go over all the individual points\n",
    "  - Stochastic gradient descent\n",
    "    - faster but unstable, as it allows to pick up individual example and make a gradient step\n",
    "    - dominates in ML world as it allows to scale\n",
    "    - if data comes online, in the fly  \n",
    "    \n",
    "<img src=\"images/03_regressionLoss2.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be284e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Can we obtain decision boundaries which are circles by using linear classifiers?\n",
    "  - Yes, \n",
    "  - linear features can produce non-linear decision boundaries\n",
    "  \n",
    "  <img src=\"images/03_linearClassifierQ.png\" width=400 height=400> \n",
    "  $\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02abc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline:\n",
    "- Features\n",
    "- Neural Networks\n",
    "- Gradient without tears\n",
    "- Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937cf59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Features\n",
    "- Scores drives prediction \n",
    "  - Score is a dot product between weight vector and feature vector\n",
    "  - In regression, score is a real number\n",
    "  - In binary classification, score is a sign  \n",
    "> $$w.\\phi(x)$$  \n",
    "- how we choose weight vector and __learn__ based on training data and  via __optimization__  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbe3f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### feature extraction spec   \n",
    "- focus on $\\phi(x)$, how we choose features  \n",
    "\n",
    "  ![alt-text](./images/03_featureExt.png \"Feature extraction\")\n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a725a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### feature template\n",
    "- organize\n",
    "  - Array representation\n",
    "    - Good for dense features\n",
    "  - Map representation\n",
    "    - Good for sparse features\n",
    "  ![alt-text](./images/03_featureTemplate.png \"Feature template\")\n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc164e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hypothesis class\n",
    "- possible predictors\n",
    "- defines what is possible out of learning\n",
    "- $\\mathbb F_{2}$ function is more expressive than $\\mathbb F_{1}$, and is a larger set function\n",
    "- it can represent more things\n",
    "- when creating a feature vector, we must think of its expressiveness\n",
    "\n",
    "<img src=\"images/03_functions.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ac36b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- set $f_{w} \\in \\mathbb F$ is the best function that represents the feature domain knowlege of $\\mathbb F$\n",
    "  - if the feature extaction is not good and is small, we will not get good accuracy\n",
    "  - if you dont optimize properly, no matter how good the hypothesis class is defined, the null hypothesis wont get achieve\n",
    "\n",
    "<img src=\"images/03_featureExtraction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c457c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example task - predict if second message is a response to the first\n",
    "- time elapsed\n",
    "- be careful about what you normalize, for example time\n",
    "- time elapsed between\n",
    "- be careful what things to look out for, 1year or 5 seconds\n",
    "- be careful how you represent features like time in discrete form or real format\n",
    "- how to discretize the features\n",
    "- when not to discretize \n",
    "- first message contains\n",
    "- second message contains\n",
    "- both message contains common word\n",
    "- be careful when would you not discretize - if you know feature is linear or quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d9340",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linear in what?\n",
    "> $$\\text{Score} = w.\\phi(x)$$  \n",
    "- Is this score linear in w? \n",
    "  - Yes, as it is a weighted combination of input\n",
    "- Is this score linear in $\\phi(x)$?  \n",
    "  - Yes, by symmetry of dot product\n",
    "- Is this score linear in x?\n",
    "  - No\n",
    "> - Predictors $f_{w}(x)$ can be expressive non-linear function and decision boundary of x.  \n",
    "> - Score is a linear function of w, which permits efficient learning.  \n",
    "> - We can get non-linear decision boundary in x space with linear decision boundary in $\\phi(x)$ space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adc585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks\n",
    "- Expressive models\n",
    "- NN are bunch of linear functions stiched together in non-linear manner\n",
    "- When any problem is given, try simplest form\n",
    "- then increase the complexity\n",
    "- Example: predicting car collision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369aac57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to decompose problem into parts\n",
    "- Problem: detect if two cars will collide\n",
    "<img src=\"images/03_carCollision.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079c2fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Decompose in linear functions\n",
    "<img src=\"images/03_carCollisionDecom.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2954d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Can we learn these functions automatically?\n",
    "- Define feature vector as $\\phi(x)$\n",
    "- In $\\phi(x)$, the first term is the bias term \n",
    "- The function $x_{1} - x_{2}\\geq 1$ is represented as the linear combination of $v_{1}.\\phi(x)\\geq 0$, where $v_{1}$ is a vector as below\n",
    "- The final sign prediction is written as linear combination of weights and $h_{1}, h_{2}$ \n",
    "- For NN, we will learn both hidden V and w and try to fit through training\n",
    "- Earlier we were tuning only w, in NN we tune both V and w\n",
    "- V defines the hidden problems that we are interested in\n",
    "- w defines how do we take the results of hidden problem and make a prediction\n",
    "<img src=\"images/03_hiddenLayerEqn.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f51eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient\n",
    "- The gradient of $h_{1}$ wrt $v_{1}$ is 0\n",
    "- this step function is not differentiable\n",
    "> $h_{1} = \\mathbb 1[v_{1}.\\phi(x) \\geq 0]$  \n",
    "- a smoother function will be logistic function, which does not reach 0 or 1\n",
    "\n",
    "<img src=\"images/03_h1Function.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921d901",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The derivative of logistic function is \n",
    "\n",
    "<img src=\"images/03_dLogisticFunction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   \n",
    "\n",
    "- This transforms the $h_{1}$ into\n",
    "> $h_{1} = \\sigma(v_{1}.\\phi(x))$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fb904",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Neural networks\n",
    "- To define NN in terms of linear function\n",
    "  - the score function will be as follows\n",
    "  - where each of the function will be weighted by w\n",
    "\n",
    "<img src=\"images/03_nnLinearFunction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441227c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- NN breakdown the problem into subproblems are the result of these intermediate computations\n",
    "- NN with one hidden layer, will be the same as above\n",
    "- the intermediate hidden units will be sigmoid function, applied to $v_{j}.\\phi(x)$\n",
    "- which is then send in through the logistic function\n",
    "\n",
    "<img src=\"images/03_nnIntermediateFunction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af62306b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Finally the output of $h_{1}$ and $h_{2}$, dot product is taken with w, to calculate the score\n",
    "\n",
    "<img src=\"images/03_nnFinalScoreFunction.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb62373",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $h_{j}$ are set of features learned automatically from data as opposed to fed into the system\n",
    "- For deep NN, we kind of stack them \n",
    "- As output of one set of classifiers becomes the features of another\n",
    "- For example with images, you start with pixels, then you find the edges, then you define the object parts, and then resolve the classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956ce18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NN Loss Minimization\n",
    "\n",
    "<img src=\"images/03_lossOptimizationNN.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5c09e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Another Approach using Computation Graph\n",
    "- benefits \n",
    "  - computation is modular and efficient using TensorFlow, PyTorch\n",
    "  - avoid long equation\n",
    "- making graphs is not needed, but gives insight\n",
    "  - A slight change in $in_{1}$, increases by 2 (gradient)\n",
    "  \n",
    "<img src=\"images/03_graphFuncGradient.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aada39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  - Here change in $in_{2}$ by $\\epsilon$, results in change in $in_{3}$ (gradient)\n",
    "<img src=\"images/03_graphFuncGradient2.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5378877",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Basic Building blocks\n",
    "- In + example - Partial derivative wrt to a is 1, wrt b is 1\n",
    "- In - example - Partial derivative wrt to a is 1, wrt b is -1\n",
    "- In . example - Partial derivative wrt to a is b, wrt b is a\n",
    "- In max(a,b) example - \n",
    "   - If a = 7, b = 3, \n",
    "     - if a = 7 + $\\epsilon$, the gradient of max function changes by $\\epsilon$\n",
    "     - if b = 3 + $\\epsilon$, the gradient of max function does not change\n",
    "     - partial derivative of max function is 1 or 0, depending on the condition, represented as $\\mathbb 1[a>b]$\n",
    "![alt-text](./images/03_basicBuildingNN.png \"NN Loss optimization2\")\n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac97794",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Composing functions\n",
    "- If mid function$_{1}$ amplifies by factor of 2 and out function$_{2}$ by a factor of n, the total effect will be 2n\n",
    "\n",
    "<img src=\"images/03_composeFun.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458ab91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Binary classification with hinge loss\n",
    "- Hinge loss\n",
    "> Loss(x,y,w) = max$\\{1-w.\\phi(x)y, 0\\}$  \n",
    "> $\\nabla_{w}\\text{Loss}(x,y,w) = \\frac{\\partial Loss(x,y,w)}{\\partial w}$  \n",
    "- Computation graph and its gradient\n",
    "  - If margin is less than 1, gradient = $-\\phi(x)y$  \n",
    "  - If margin is more than 1, gradient = 0  \n",
    "  \n",
    "![alt-text](./images/03_hingeLossGraph2.png \"Hinge Loss Graph\")\n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cea427",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### NN Computation graph \n",
    "\n",
    "<img src=\"images/03_computationGraphNNLoss2.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bba609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation\n",
    "- Allows computation of gradient of any type\n",
    "- PyTorch and TensorFlow has the default implementation for it\n",
    "- it computes 2 types of value\n",
    "  - Forward value - $f_{i}$ for every node is the value of the expression tree\n",
    "  - Backward value - $\\frac{\\partial \\text{out}}{\\partial f_{i}}$ is the partial derivative wrt $f_{i}$, for that particular node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490c391",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Backpropagation - Algorithm\n",
    "- Forward pass - computes each $f_{i}$ from leaves to root\n",
    "- Backward pass - computes each $g_{i}$ from root to leaves\n",
    "\n",
    "<img src=\"images/03_backpropagationAlgo.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26468b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Notes on optimization\n",
    "- Optimization of NN is generally hard\n",
    "- Linear functions has convex loss\n",
    "- NN functions has non-convex loss, if you find local minima chances are you will get stuck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc59ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Nearest neighbors\n",
    "- Find the midpoint between two zones\n",
    "<img src=\"images/03_nearestNeighborAlgo.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   \n",
    "\n",
    "<img src=\"images/03_nearestNeighborDia.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   \n",
    "\n",
    "- each point has its own parameter\n",
    "- can fit very expressive models\n",
    "- computationally expensive as we need to store entire training example\n",
    "\n",
    "<img src=\"images/03_voronoiDia.png\" width=400 height=400> \n",
    "$\\tiny{\\text{YouTube-Stanford-CS221-Percy Liang}}$   "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
