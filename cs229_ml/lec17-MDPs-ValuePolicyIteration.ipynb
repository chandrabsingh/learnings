{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00091f09",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4eea24",
   "metadata": {},
   "source": [
    "## Lecture 16\n",
    "\n",
    "#### Outline\n",
    "- Reinforcement Learning \n",
    "  - MDP (recap)\n",
    "  - Value function\n",
    "  - Value iteration/Policy iteration\n",
    "  - Learning state transition probability/putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8bc31",
   "metadata": {},
   "source": [
    "### How do you compute optimal policy\n",
    "- One of the challenges of finding optimal policy is if we have 11 states and 4 actions, there is a exponentially large number of possible policies $4^{11}$ \n",
    "- how to find the best policy\n",
    "- we need to define 3 things\n",
    "  - $V^{\\pi}$ - For policy $\\pi$, $V^{\\pi}: S \\mapsto R $ is s.t. $V^{\\pi}(s)$ is the expected total payoff for starting in state s and executing $\\pi$ (take actions according to policy $\\pi$) is \n",
    "    > $V^{\\pi}(s) = E[R(s_{0}, a_{0}) + \\gamma R(s_{1}, a_{1}) + \\gamma^{2} R(s_{2}, a_{2}) + ... | \\pi, s_{0}=s]$  \n",
    "    - $V^{\\pi}$ is called \"Value function for policy $\\pi$\"  \n",
    "  - $V^{*}$ - is the optimal value function\n",
    "  - $\\pi^{*}$ - is the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e65195",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/17_v_pi1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fe7eb",
   "metadata": {},
   "source": [
    "- Bellman's equation - governs the value function\n",
    "- it says that \n",
    "> $V^{\\pi}(s) = R(s) + \\gamma\\sum\\limits_{s'\\in S}P_{s\\pi(s)}(s')V^{\\pi}(s')$\n",
    "\n",
    "- $s \\rightarrow s_{0}$, $s' \\rightarrow s_{1}$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/17_v_be1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/17_v_be2.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a6dbd17",
   "metadata": {},
   "source": [
    "#### Optimal Value Function V*\n",
    "- Look at all the possible exponential policies for this MDP, and the max of all policies is the optimal value function\n",
    "> $V^{*}(s) = \\max\\limits_{\\pi}V^{\\pi}(s)$\n",
    "- there is a different version of Bellman's equation\n",
    "> $V^{\\pi}(s) = R(s) + \\max\\limits_{a}\\gamma\\sum\\limits_{s'\\in S}P_{sa}(s')V^{*}(s')$\n",
    "\n",
    "\n",
    "- $\\pi^{*}$ is the optimal policy\n",
    "> $\\pi^{*} = \\text{arg}\\max\\limits_{a}\\sum\\limits_{s'\\in S}P_{sa}(s')V^{*}(s')$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/17_vstar_be1.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a3bc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96561212",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "951ea8a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62777b7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e256eb49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "016c5360",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e4937f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2633043",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
