{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde1825",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72dfc6",
   "metadata": {},
   "source": [
    "## Lecture 17\n",
    "\n",
    "#### Outline\n",
    "- Reinforcement Learning \n",
    "  - MDP (recap)\n",
    "  - Value function\n",
    "  - Value iteration/Policy iteration\n",
    "  - Learning state transition probability/putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49764b2",
   "metadata": {},
   "source": [
    "### Value function for policy $\\pi$\n",
    "- One of the challenges of finding optimal policy is if we have 11 states and 4 actions, there is a exponentially large number of possible policies $4^{11}$ \n",
    "- how to find the best policy\n",
    "- we need to define 3 things\n",
    "  - $V^{\\pi}$ - For policy $\\pi$, $V^{\\pi}: S \\mapsto R $ is s.t. $V^{\\pi}(s)$ is the expected total payoff for starting in state s and executing $\\pi$ (take actions according to policy $\\pi$) is \n",
    "    > $V^{\\pi}(s) = E[R(s_{0}, a_{0}) + \\gamma R(s_{1}, a_{1}) + \\gamma^{2} R(s_{2}, a_{2}) + ... | \\pi, s_{0}=s]$  \n",
    "    - $V^{\\pi}$ is called \"Value function for policy $\\pi$\"  \n",
    "  - $V^{*}$ - is the optimal value function\n",
    "  - $\\pi^{*}$ - is the optimal policy\n",
    "  - +1/-1 - is called absorbing state. there is no more policy once it reaches to this state\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef8784",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/17_v_pi1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7917c3",
   "metadata": {},
   "source": [
    "#### Bellman's equation\n",
    "- governs the value function\n",
    "\n",
    "- The intution is - robot woke up at $s_{0}$ state, the reward you get is $R(s_{0})$, which is the _immediate reward_. Then it takes some action and ends up at state $s_{1}$, the _future reward_ you get is $\\gamma R(s_{1})$. And then $\\gamma R(s_{2})$ and so on. given that we execute the policy $\\pi$ and we start off in this state $s_{0}$. This can be written as:\n",
    "\n",
    "> $\\begin{equation}\\\\\n",
    "\\begin{aligned}\\\\\n",
    "V^{\\pi}(s) &= E[R(s_{0}) + \\gamma R(s_{1})+ \\gamma^{2} R(s_{2}) + .. |\\pi, s_{0}]\\\\\n",
    "&= E[R(s_{0}) + \\gamma \\{ R(s_{1})+ \\gamma^{1} R(s_{2}) + ..\\} |\\pi, s_{0}]\\\\\n",
    "&= E[R(s_{0}) + \\gamma V^{\\pi}(s_{1}) |\\pi, s_{0}]\\\\\n",
    "\\end{aligned}\\\\\n",
    "\\end{equation}\\\\\n",
    "$  \n",
    "\n",
    "- the bellman's equation says that the expected total payoff you get if the robot wakes up at state s is the immediate reward plus gamma times the expected future rewards\n",
    "\n",
    "- the mapping between these two equation will be:\n",
    "> $s \\rightarrow s_{0}$(current state), $s' \\rightarrow s_{1}$(future state). \n",
    "- So the equation with relevant mapping will be:\n",
    "> $V^{\\pi}(s) = E[R(s) + \\gamma V^{\\pi}(s')]$\n",
    "\n",
    "- What is s' drawn from which distribution?\n",
    "> $s' \\sim P_{s \\pi (s)}$\n",
    "- in state s, take action $a=\\pi(s)$. so the distribution from which s' is drawn from\n",
    "> $s' \\sim P_{s a}$\n",
    "\n",
    "- so the bellman equation becomes:\n",
    "> $V^{\\pi}(s) = R(s) + \\gamma\\sum\\limits_{s'\\in S}P_{s\\pi(s)}(s')V^{\\pi}(s')$\n",
    "\n",
    "- given $\\pi$, this gives us a linear system of equation in terms of $V^{\\pi}(s)$\n",
    "- the unknown variables gives us a system of 11 linear equations with 11 unknowns\n",
    "- can use linear algebra solver to solve this\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/17_v_be2.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf720f",
   "metadata": {},
   "source": [
    "### Optimal Value Function V*\n",
    "- Look at all the possible combinatorial exponential policies for this MDP, and the max of all policies is the optimal value function\n",
    "> $V^{*}(s) = \\max\\limits_{\\pi}V^{\\pi}(s)$\n",
    "\n",
    "#### Bellman's equation\n",
    "- there is a different version of Bellman's equation\n",
    "> $V^{\\pi}(s) = R(s) + \\max\\limits_{a}\\gamma\\sum\\limits_{s'\\in S}P_{sa}(s')V^{*}(s')$\n",
    "\n",
    "### Optimal policy\n",
    "- Lets say we have a way of how to calculate $V^{*}$\n",
    "\n",
    "- $\\pi^{*}$ is the optimal policy\n",
    "> $\\pi^{*} = \\text{arg}\\max\\limits_{a}\\sum\\limits_{s'\\in S}P_{sa}(s')V^{*}(s')$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/17_vstar_be1.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e9a29",
   "metadata": {},
   "source": [
    "- For every $\\pi$, s; the optimal value for state S is equal to \n",
    "> $V^{*}(s) = V^{\\pi *}(s) \\ge V^{\\pi}(s)$\n",
    "\n",
    "### Strategy for finding optimal policy\n",
    "  - a) find $V^{*}$\n",
    "  - b) use arg max equation to find $\\pi^{*}$\n",
    "\n",
    "#### Value Iteration\n",
    "- Value iteration gives us the $V^{*}$. The algorithm is as follows. It uses synchronous or asynchronous update. synchronous update is the more common one, where you update all 11 values at the same time.\n",
    "\n",
    "<img src=\"images/17_valueIteration1.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5deb42",
   "metadata": {},
   "source": [
    "#### Policy Iteration\n",
    "\n",
    "- the focus is policy rather than value\n",
    "- solve for the value function for the policy $\\pi$, which was linear system of equations $V^{\\pi}$\n",
    "- assume V is the optimal value function and update $\\pi(s)$, using Bellman's equation\n",
    "- iterate until it converges\n",
    "\n",
    "<img src=\"images/17_policyIteration1.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8064f86",
   "metadata": {},
   "source": [
    "#### Pros and cons\n",
    "- policy iteration \n",
    "  - is relatively easy for small state space\n",
    "- value iteration \n",
    "  - is relatively better for large state space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d40818",
   "metadata": {},
   "source": [
    "### State transition probability\n",
    "- is not known in advance\n",
    "- estimate $P_{sa}(s')$ from data\n",
    "> $P_{sa}(s') = \\frac{\\text{# times took action a in state s and got to state s'}}{\\text{# times took action a in state s}}$ \n",
    "\n",
    "<img src=\"images/17_stateTransitionProb.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f6f64",
   "metadata": {},
   "source": [
    "#### Exploration vs Exploitation\n",
    "- may converge to a local optima \n",
    "- exploitation\n",
    "  - how aggressive or how greedy you should be at taking actions to maximize your rewards\n",
    "- exploration\n",
    "  - the process of taking actions that may appear less optimal at the outset, maybe if it tries some new things it's never tried before, maybe it will find a new thing\n",
    "- this is the exploitation/exploration trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f721a",
   "metadata": {},
   "source": [
    "##### epsilon-greeedy \n",
    "- 0.9 chance wrt $\\pi$\n",
    "- 0.1 chance act randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e6e1",
   "metadata": {},
   "source": [
    "- Search for \"intrinsic motivation\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
