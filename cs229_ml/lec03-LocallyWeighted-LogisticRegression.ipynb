{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97055344",
   "metadata": {},
   "source": [
    ">>> Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548f45b",
   "metadata": {},
   "source": [
    "### Outline:  \n",
    "- Linear Regression (recap)  \n",
    "- Locally Weighted regression  \n",
    "- Probabilistic interpretation  \n",
    "- Logistic Regression  \n",
    "- Newton's method  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c3b60",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Notation  \n",
    "* $ (x^{(i)}, y^{(i)}) - i^{th} $ example  \n",
    "* $ x^{(i)} \\in \\mathbb R^{n+1}, y^{(i)} \\in \\mathbb R, x_{0} = 1 $  \n",
    "* m - # examples, n = #features  \n",
    "* $ h_{\\theta}(x) = \\sum\\limits_{j=0}^{n} \\theta_{j}x_{j} = \\theta^{T}x $  \n",
    "* Cost function $ J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c496f69",
   "metadata": {},
   "source": [
    "### Feature selection algorithm \n",
    "- what sort of feature do you want to fit  \n",
    "    - $\\theta_{0} + \\theta_{1}x_{1}$ \n",
    "    - $\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x^{2}$  \n",
    "        * quadratic equation, it comes back to the axis\n",
    "    - $\\theta_{0} + \\theta_{1}x + \\theta_{2}\\sqrt x $ \n",
    "        * if we dont want quadratic function to curve back\n",
    "    - $\\theta_{0} + \\theta_{1}x + \\theta_{2}\\sqrt x + \\theta_{3} log(x)$ \n",
    "- Once we have defined how we want to fit, we can apply the machinery of linear regression here\n",
    "    - Later, we will learn, what sort of feature ($x, \\sqrt x, x^{2}$) is best to fit\n",
    "\n",
    "\n",
    "Different ways of adjusting this problem when regression does not fit in a single line, we try to solve it using locally weighted linear regression  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff78a77",
   "metadata": {},
   "source": [
    "### Locally weighted regression   \n",
    "- \"Parameteric\" learning algorithm -   \n",
    "  - fit fixed set of parameters ($\\theta_{i}$) to data  \n",
    "  - For example: Linear Regression\n",
    "- \"Non-Parameteric\" learning algorithm    \n",
    "  - Amount of data/parameter, you need to keep grows (linearly) with size of training set data\n",
    "  - to make predictions, we need to save lots of data\n",
    "  - For example: Locally weighted regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930d0df",
   "metadata": {},
   "source": [
    "To evaluate h at certain value of input X:\n",
    "- For linear regression, \n",
    "- fit $\\theta$ to minimize the cost function, \n",
    "> $\\frac{1}{2}\\sum\\limits_{i}(y^{(i)}-\\theta^{T}x^{(i)})^{2}$\n",
    "- and return $\\theta^{T}X$\n",
    "- For locally weighted regression, __more weights are put on some local areas__, rather than focusing globally\n",
    "> $\\sum\\limits_{i=1}^{m}w^{(i)}(y^{(i)}-\\theta^{T}x^{(i)})^{2}$ - __Eq 1__\n",
    "- where $w^{(i)}$ is a weighting function\n",
    "> $w^{(i)} = exp(\\frac{-(x^{(i)} - x)^{2}}{2\\tau^{2}})$\n",
    "- x is the location where we want to make prediction\n",
    "- $x^{i}$ is the ith training example\n",
    "- If $|x^{(i)} - x|$ is small, $ x^{(i)} \\approx 1$\n",
    "- If $|x^{(i)} - x|$ is large, $ x^{(i)} \\approx 0$\n",
    "- The equation 1 is similar to what we saw in linear regression LMS. The difference is $w^{(i)}$. \n",
    "    - If the example $x^{(i)}$ is far from the prediction is to be made, the prediction is multiplied with error term 0\n",
    "    - If the example $x^{(i)}$ is close from the prediction is to be made, the prediction is multiplied with error term 1\n",
    "- If bandwidth $\\tau$ is too large, the fitting will be over-smoothing. If bandwidth is too small, the fitting will be too jagged. This hyperparameter allows you to say, how many parameters would you choose to make a prediction\n",
    "- _depending on which point, you want to make a prediction, we focus on that locality_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c0f26",
   "metadata": {},
   "source": [
    "<img src=\"images/03_locallyWt.png\" width=400 height=400 />   $\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "<img src=\"images/03_locallyWt2.png\" width=400 height=200 />   $\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee43e6",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Probabilistic interpretation of Linear Regression\n",
    "\n",
    "- Why least squares has squared errors?\n",
    "  - Assume that true price of every house $y^{(i)} = \\theta^{T}x^{(i)} + \\epsilon^{(i)}$, where first term is the true price and the second error term $\\epsilon^{(i)}$ is due to unmodelled effects and random noise\n",
    "  -  where $\\epsilon^{(i)} \\sim N(0, \\sigma^{2})$ - is distributed Gaussian with mean 0 and variance $\\sigma^{2}$\n",
    "  - $ P(\\epsilon^{(i)}) = \\frac{1}{\\sqrt (2\\pi)\\sigma}exp(- \\frac{(\\epsilon^{(i)})^{2}}{2\\sigma^{2}} )$, which integrates to 1\n",
    "  - Assumption is $\\epsilon^{(i)}$ is IID, error term of one house is not affected by price of other house in the next lane. It is not absolutely true, but is good enough\n",
    "  - this implies that $P(y^{(i)}| x^{(i)}; \\theta) = \\frac{1}{\\sqrt (2\\pi)\\sigma} exp(- \\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}})$ \n",
    "    - \";\" means \"parameterized by\". Instead if \", $\\theta$\" would mean - \"conditioned by $\\theta$, which will be a random variable\", which is not the case here\n",
    "    > i.e., $ (y^{(i)}| x^{(i)}; \\theta) \\sim N(\\theta^{T}x^{(i)}, \\sigma^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fda8a5",
   "metadata": {},
   "source": [
    "#### Likelihood of parameter $\\theta$\n",
    "- $L(\\theta) = P(y^{(i)}| x^{(i)}; \\theta) $\n",
    "\n",
    "#### Difference between likelihood and probability?\n",
    "- likelihood of parameters - make the data as a fixed thing and vary parameters\n",
    "- probability of data - make the parameter as a fixed thing and vary the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8ce83",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "> $l(\\theta) = \\text{log } L(\\theta)\\\\\n",
    "= \\text{log} \\prod\\limits_{i=1}^{m} \\frac{1}{\\sqrt (2\\pi)\\sigma} \\exp(- \\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}})\\\\\n",
    "= \\sum()\n",
    "$\n",
    "\n",
    "<img src=\"images/03_logLikelihoodDerivation.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b1380",
   "metadata": {},
   "source": [
    "### Classification\n",
    "- Here we used a setup of probabilistic assumptions, with the key assumption that Gaussian errors are iid, which turns out to be exactly like least squares algorithm\n",
    "- Use this framework and apply it to a different type of problem - classification problem\n",
    "  - make assumption about $P(y|x; \\theta)$\n",
    "  - find max likelihood estimation\n",
    "- in classification, the value of __y is either 0 or 1__\n",
    "\n",
    "#### Binary classification\n",
    "- $y \\in \\{0,1\\}$\n",
    "- Why linear regression should not be used for binary classification?\n",
    "  - dont use linear regression to binary classification\n",
    "  - it will be a very bad fit especially when there are outliers\n",
    "  - the decision boundary will be very different\n",
    "\n",
    "<img src=\"images/03_binaryClassificationRegression.png\" width=400 height=200 />   $\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f262eb0",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- Goal: __hypothesis function to output values between 0 and 1__\n",
    "> $$h_{\\theta}(x) \\in [0,1]$$\n",
    "- choose following form of hypothesis\n",
    "> $$h_{\\theta}(x) = g(\\theta^{T}x) = \\frac{1}{1+e^{-\\theta^{T}x}} $$\n",
    "- where g(z) is logistic function or sigmoid function\n",
    "> $$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- In Linear Regression, the hypothesis was\n",
    "> $$h_{\\theta}(x) = \\theta^{T}x $$\n",
    "- so in logistic regression, the hypothesis function uses this sigmoid function that force generate output between 0 and 1\n",
    "<br>\n",
    "\n",
    "- Why did we specifically choose sigmoid function?\n",
    "  - there is a broader class of algorithms called generalized linear models(GLM) of which this is a special case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00c2cf",
   "metadata": {},
   "source": [
    "#### How do we fit $\\theta$\n",
    "- we fit the parameters using maximum likelihood\n",
    "- Let us assume\n",
    "> $P(y=1|x;\\theta) = h_{\\theta}(x)$\n",
    "> $P(y=0|x;\\theta) = 1 - h_{\\theta}(x)$\n",
    "- generalizing above, this can be written using\n",
    "> $y \\in \\{0,1\\}$\n",
    "- as\n",
    "> $$P(y|x;\\theta) = h(x)^{y}(1-h(x))^{(1-y)}$$\n",
    "- above equation is a form of if..else \n",
    "\n",
    "#### Log likelihood \n",
    "\n",
    "<img src=\"images/03_logisticLikelihoodEst.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "\n",
    "<img src=\"images/03_logisticLogLikelihoodEst.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "- Choose $\\theta$ so as to maximizes the log-likelihood $l(\\theta)$\n",
    "- And then having chosen the value of $\\theta$, when a new patient comes in, then use $h(\\theta)$ to estimate the chance of this tumor in the new patient\n",
    "\n",
    "- Update the parameter $\\theta_{j}$ using \"batch gradient ascent\"\n",
    "> $\\theta_{j} := \\theta_{j} + \\alpha\\frac{\\partial}{\\partial \\theta_{j}}l(\\theta)$\n",
    "\n",
    "- the difference between the equation above and one for linear regression is \n",
    "  - instead of squared cost function, we are trying to optimize the log-likelihood function\n",
    "  - in least square, we tried to minimize the squared error. in logistic, we maximize the log likelihood\n",
    "  \n",
    "Note: learning rate $\\alpha$ is missing from these equation below\n",
    "<img src=\"images/03_logisticVslinear.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "  \n",
    "<img src=\"images/03_logisticVslinear_gd.png\" width=200 height=200 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "- Plug in the value of $h(\\theta)$ in log likelihood equation, and applying calculus/derivative, the gradient ascent equation becomes\n",
    "> $\\theta_{j} := \\theta_{j} + \\alpha\\sum\\limits_{i=1}^{m}(y^{(i)} - h_{\\theta}(x^{(i)}))x_{j}^{(i)}$\n",
    "- and for stochastic gradient ascent equation becomes\n",
    "> $\\theta_{j} := \\theta_{j} + \\alpha(y^{(i)} - h_{\\theta}(x^{(i)}))x_{j}^{(i)}$\n",
    "<br>\n",
    "\n",
    "- This eqution looks similar to the one derived for linear regression. The difference is $h_{\\theta}(x^{(i)})$ is now defined as a non-linear function of $\\theta^{T}x^{(i)}$. \n",
    "- Both linear regression and logistic regression are special case of GLM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f45fa4",
   "metadata": {},
   "source": [
    "### Newton's method\n",
    "- Gradient descent takes lot of steps to converge\n",
    "- Newton's method allows lot bigger step\n",
    "- but each iteration is much more expensive\n",
    "\n",
    "- 1-dimensional problem\n",
    "- Suppose we have function f and we want to find $\\theta$, s.t. $f(\\theta) = 0$\n",
    "- maximize $l(\\theta)$ by finding $l'(\\theta) = 0$\n",
    "\n",
    "- Goal: find $f(\\theta) = 0$\n",
    "  - take a random point $\\theta^{(0)}$\n",
    "  - find tangent that touches the horizontal axis and this point becomes $\\theta^{(1)}$\n",
    "  - repeat\n",
    "  \n",
    "  \n",
    "<img src=\"images/03_newtonMethodDia.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "- Solve for the value of $\\Delta$, and replace it to find value of $\\theta$\n",
    "  \n",
    "<img src=\"images/03_newtonMethodDerv.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "#### Quadratic convergence\n",
    "- In Newton's method, the convergence is quadratic\n",
    "\n",
    "- When $\\theta$ is a vector, i.e., $(\\theta \\in \\mathbb R^{n+1})$\n",
    "- when n increases to say 1000 dimension, taking $H^{-1}$ gets difficult\n",
    "\n",
    "- If the number of parameters is say 10,000 parameters, rather than dealing with 10Kx10k matrix inversion, stochastic gradient descent is a better choice\n",
    "- If the number of parameters is relatively smaller, Newton's method is a better choice\n",
    "\n",
    "<img src=\"images/03_newtonMethodHigherDim.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdb59a",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- READ: Bayesian vs Frequentist statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
