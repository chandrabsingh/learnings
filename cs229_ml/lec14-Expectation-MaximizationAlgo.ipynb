{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b90596",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367da134",
   "metadata": {},
   "source": [
    "## Lecture 14\n",
    "\n",
    "#### Outline\n",
    "- Unsupervised learning\n",
    "  - k-means clustering\n",
    "  - Mixture of Gaussians\n",
    "  - EM (Expectation Maximization)\n",
    "  - Derivation of EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00987e01",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "- In supervised learning, we are given labeled data and we classify them and find decision boundary. We are given (x,y)\n",
    "- Instead in unsupervised learning, we are given unlabeled data. We are given only x - $(x^{(1)}, x^{(2)}, ..., x^{(m)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f8f47",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "- group data into clusters\n",
    "- example - market segmentation\n",
    "  - what are the different market segments\n",
    "  - age range\n",
    "  - education\n",
    "  - different parts of country\n",
    "- k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5651dc",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "- pick 2 points called cluster centroids\n",
    "- cluster centroids which is your best guess, where the centers of two clusters are\n",
    "- k-means is an iterative algorithm and repeated do 2 things  \n",
    "  - 1st thing \n",
    "    - for each training example, color each training example as which cluster centroid is it closer to\n",
    "  - 2nd thing \n",
    "    - for each colored examples, compute the average\n",
    "    - move the centroid to the new average\n",
    "  - keep looking if the algorithm is converging or not\n",
    "  \n",
    "#### Algorithm\n",
    "- Data has no labels : $(x^{(1)}, x^{(2)}, ..., x^{(m)})$\n",
    "- 1. Initialize cluster centroid $\\mu_{1}, \\mu_{2}, ..., \\mu_{k} \\in \\mathbb R^{n}$ - by randomly pick k example out of your training set and set cluster centroids to k-randomly chosen examples\n",
    "- 2. Repeat until convergence\n",
    "  - a. Set $c^{(i)} := \\text{arg }\\min\\limits_{j} \\Vert (x^{(i)} - \\mu_{j})\\Vert_{2}$  (\"color the points\")\n",
    "    - Set $c^{(i)}$ equal to either j = 1 or 2 depending on whether that example $x^{(i)}$ is closer to cluster centroid 1 or 2\n",
    "    - Notation:\n",
    "      - L1 norm: $\\Vert x \\Vert_{1}$\n",
    "      - L2 norm: $\\Vert x \\Vert$ or $\\|x\\|^{2}$\n",
    "  - b. For j =1,2,..,k  (\"move the cluster centroids\")\n",
    "  > $\\mu_{j} := \\frac{\\sum\\limits_{i=1}^{m} \\mathbb 1 \\{c^{(i)} = j\\}x^{(i)}} {\\sum\\limits_{i=1}^{m} \\mathbb 1 \\{c^{(i)} = j\\}}$\n",
    "  \n",
    "- This algorithm is not guaranteed to converge, as it is a non-convex function\n",
    "- Cost/Distortion function\n",
    "> $J(c,\\mu) = \\min\\sum\\limits_{i=1}^{m}\\|x^{(i)} - \\mu_{c^{(i)}}\\|^{2}$\n",
    "- how do you choose k?\n",
    "  - choose manually, depending on what is the purpose of this algorithm\n",
    "  - if it is meant for market segmentation for 4 categories, it makes sense to have 4 cluster rather than more\n",
    "  - some formula available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e063b4",
   "metadata": {},
   "source": [
    "### Density estimation\n",
    "- Applications\n",
    "  - unusual/anomaly detection - aircraft \n",
    "  - very strange network traffic - anomaly detection\n",
    "  - very strange cellular traffic patterns - anomaly detection\n",
    "- Given all the training examples, __can you model what is the density from which x was drawn__? \n",
    "  - if $p(x) < \\epsilon$ is very small, you detect it as anomaly\n",
    "  - Mixture of Gaussians\n",
    "    - how do you p(x) with the data coming from an L-shaped \n",
    "    - there is no single distribution for modeling such complex distributions\n",
    "    - instead we can look into mixture of Gaussians models \n",
    "    - may be this data comes from two Gaussians\n",
    "    - may be there is more probability mass density in the lower Gaussian and low probability mass density in the upper one\n",
    "    - __contours of Gaussian mixture__\n",
    "\n",
    "<img src=\"images/14_densityEstimation.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff34066",
   "metadata": {},
   "source": [
    "### Gaussian mixture model\n",
    "- Lets consider 1 dimensional $(x \\in \\mathbb R)$\n",
    "- say there are two Gaussian distributions, and x's represent data points coming from 1st distribution and o's represent data points coming from 2nd distribution\n",
    "- following is the overall density of mixture distribution\n",
    "- if we knew that x's came from Gaussian 1 and o's from Gaussian 2, then we could have used GDA - Gaussian discriminant analysis to fit this model\n",
    "- the problem is we see only x's and maybe the data came from 2 different Gaussian, but we dont know which example it came from\n",
    "- the Expectation-Maximization EM algorithm helps us model despite not knowing which Gaussian this example came from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afaccb4",
   "metadata": {},
   "source": [
    "#### Mixture of Gaussian model\n",
    "- Suppose there is a latent(hidden/unobserved) random variable z; and $x^{(i)}, z^{(i)}$ are modeled as a joint distribution \n",
    "> $p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)})p(z^{(i)})$, where $z \\in \\{1,2,..,k\\}$\n",
    "- where $x^{(i)} \\sim $ Multinomial($\\phi$) and $x^{(i)}|z^{(i)} = j \\sim N(\\mu_{j}, \\Sigma_{j})$\n",
    "\n",
    "- If there are only two distributions, this model will be Bernoulli. If there are k Gaussians, z can take values from $\\{1,2,..,k\\}$ \n",
    "- Once we know that the example came from Gaussian number j, then x conditioned that z=j is drawn from a Gaussian distribution with some mean $\\mu_{j}$ and some covariance $\\Sigma_{j}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Difference between this model and Gaussian Discriminant Analysis(GDA) is:\n",
    "  - In GDA we had labeled examples $x^{(i)}, y^{(i)}$, where $y^{(i)}$ was observed. In this model, $z^{(i)}$ is hidden/unobserved.\n",
    "  - In GDA, y took one of two values. Here it is one of k values\n",
    "  - In GDA, we used one $\\Sigma$. Here we use $\\Sigma_{j}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "- If and only if we knew $z^{(i)}$, we can use MLE\n",
    "> $l(\\phi, \\mu, \\Sigma) = \\sum\\limits^{n}_{i=1}\\log p(x^{(i)}; \\phi, \\mu, \\Sigma)$\n",
    "- Maximizing this, we get\n",
    "> $\\phi_{j} = \\frac{1}{n}\\sum\\limits^{n}_{i=1}\\mathbb 1\\{z^{(i)} = j\\}$  \n",
    "> $\\mu_{j} = \\frac{\\sum^{n}_{i=1}\\mathbb 1\\{z^{(i)} = j\\}x^{(i)}}{\\sum^{n}_{i=1}\\mathbb 1\\{z^{(i)} = j\\}}$  \n",
    "> $\\Sigma_{j} = \\frac{\\sum^{n}_{i=1}\\mathbb 1\\{z^{(i)} = j\\}(x^{(i)} - \\mu_{j})(x^{(i)} - \\mu_{j})^{T}}{\\sum^{n}_{i=1}\\mathbb 1\\{z^{(i)} = j\\}}$  \n",
    "<br>\n",
    "\n",
    "- But we cannot use these parameters, as we don't know z\n",
    "\n",
    "### EM algorithm\n",
    "- 1st step - E step - we will guess the values of z\n",
    "- 2nd step - M step - we will use the guessed value of z\n",
    "- A bootstrap procedure - Iterate, update the guesses and rerun \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622c6b",
   "metadata": {},
   "source": [
    "#### E step\n",
    "- calculate posterior probability \n",
    "- $w_{j}^{(i)}$ is the strength of how much $x^{(i)}$ is assigned to that particular $\\mu_{j}$ Gaussian  \n",
    "- this is a number between 0 and 1, and the strength of all the assignments and every point is assigned a sign with a total strength equal to 1 \n",
    "- will assign 0.8 to more close Gaussian and 0.2 to a more distinct Gaussian\n",
    "> $w_{j}^{(i)} = p(z^{(i)} = j|x^{(i)}; \\phi,\\mu,\\Sigma) = \\frac{p(x^{(i)}|z^{(i)} = j; \\mu,\\Sigma) p(z^{(i)}=j; \\phi)}{\\sum_{l=1}^{k} p(x^{(i)}|z^{(i)} = l; \\mu,\\Sigma) p(z^{(i)}=l; \\phi)}$  \n",
    "- $p(x^{(i)}|z^{(i)} = j; \\mu,\\Sigma)$ is given by evaluating the density of Gaussian $N(\\mu_{j}, \\Sigma_{j})$\n",
    "> $\\frac{1}{(2\\pi)^{n/2}|\\Sigma_{j}|^{1/2}} exp\\left( -\\frac{1}{2} (x^{(i)} - \\mu_{j})^{T}\\Sigma_{j}^{-1}(x^{(i)} - \\mu_{j}) \\right)$\n",
    "- $p(z^{(i)}=j; \\phi)$ is the multinomial given by $\\phi_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4efed",
   "metadata": {},
   "source": [
    "#### M step\n",
    "- instead of using indicator function as which gaussian did data point come from, $w_{j}$ is being used which is the expected value of indicator function $E[\\mathbb 1\\{z^{(i)} = j\\}]$ which again is equal to probability of indicator function being true\n",
    "\n",
    "> $\\phi_{j} = \\frac{1}{m}\\sum\\limits^{m}_{i=1}w_{j}^{(i)}$  \n",
    "> $\\mu_{j} = \\frac{\\sum^{m}_{i=1}w_{j}^{(i)}x^{(i)}}{\\sum^{n}_{i=1}w_{j}^{(i)}}$  \n",
    "> $\\Sigma_{j} = \\frac{\\sum^{n}_{i=1}w_{j}^{(i)}(x^{(i)} - \\mu_{j})(x^{(i)} - \\mu_{j})^{T}}{\\sum^{n}_{i=1}w_{j}^{(i)}}$  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c975641",
   "metadata": {},
   "source": [
    "##### Intuition 1\n",
    "- In k-means clustering algorithm, hard way of assigning points are being used. We take each point and assign it to one of k cluster centroids. This was hard way of assignment\n",
    "- In EM algorithm, soft way of assigning points are being used. It uses probability in the form of weights, as how much is assigned to Gaussian 1 vs Gaussian 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbad5e",
   "metadata": {},
   "source": [
    "- It can fit algorithm of different type of Gaussian mixtures, which is a rich distribution \n",
    "\n",
    "> $p(x) \\ge \\epsilon$ - this is okay  \n",
    "> $p(x) \\lt \\epsilon$ - this is anomaly  \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/14_gaussianMixtures.png\" width=200 height=200>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf4cdf",
   "metadata": {},
   "source": [
    "##### Intuition 2\n",
    "- In EM algorithm, we never know what is the true value of z. We are given dataset and we only know the x's\n",
    "- In the aircraft engine example, say there are 2 plants/Gaussians from which engines come but then they are mixed. After assembling there is no certainty, the engine came from which plant. But we know that there are 2 plants. \n",
    "- E step\n",
    "  - So in every iteration of EM, we guess what are the chance of x training example/engine belonged to plant 1 and what are the chance that it belonged to plant 2. \n",
    "- M step\n",
    "  - we look at the engines which we are guessing as which process did it come from, and we update the Gaussian trying to make it a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c75f13",
   "metadata": {},
   "source": [
    "#### Jensen's inequality\n",
    "- why is EM algorithm a reasonable algorithm?\n",
    "- why is MLE expected to converge?\n",
    "\n",
    "- Prove:\n",
    "  - Let f be a convex function, (i.e., $f''(x) \\gt 0$)\n",
    "  - Let X be a random variable\n",
    "    > $f(EX) \\le E[f(x)]$\n",
    "  - This inequality always hold true for convex functions\n",
    "\n",
    "<img src=\"images/14_jensenInequality.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "- If $f''(x) \\gt 0$ (f is strictly convex), then\n",
    "  - $f(EX) = E[f(x)] \\Leftrightarrow $ X is a constant\n",
    "- A straight line is also a convex function\n",
    "- In Jensen's inequality, the equality is possible only if the random variable always take the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804d4fa",
   "metadata": {},
   "source": [
    "#### Jensen's inequality - for concave function\n",
    "\n",
    "- Negative of convex function is concave function\n",
    "- Jensen's inequality holds true for concave as well, with sign flipped\n",
    "- If $f''(x) \\lt 0$ (f is strictly concave), then\n",
    "  - $f(EX) = E[f(x)] \\Leftrightarrow $ X is a constant\n",
    "\n",
    "\n",
    "<img src=\"images/14_jensenInequalityConcave.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f3e48",
   "metadata": {},
   "source": [
    "#### EM algorithm converges to local optimum\n",
    "- training set $(x^{(1)}, x^{(2)}, ..., x^{(n)})$\n",
    "- the latent hidden variable is z\n",
    "- the density for x can be calculated by marginalizing over the latent variable z\n",
    "> $p(x; \\theta) = \\sum\\limits_{z}p(x,z;\\theta)$\n",
    "- we fit the parameters $\\theta$ by maximizing the log-likelihood of training set\n",
    "> $l(\\theta) = \\sum\\limits_{i=1}^{n}\\log p(x^{(i)}; \\theta)$\n",
    "- which in joint density term $p(x,z;\\theta)$ becomes\n",
    "> $\\begin{equation}\\\\\n",
    "\\begin{aligned}\\\\\n",
    "l(\\theta) &= \\sum\\limits_{i=1}^{n}\\log p(x^{(i)}; \\theta)\\\\\n",
    "&= \\sum\\limits_{i=1}^{n}\\log \\sum\\limits_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\theta)\\\\\n",
    "\\end{aligned}\\\\\n",
    "\\end{equation}\\\\\n",
    "$\n",
    "- maximize the log likelihood estimate $\\log\\max\\limits_{\\theta} l(\\theta)$\n",
    "\n",
    "<br>\n",
    "\n",
    "- in the graph below:\n",
    "  - the green curve is a lower bound, over all values of $\\theta$, it always remain under the blue curve\n",
    "  - the green curve is equal to the blue curve at the current value of $\\theta$\n",
    "  - 1st iteration\n",
    "    - what E step does\n",
    "      - it constructs the lower bound like green curve\n",
    "    - what M step does\n",
    "      - it takes the green curve and finds the maximum \n",
    "    - one step of EM will then move $\\theta$ from this green value to this red value\n",
    "  - 2nd iteration\n",
    "    - what E step does\n",
    "      - it takes the new $\\theta$(red) and finds a new lower bound\n",
    "    - what M step does\n",
    "      - it maximizes this red curve\n",
    "  - it converges to local optimum\n",
    "  \n",
    "\n",
    "<img src=\"images/14_EMCurve_withMinima.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fddb5",
   "metadata": {},
   "source": [
    "#### Derivation of EM algorithm\n",
    "\n",
    "<img src=\"images/14_EMDerivation3.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc4d03",
   "metadata": {},
   "source": [
    "- using concave form of Jensen's inequality\n",
    "- log is a concave function\n",
    "- this result of this function is only dependent on $\\theta$, as x's are fixed, and z is something we are summing over\n",
    "- this result is the lower bound for log-likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580adac1",
   "metadata": {},
   "source": [
    "- In this derivation, we want the Jensen's inequality to hold equality\n",
    "- and the way to do this is by making the green lower bound function/curve to be tightly bound/equal to the blue curve or tangent to each other\n",
    "- we want the left hand side of log likelihood function to be equal to the right side for the current value of $\\theta$\n",
    "\n",
    "\n",
    "<img src=\"images/14_EMDerivation4_JensenEquality.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "<img src=\"images/14_EMDerivation5_JensenEquality.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "<img src=\"images/14_EMDerivation6_JensenEquality.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
