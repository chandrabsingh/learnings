{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c188b9",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60472310",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- Assume there is house property dataset, with size and price, and goal is to have a function which can predict the price given size\n",
    "    - in supervised learning, we have a training set, which we fed to learning algorithm, whose job is to output a function h or hypothesis, which can make predictions about housing prices\n",
    "    - the job of hypothesis for a given house size, it gives price estimation \n",
    "- Question \n",
    "    - how to represent learning algorithm h?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902aef6",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "- $\\theta$ - parameters or the weights of learning algorithm parameterizing the space of linear functions mapping from $x$ to $y$\n",
    "  - choose $\\theta$ such that $h(x) \\approx y$ for training examples\n",
    "- $x_{j}^{(i)}$ - inputs/features - $j^{th}$ training example of ith feature in the training set ( a bit confusing with i/j and subscript/superscript)\n",
    "  - input features \n",
    "    - $x_{1}$ - size of house\n",
    "    - $x_{2}$ - # of bedroom\n",
    "  - Weight vector $\\begin{equation*}\n",
    "\\theta   = \n",
    "\\begin{bmatrix}\n",
    "\\theta_{0}  \\\\\n",
    "\\theta_{1}  \\\\\n",
    "\\theta_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "  - Feature vector $\\begin{equation*}\n",
    "x   = \n",
    "\\begin{bmatrix}\n",
    "x_{0}  \\\\\n",
    "x_{1}  \\\\\n",
    "x_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$, where $x_{0}$ = 1\n",
    "\n",
    "- $y$ - output\n",
    "- $(x, y)$ - one  training example\n",
    "- $(x^{(i)}, y^{(i)})$ - $i^{th}$  training example\n",
    "- $h_{\\theta}(x)$ - hypothesis\n",
    "  - hypothesis depends on both the parameters $\\theta$ and the input features $x$\n",
    "> $$h_{\\theta}(x) = \\sum\\limits_{i=0}^{n}\\theta_{i}x_{i} = \\theta^{T}x$$\n",
    "- $J(\\theta)$ - cost function\n",
    "  - choose values of $\\theta$ so that the equation below is minimized\n",
    "  - adding 1/2 makes math a little bit simpler\n",
    "  - why squared error?\n",
    "    - will talk about it during generalized linear models (GLM)\n",
    "> $$ J(\\theta) = \\frac{1}{2}\\sum\\limits_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$\n",
    "\n",
    "- m - number of training examples (# of rows)\n",
    "- n - number of features\n",
    "- number of dimensions is n+1 because of constant \n",
    "\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff777323",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- find an algorithm that minimizes the cost function\n",
    "\n",
    "- _generally speaking if you run gradient descents on linear regression, we don't end up with local optimum_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56af61",
   "metadata": {},
   "source": [
    "#### Least Mean Squares (LMS) algorithm\n",
    "\n",
    "* horizontal axis - $ \\theta _{0} \\space and \\space \\theta _{1} $   \n",
    "* vertical axis - look all around you - keep changing $ \\theta $ to reduce the $ J(\\theta) $\n",
    "  * $ J(\\theta) $ - cost function\n",
    "* start with some $ \\theta $ (say $ \\theta = \\overrightarrow{\\rm 0} $ )\n",
    "* keep changing/moving till you reach at the global minima and not local minima  \n",
    "  * <i> Use $ := $ sign for assignments </i>\n",
    "* $ \\theta _{j} := \\theta _{j} - \\alpha \\frac{\\partial}{\\partial \\theta _{j}}J(\\theta) $ - __Eq 1__\n",
    "  * $ \\alpha $ is the learning rate\n",
    "  * for each value of $ j = 0,1,2,..n $, for n features\n",
    "    * <i> $ a := a + 1 $ - means increment the value of a and assign it to a </i>\n",
    "    * <i> $ a = b $ - means that is an assertment that it is a fact that a is equal to b </i>\n",
    "* derivative of function defines the direction of the gradient \n",
    "* assuming for 1 training example..  \n",
    "> $ \\frac{\\partial}{\\partial \\theta _{j}} J(\\theta) $   \n",
    "> $ = \\frac{\\partial}{\\partial \\theta _{j}} \\frac{1}{2} (h _{\\theta}(x) - y)^{2} $   \n",
    "> $ = (h _{\\theta}(x) - y) \\frac{\\partial}{\\partial \\theta _{j}} (\\theta _{0} x _{0} + \\theta _{1} x _{1} + .. \\theta _{n} x _{n} - y) $   \n",
    "* partial derivative of every term will be 0 other than $ \\theta _{j} $ term, which resolves into following  \n",
    "$ = (h_{\\theta}(x) - y).x_{j} $  \n",
    "* substituting in Eq1  \n",
    "> $ \\theta _{j} := \\theta _{j} - \\alpha (h_{\\theta}(x) - y).x_{j} $  \n",
    "\n",
    "* for __\"m training example\"__, the above results in:  \n",
    "$ \\theta _{j} := \\theta _{j} - \\alpha \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}).x_{j}^{(i)} $  - __Eq 2__ \n",
    "* all that is done, that sum over all m training examples, where $(i)$ is the $ i^{th} $ training example.  \n",
    "* Gradient descent algorithm is to be repeated till it convergences\n",
    "  * for each value of $ j = 0,1,2,..n $, for __\"n features\"__ ( in this example it's 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aacc9c",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "\n",
    "* $J(\\theta)$ has no local optima, it has only global optimum\n",
    "* other way to look into the cost function is to look at the contours of this curve - ellipses\n",
    "    * if GD is run on this\n",
    "    * if \\alpha is too large - it will overshoot\n",
    "    * if you look into the contours, the direction of steepest descent is always orthogonal to contour direction\n",
    "    * try a few values, to \n",
    "    * If cost function is increasing, it indicates that the learning rate is too large\n",
    "    * try few values at exponential rate, 0.02, 0.04, 0.08, 0.16, .. - which tells you the direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80cac",
   "metadata": {},
   "source": [
    "#### Convergence\n",
    "\n",
    "* training egs here are 49\n",
    "* initially hypothesis - $ \\theta _{0} \\space and \\space \\theta _{1} $ are assigned the value 0 - the cost function result will be too high\n",
    "* after each iteration hypothesis, the cost function is minimized by the gradiend descent algorithm\n",
    "* eventually it converges\n",
    "<br>\n",
    "\n",
    "- Iteration - Base\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 0\n",
    "<img src=\"./images/02_housingPrices_sqft0.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 1\n",
    "<img src=\"./images/02_housingPrices_sqft1.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 2\n",
    "<img src=\"./images/02_housingPrices_sqft2.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration n\n",
    "<img src=\"./images/02_housingPrices_sqftn.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86304d",
   "metadata": {},
   "source": [
    "##### Question 1\n",
    "Why is negative $\\alpha$ multiplied to the gradient descent, instead of positive $\\alpha$ ?\n",
    "* because you will go uphill the gradient descent instead of going downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bedf34",
   "metadata": {},
   "source": [
    "##### Question 2\n",
    "When do you stop\n",
    "* Plot $J(\\theta)$ over time\n",
    "* linear regression does not have local minima, so you will not have the problem of convergence\n",
    "* but training nonlinear like neural network will have such acute problem of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa50705",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "- we look into data in batches - for example in this case, there is a batch of 49 training example\n",
    "- disadv\n",
    "  - if we have a large dataset, inorder to make one single step of gd, we will have to calculate the sum of __Eq 2__ above\n",
    "  - if m is 1M, to make one step we will have to iterate over 1M times\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "- Alternative to Batch GD\n",
    "- instead of scaning through 1M training examples, we loop over i (features) to update for all j from 1 to n, using 1 training example\n",
    "- this never truely converge\n",
    "- but makes very faster progress\n",
    "- Mini-Batch Gradient Descent\n",
    "\n",
    "<img src=\"./images/02_sgd.png\" width=400 height=400 />  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718d33b",
   "metadata": {},
   "source": [
    "### Normal Equation \n",
    "- If our goal is to solve linear regression, we dont need to run the process iteratively\n",
    "- we can solve for the optimal value of parameter $\\theta$ straight-away\n",
    "- this works only for linear regression and not anything else\n",
    "<br>\n",
    "\n",
    "- Partial derivative of cost function\n",
    "  - $ \\nabla_{\\theta} J(\\theta) $ - derivative of $J(\\theta)$ wrt to $\\theta$, where $\\theta \\in \\mathbb R ^{n+1}$. In our case with $ \\theta _{0} \\space, \\theta _{1} and \\space \\theta _{2} $, we have 3 dimensions of $\\mathbb R$, i.e., $\\theta \\in \\mathbb R^{n+1}$  \n",
    "\n",
    "> $\\begin{equation*}\n",
    "\\nabla_{\\theta} J(\\theta)  = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\theta_{0}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{1}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{2}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f91e70",
   "metadata": {},
   "source": [
    "#### How to find the global minima\n",
    "\n",
    "> $$ \\nabla_{\\theta} J(\\theta) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $$  \n",
    "- solving this gives you a global minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663d75",
   "metadata": {},
   "source": [
    "#### Matrix trace properties:\n",
    "* tr(A) = tr A = sum of diagonal entries = $\\sum_{i}A_{ii}$\n",
    "* $tr A = tr A^{T}$\n",
    "* If $ f(A) = tr AB $, then $ \\nabla _{A} f(A) = B^{T}$\n",
    "* tr AB = tr BA \n",
    "* tr ABC = tr CAB - by cyclic permutation property\n",
    "* $ \\nabla _{A} tr AA^{T}C = CA + C^{T}A $\n",
    "  * The above is analogous to $\\frac{d}{da}a^{2}c = 2ac $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b3a5a",
   "metadata": {},
   "source": [
    "#### How to solve for $\\theta$\n",
    "\n",
    "- Let the cost function \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "\n",
    "- and design matrix \n",
    "> $$\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "--(X^{(1)})^{T}--  \\\\\n",
    "--(X^{(2)})^{T}--  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "--(X^{(m)})^{T}--  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- and \n",
    "$$\\overrightarrow{\\rm y} = \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}  \\\\\n",
    "y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "y^{(m)}  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- then  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta = \n",
    "\\begin{bmatrix}\n",
    "(X^{(1)})^{T}\\theta  \\\\\n",
    "(X^{(2)})^{T}\\theta  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "(X^{(m)})^{T}\\theta \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)})  \\\\\n",
    "h_{\\theta}(X^{(2)})  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)})  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- Sum of all the errors the algorithm is making between prediction and actual for m training examples  \n",
    "= Sum of the residuals =  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta - y = \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)}) - y^{(1)}  \\\\\n",
    "h_{\\theta}(X^{(2)}) - y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)}) - y^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- So, we can write:  \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "> $$ = \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e2e28",
   "metadata": {},
   "source": [
    "- Substituting\n",
    "$ \\nabla_{\\theta} J(\\theta)  \\\\\n",
    "= \\nabla_{\\theta} \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T} - y^{T})(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T}X\\theta - \\theta^{T}X^{T}y - y^{T}X\\theta + y^{T}y)  \\\\\n",
    "$ using matrix derivative $ \\\\\n",
    "= \\frac{1}{2} (X^{T}X\\theta + X^{T}X\\theta - X^{T}y - X^{T}y) \\\\\n",
    "= (X^{T}X\\theta - X^{T}y) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $  \n",
    "which results in  \n",
    "$ X^{T}X\\theta = X^{T}y $ - which is called __\"Normal equation\"__  \n",
    "> $ \\theta = (X^{T}X)^{-1}X^{T}y $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eed2f5",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Implement GD/SGD/MBGD with Keras/Tensorflow/PyTorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf7f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 152.91886182616167\n",
      "Coefficients: \n",
      " [938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Coefficient of determination: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", regr.intercept_)\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8282fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.5349\n",
      "mean_squared_log_error:  0.2171\n",
      "r2:  0.4726\n",
      "MAE:  41.2271\n",
      "MSE:  2548.0724\n",
      "RMSE:  50.4784\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "regression_results(diabetes_y_test, diabetes_y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
