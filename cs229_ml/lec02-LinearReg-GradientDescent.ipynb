{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c188b9",
   "metadata": {},
   "source": [
    ">>> Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60472310",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- Assume there is house property dataset, with size and price, and goal is to have a function which can predict the price given size\n",
    "    - in supervised learning, we have a training set, which we fed to learning algorithm, whose job is to output a function h or hypothesis, which can make predictions about housing prices\n",
    "    - the job of hypothesis for a given house size, it gives price estimation \n",
    "- Question \n",
    "    - how to represent learning algorithm h?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902aef6",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "- $\\theta$ - parameters or the weights of learning algorithm parameterizing the space of linear functions mapping from $x$ to $y$\n",
    "  - choose $\\theta$ such that $h(x) \\approx y$ for training examples\n",
    "- $x_{j}^{(i)}$ - inputs/features - $j^{th}$ training example of ith feature in the training set ( a bit confusing with i/j and subscript/superscript)\n",
    "  - input features \n",
    "    - $x_{1}$ - size of house\n",
    "    - $x_{2}$ - # of bedroom\n",
    "  - Weight vector $\\begin{equation*}\n",
    "\\theta   = \n",
    "\\begin{bmatrix}\n",
    "\\theta_{0}  \\\\\n",
    "\\theta_{1}  \\\\\n",
    "\\theta_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "  - Feature vector $\\begin{equation*}\n",
    "x   = \n",
    "\\begin{bmatrix}\n",
    "x_{0}  \\\\\n",
    "x_{1}  \\\\\n",
    "x_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$, where $x_{0}$ = 1\n",
    "\n",
    "- $y$ - output\n",
    "- $(x, y)$ - one  training example\n",
    "- $(x^{(i)}, y^{(i)})$ - $i^{th}$  training example\n",
    "- $h_{\\theta}(x)$ - hypothesis\n",
    "  - hypothesis depends on both the parameters $\\theta$ and the input features $x$\n",
    "> $$h(x) = \\sum\\limits_{i=0}^{n}\\theta_{i}x_{i} = \\theta^{T}x$$\n",
    "- $J(\\theta)$ - cost function\n",
    "  - choose values of $\\theta$ so that the equation below is minimized\n",
    "  - adding 1/2 makes math a little bit simpler\n",
    "  - why squared error?\n",
    "    - will talk about it during generalized linear models (GLM)\n",
    "> $$ J(\\theta) = \\frac{1}{2}\\sum\\limits_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$\n",
    "\n",
    "- m - number of training examples (# of rows)\n",
    "- n - number of features\n",
    "- number of dimensions is n+1 because of constant \n",
    "\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff777323",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- find an algorithm that minimizes the cost function\n",
    "\n",
    "- _generally speaking if you run gradient descents on linear regression, we don't end up with local optimum_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56af61",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "\n",
    "* horizontal axis - $ \\theta _{0} \\space and \\space \\theta _{1} $   \n",
    "* vertical axis - look all around you - keep changing $ \\theta $ to reduce the $ J(\\theta) $\n",
    "  * $ J(\\theta) $ - cost function\n",
    "* start with some $ \\theta $ (say $ \\theta = \\overrightarrow{\\rm 0} $ )\n",
    "* keep changing/moving till you reach at the global minima and not local minima  \n",
    "  * <i> Use $ := $ sign for assignments </i>\n",
    "* $ \\theta _{j} := \\theta _{j} - \\alpha \\frac{\\partial}{\\partial \\theta _{j}}J(\\theta) $ - __Eq 1__\n",
    "  * $ \\alpha $ is the learning rate\n",
    "  * for each value of $ j = 0,1,2,..n $, for n features\n",
    "    * <i> $ a := a + 1 $ - means increment the value of a and assign it to a </i>\n",
    "    * <i> $ a = b $ - means that is an assertment that it is a fact that a is equal to b </i>\n",
    "* derivative of function defines the direction of the gradient \n",
    "* assuming for 1 training example..  \n",
    "> $ \\frac{\\partial}{\\partial \\theta _{j}} J(\\theta) $   \n",
    "> $ = \\frac{\\partial}{\\partial \\theta _{j}} \\frac{1}{2} (h _{\\theta}(x) - y)^{2} $   \n",
    "> $ = (h _{\\theta}(x) - y) \\frac{\\partial}{\\partial \\theta _{j}} (\\theta _{0} x _{0} + \\theta _{1} x _{1} + .. \\theta _{n} x _{n} - y) $   \n",
    "* partial derivative of every term will be 0 other than $ \\theta _{j} $ term, which resolves into following  \n",
    "$ = (h_{\\theta}(x) - y).x_{j} $  \n",
    "* substituting in Eq1  \n",
    "$ \\theta _{j} := \\theta _{j} - \\alpha (h_{\\theta}(x) - y).x_{j} $  \n",
    "\n",
    "* for __\"m training example\"__, the above results in:  \n",
    "$ \\theta _{j} := \\theta _{j} - \\alpha \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}).x_{j}^{(i)} $  - __Eq 2__ \n",
    "* all that is done, that sum over all m training examples, where $(i)$ is the $ i^{th} $ training example.  \n",
    "* Gradient descent algorithm is to be repeated till it convergences\n",
    "  * for each value of $ j = 0,1,2,..n $, for __\"n features\"__ ( in this example it's 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aacc9c",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "\n",
    "* $J(\\theta)$ has no local optima, it has only global optimum\n",
    "* other way to look into the cost function is to look at the contours of this curve - ellipses\n",
    "    * if GD is run on this\n",
    "    * if \\alpha is too large - it will overshoot\n",
    "    * if you look into the contours, the direction of steepest descent is always orthogonal to contour direction\n",
    "    * try a few values, to \n",
    "    * If cost function is increasing, it indicates that the learning rate is too large\n",
    "    * try few values at exponential rate, 0.02, 0.04, 0.08, 0.16, .. - which tells you the direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80cac",
   "metadata": {},
   "source": [
    "#### Convergence\n",
    "\n",
    "* training egs here are 49\n",
    "* initially hypothesis - $ \\theta _{0} \\space and \\space \\theta _{1} $ are assigned the value 0 - the cost function result will be too high\n",
    "* after each iteration hypothesis, the cost function is minimized by the gradiend descent algorithm\n",
    "* eventually it converges\n",
    "<br>\n",
    "\n",
    "- Iteration - Base\n",
    "<img src=\"./images/02_housingPrices_sqft.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 0\n",
    "<img src=\"./images/02_housingPrices_sqft0.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 1\n",
    "<img src=\"./images/02_housingPrices_sqft1.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration 2\n",
    "<img src=\"./images/02_housingPrices_sqft2.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n",
    "\n",
    "- Iteration n\n",
    "<img src=\"./images/02_housingPrices_sqftn.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86304d",
   "metadata": {},
   "source": [
    "##### Question 1\n",
    "Why is negative $\\alpha$ multiplied to the gradient descent, instead of positive $\\alpha$ ?\n",
    "* because you will go uphill the gradient descent instead of going downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bedf34",
   "metadata": {},
   "source": [
    "##### Question 2\n",
    "When do you stop\n",
    "* Plot $J(\\theta)$ over time\n",
    "* linear regression does not have local minima, so you will not have the problem of convergence\n",
    "* but training nonlinear like neural network will have such acute problem of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa50705",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "- we look into data in batches - for example in this case, there is a batch of 49 training example\n",
    "- disadv\n",
    "  - if we have a large dataset, inorder to make one single step of gd, we will have to calculate the sum of __Eq 2__ above\n",
    "  - if m is 1M, to make one step we will have to iterate over 1M times\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "- Alternative to Batch GD\n",
    "- instead of scaning through 1M training examples, we loop over i (features) to update for all j from 1 to n, using 1 training example\n",
    "- this never truely converge\n",
    "- but makes very faster progress\n",
    "- Mini-Batch Gradient Descent\n",
    "\n",
    "<img src=\"./images/02_sgd.png\" width=400 height=400 />\n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718d33b",
   "metadata": {},
   "source": [
    "### Normal Equation \n",
    "- If our goal is to solve linear regression, we dont need to run the process iteratively\n",
    "- we can solve for the optimal value of parameter $\\theta$ straight-away\n",
    "- this works only for linear regression and not anything else\n",
    "<br>\n",
    "\n",
    "- Partial derivative of cost function\n",
    "  - $ \\nabla_{\\theta} J(\\theta) $ - derivative of $J(\\theta)$ wrt to $\\theta$, where $\\theta \\in \\mathbb R ^{n+1}$. In our case with $ \\theta _{0} \\space, \\theta _{1} and \\space \\theta _{2} $, we have 3 dimensions of $\\mathbb R$, i.e., $\\theta \\in \\mathbb R^{n+1}$  \n",
    "\n",
    "> $\\begin{equation*}\n",
    "\\nabla_{\\theta} J(\\theta)  = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\theta_{0}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{1}}  \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{2}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f91e70",
   "metadata": {},
   "source": [
    "#### How to find the global minima\n",
    "\n",
    "> $$ \\nabla_{\\theta} J(\\theta) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $$  \n",
    "- solving this gives you a global minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663d75",
   "metadata": {},
   "source": [
    "#### Matrix trace properties:\n",
    "* tr(A) = tr A = sum of diagonal entries = $\\sum_{i}A_{ii}$\n",
    "* $tr A = tr A^{T}$\n",
    "* If $ f(A) = tr AB $, then $ \\nabla _{A} f(A) = B^{T}$\n",
    "* tr AB = tr BA \n",
    "* tr ABC = tr CAB - by cyclic permutation property\n",
    "* $ \\nabla _{A} tr AA^{T}C = CA + C^{T}A $\n",
    "  * The above is analogous to $\\frac{d}{da}a^{2}c = 2ac $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b3a5a",
   "metadata": {},
   "source": [
    "#### How to solve for $\\theta$\n",
    "\n",
    "- Let the cost function \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "\n",
    "- and design matrix \n",
    "> $$\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "--(X^{(1)})^{T}--  \\\\\n",
    "--(X^{(2)})^{T}--  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "--(X^{(m)})^{T}--  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- and \n",
    "$$\\overrightarrow{\\rm y} = \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}  \\\\\n",
    "y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "y^{(m)}  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "- then  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta = \n",
    "\\begin{bmatrix}\n",
    "(X^{(1)})^{T}\\theta  \\\\\n",
    "(X^{(2)})^{T}\\theta  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "(X^{(m)})^{T}\\theta \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)})  \\\\\n",
    "h_{\\theta}(X^{(2)})  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)})  \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- Sum of all the errors the algorithm is making between prediction and actual for m training examples  \n",
    "= Sum of the residuals =  \n",
    "> $$\\begin{equation*}\n",
    "X \\theta - y = \n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(X^{(1)}) - y^{(1)}  \\\\\n",
    "h_{\\theta}(X^{(2)}) - y^{(2)}  \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "h_{\\theta}(X^{(m)}) - y^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- So, we can write:  \n",
    "> $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$  \n",
    "> $$ = \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e2e28",
   "metadata": {},
   "source": [
    "- Substituting\n",
    "$ \\nabla_{\\theta} J(\\theta)  \\\\\n",
    "= \\nabla_{\\theta} \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T} - y^{T})(X\\theta - y)  \\\\\n",
    "= \\frac{1}{2} \\nabla_{\\theta} (\\theta^{T}X^{T}X\\theta - \\theta^{T}X^{T}y - y^{T}X\\theta + y^{T}y)  \\\\\n",
    "$ using matrix derivative $ \\\\\n",
    "= \\frac{1}{2} (X^{T}X\\theta + X^{T}X\\theta - X^{T}y - X^{T}y) \\\\\n",
    "= (X^{T}X\\theta - X^{T}y) \\stackrel{set}{=} \\overrightarrow{\\rm 0} $  \n",
    "which results in  \n",
    "$ X^{T}X\\theta = X^{T}y $ - which is called __\"Normal equation\"__  \n",
    "> $ \\theta = (X^{T}X)^{-1}X^{T}y $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eed2f5",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Implement GD/SGD/MBGD with Keras/Tensorflow/PyTorch  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
