{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecedb2bd",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ddae7",
   "metadata": {},
   "source": [
    "# Lecture Notes\n",
    "\n",
    "#### Outline:\n",
    "- Naive Bayes\n",
    "  - Laplace smoothing\n",
    "  - Element models\n",
    "- Comments on applying ML\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40592b63",
   "metadata": {},
   "source": [
    "## Naive Bayes \n",
    "- is a generative type of algorithm\n",
    "- To build generative model, P(x|y) and P(y) needs to be modeled. Gaussian discriminative model uses Gaussian and Bernoulli respectively to model this. Naive Bayes uses a different model. \n",
    "> - $ x_{j} = \\mathbb 1$ {indicator - word j appears in email or not}  \n",
    "> - $ P(x|y) = \\prod\\limits_{j=1}^{n}P(x_{j}|y)$ - NB uses the product of conditional probabilities of individual features given in the class label y   \n",
    "- Parameters of NB model are:    \n",
    "  >> - $P(y=1) = \\phi_{y} $ - the class prior for y=1 \n",
    "  >> - $P(x_{j}=1|y=0) = \\phi_{j|y=0}$ - chances of the word appearing in non-spam email.    \n",
    "  >> - $P(x_{j}=1|y=1) = \\phi_{j|y=1}$ - chances of the word appearing in spam email.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a8c47",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimate  \n",
    "> $\\phi_{y} = \\frac{\\sum\\limits_{i=1}^{m}\\mathbb 1 \\{y^{(i)}=1\\}}{m}$  \n",
    "> $\\phi_{j|y=0} = \\frac{\\sum\\limits_{i=1}^{m}\\mathbb 1 \\{x_{j}^{(i)}=1, y^{(i)}=0\\}}{\\sum\\limits_{i=1}^{m}\\mathbb 1 \\{y^{(i)}=0\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ab78b",
   "metadata": {},
   "source": [
    "### Laplace smoothing\n",
    "- ML conference papers - NIPS - Neural Information Processing Systems\n",
    "\n",
    "- __NB breaks__ - because the probability of some event that you have not seen is trained as 0, which is statistically wrong\n",
    "- the classifier gives wrong result, when it gets such event for the first time\n",
    "  - instead using __Laplace smoothing__ helps this problem\n",
    "    - add 1 each to pass and fail scenario\n",
    "    \n",
    "#### Maximum likelihood estimate  \n",
    "> $\\phi_{x=i} = \\frac{\\sum\\limits_{j=1}^{m}\\mathbb 1 \\{x^{(i)}=j\\} + 1} {m + k}$  \n",
    "  - k is the size of dictionary\n",
    "> $\\phi_{i|y=0} = \\frac{\\sum\\limits_{i=1}^{m}\\mathbb 1 \\{x_{j}^{(i)}=1, y^{(i)}=0\\} + 1}{\\sum\\limits_{i=1}^{m}\\mathbb 1 \\{y^{(i)}=0\\} + 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1eeea",
   "metadata": {},
   "source": [
    "## Event models for text classification\n",
    "- The functionality can be generalized from binary to multinomial features instead \n",
    "  - Multivariate Bernoulli event\n",
    "  - Multinomial event\n",
    "  \n",
    "- We can discretize the size of house and transform a Bernoulli event to Multinomial event\n",
    "  - a rule of thumb is to discretize variables into 10 buckets \n",
    "\n",
    "\n",
    "<img src=\"images/06_multinomialEventSqFt.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8447e2",
   "metadata": {},
   "source": [
    "## NB variation\n",
    "- for text classification\n",
    "  - if the email contains text \"Drugs! Buy drugs now\", \n",
    "    - the feature vector will make binary vector of all words appearing in the email\n",
    "    - it will lose the information that the word \"drug\" appeared twice\n",
    "    - each featture only stores information $x_{j} \\in \\{0,1\\}$\n",
    "    - instead of making a feature vector of 10000 words of dictionary, we can make a feature vector of 4 words(as above) holding word index and $x_{j} \\in \\{1, 2, ... ,10000\\}$\n",
    "      - algorithm containing feature vector of 10000 is called __\"Multivariate Bernoulli algorithm\"__\n",
    "      - algorithm containing feature vector of 4 is called __\"Multinomial algorithm\"__\n",
    "        - Andrew McCallum used these 2 names\n",
    "\n",
    "\n",
    "\n",
    "### NB advantage\n",
    "- quick to implement\n",
    "- computationally efficient\n",
    "- no need to implement gradient descent\n",
    "- easy to do a quick and dirty type of work\n",
    "- SVM or logistic regression does better work in classification problems\n",
    "- NB or GDA does not result in very good classification, but is very quick to implement, it is quick to train, it is non-iterative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921863f",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "- find classification in the form of non linear decision boundaries\n",
    "- SVM does not have many parameters to fiddle with\n",
    "  - they have very robust packages\n",
    "  - and does not have parameters like learning rate, etc to fiddle with\n",
    "  \n",
    "<br>\n",
    "\n",
    "- Let the non-linear feature variables be mapped in vector form as below. This non-linear function can be viewed as a linear function over the variables $\\phi(x)$\n",
    "- derive an algorithm that can take input features of the $x_{1}, x_{2}$ and map them to much higher dimensional set of feature and then apply linear classifier, similar to logistic regression but different in details. This allows us to learn very non-linear decision boundaries.  \n",
    "\n",
    "<img src=\"images/06_svmBoundary.png\" width=200 height=200>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "\n",
    "### Outline for SVM\n",
    "- Optimal margin classifier (separable case)\n",
    "\n",
    "- Kernel\n",
    "  - Kernels will allow us to fit in a large set of features\n",
    "  - How to take feature vector say $\\mathbb R^{2}$ and map it to $\\mathbb R^{5}$ or $\\mathbb R^{10000}$ or $\\mathbb R^{\\inf}$, and train the algorithm to this higher feature. Map 2-dimensional feature space to infinite dimensional set of features. What it helps us is relieve us from lot of burden of manually picking up features. ($x^{2}, \\space x^{3}, \\space \\sqrt{x}, \\space x_{1}x_{2}^{2/3}, ...$)\n",
    "\n",
    "- Inseparable case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f8000",
   "metadata": {},
   "source": [
    "### Optimal margin classifier (separable case)\n",
    "- Separable case means data is separable\n",
    "\n",
    "#### Functional margin of classifier\n",
    "\n",
    "- How confidently and accurately do you classify an example\n",
    "- Using binary classification and logistic regression\n",
    "- In logistic classifier:\n",
    "> $h_{\\theta}(x) = g(\\theta^{T}x)$\n",
    "- If you turn this into binary classification, if you have this algorithm predict not a probability but predict 0 or 1, \n",
    "> - Predict \"1\", if $\\theta^{T}x > 0 \\implies h_{\\theta}(x) = g(\\theta^{T}x) \\ge 0.5$)  \n",
    "> - Predict \"0\", otherwise\n",
    "  - If $y^{(i)} = 1$, hope that $\\theta^{T}x^{(i)} \\gg 0$\n",
    "  - If $y^{(i)} = 0$, hope that $\\theta^{T}x^{(i)} \\ll 0$\n",
    "    - this implies that the prediction is very correct and accurate \n",
    "\n",
    "#### Geometric margin\n",
    "- assuming data is linearly separable, \n",
    "- and there are two lines that separates the true and false variables\n",
    "- the line that has a bigger separation or geometric margin meaning a physical separation from the training examples is a better choice\n",
    "\n",
    "- We need to prove the optimal classifier is the algorithm that tries to maximize the geometric margin\n",
    "- what SVM and low dimensional classifier do is pose an optimization problem to try and find the line that classify these examples to find bigger separation margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50776a",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "- Labels: $y \\in \\{-1, +1\\} $ to denote class labels\n",
    "- SVM will generate hypothesis output value as {-1, +1} instead of probability as in logistic regression\n",
    "- g(z) = $\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      +1 & \\text{if z $\\ge$ 0}\\\\\n",
    "      -1 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$\n",
    "  - instead of smooth transition, here we have a hard transition\n",
    "\n",
    "\n",
    "Previously in logistic regression:\n",
    "> $h_{\\theta}(x) = g(\\theta^{T}x)$\n",
    "- where x is $\\mathbb R^{n+1} $ and $x_{0} = 1$\n",
    "\n",
    "In SVM:\n",
    "> $h_{W,b}(x) = g(W^{T}x + b)$\n",
    "- where x is $\\mathbb R^{n} $ and b is $\\mathbb R $ and drop $x_{0}=1$\n",
    "- The term \n",
    "\n",
    "Other way to think about is:  \n",
    "> $\n",
    "\\begin{bmatrix}\n",
    "\\theta_{0}\\\\\n",
    "\\theta_{1}\\\\\n",
    "\\theta_{2}\\\\\n",
    "\\theta_{3}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- where $\\theta_{0}$ corresponds to b and $\\theta_{1}, \\theta_{2}, \\theta_{3}$ corresponds to W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e73791",
   "metadata": {},
   "source": [
    "#### Functional margin of hyperplane (cont)\n",
    "- Functional margin __wrt single training example__\n",
    "  - Functional margin of hyperplane defined by $(w, b)$ wrt a single training example $(x^{(i)}, y^{(i)})$ is\n",
    "  > $\\hat{\\gamma}^{(i)} = y^{(i)} (w^{T}x^{(i)} + b)$\n",
    "    - If $y^{(i)} = +1$, hope that $(w^{T}x^{(i)} + b) \\gg 0$\n",
    "    - If $y^{(i)} = -1$, hope that $(w^{T}x^{(i)} + b) \\ll 0$\n",
    "  - Combining these two statements:\n",
    "    - we hope that $\\hat{\\gamma}^{(i)} \\gg 0$\n",
    "  - If $\\hat{\\gamma}^{(i)} \\gt 0$, \n",
    "    - implies $h(x^{(i)}) = y^{(i)}$\n",
    "  - In logistic regression \n",
    "    - $\\gt 0$ means the prediction is atleast a little bit above 0.5 or little bit below 0.5\n",
    "    - $\\gg 0$ means the prediction is either very close to 1 or very close to 0\n",
    "- Functional margin __wrt entire training set__ (how well are you doing on the worst example in your training set)  \n",
    "  > $\\hat{\\gamma} = \\min\\limits_{i} \\hat{\\gamma}^{(i)}$\n",
    "  - where i = 1,..,m - all the training examples\n",
    "  - here the assumption is training set is linearly separable\n",
    "    - we can assume this kind of worst-case notion because we are assuming that the boundary  is linearly separable\n",
    "  - we can normalize (w, b), which does not change the classification boundary, it simply rescales the parameters\n",
    "  > $(w, b) \\rightarrow (\\frac{w}{\\Vert w \\Vert}, \\frac{b}{\\Vert b \\Vert})$\n",
    "  - classification remains the same, with any rescale number\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb90921",
   "metadata": {},
   "source": [
    "#### Geometric margin of hyperplane (cont)\n",
    "- Geometric margin __wrt single training example__\n",
    "  - Upper half of hyperplane has positive cases and the lower half negative cases\n",
    "  - Let $(x^{(i)}, y^{(i)})$ be a training example which the classifier is classifying correctly, by predicting it as $h_{w,b}(x) = +1$. It predicts the lower half as $h_{w,b}(x) = -1$.\n",
    "  - Let the distance from the plane to the training example be the geometric margin\n",
    "  - Geometric margin of hyperplane (w,b) wrt $(x^{(i)}, y^{(i)})$ be\n",
    "  > $\\hat{\\gamma} = \\frac{(y^{(i)})(w^{T}x^{(i)} + b)}{\\Vert w \\Vert}$\n",
    "    - This is the Euclidean distance between training example and decision boundary\n",
    "    - For positive examples, $y^{(i)}$ is +1 and the equation reduces to\n",
    "    > $\\hat{\\gamma} = \\frac{(w^{T}x^{(i)} + b)}{\\Vert w \\Vert}$\n",
    "  \n",
    "\n",
    "<img src=\"images/06_geometricMargin1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16aea4",
   "metadata": {},
   "source": [
    "#### Relationship between functional and geometric margin\n",
    "> $\\gamma^{(i)} = \\frac{\\hat{\\gamma}^{(i)}}{\\Vert w \\Vert}$  \n",
    "> i.e., $\\text{Geometric Margin} = \\frac{\\text{Functional Margin}}{\\text{Norm of w}}$\n",
    "\n",
    "#### Geometric margin of hyperplane (cont)\n",
    "- Geometric margin __wrt entire training example__\n",
    "  > $\\gamma = \\min\\limits_{i} \\gamma^{(i)}$\n",
    "  \n",
    "#### Optimal margin classifier (cont)\n",
    "- what the optimal margin classifier does is to choose w,b to maximize $\\gamma$\n",
    "> $\\max_{\\gamma, w, b} \\text{s.t.} \\frac{(y^{(i)})(w^{T}x^{(i)} + b)}{\\Vert w \\Vert} \\ge \\gamma$ for i=1,...,m\n",
    "  - subject to for every single training example  must have the geometric margin greater than or equal to gamma\n",
    "  - this causes to maximize the worst-case geometric margin\n",
    "  - this is not a convex optimization problem and cannot be solved using gradient descent or local optima\n",
    "  - but this can be re-written/reformulated into a equivalent problem which is minimizing norm of w subject to the geometric margin\n",
    "> $\\min_{w,b} \\Vert w \\Vert ^{2}   \\\\\n",
    "\\text{s.t. } y^{(i)}(w^{T}x^{(i)} + b) \\ge 1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
