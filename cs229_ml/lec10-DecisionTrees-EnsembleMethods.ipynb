{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe1c9eb",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng/Head TA-Raphael Townshend - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ef654",
   "metadata": {},
   "source": [
    "## Lecture Notes\n",
    "\n",
    "#### Outline\n",
    "- Decision Trees\n",
    "- Ensemble Methods\n",
    "- Bagging\n",
    "- Random Forests\n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16edc21",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "- Non-linear model\n",
    "- A model is called linear if the hypothesis function is of the form $h(x) = \\theta^{T}x$ \n",
    "- Ski example - months vs latitude - when you can ski\n",
    "  - we cannot get a linear classifier or use SVM for this\n",
    "  - with decision trees you will have a very natural way of classifying this\n",
    "    - partition this into individual regions, isolating positive and negative examples\n",
    "\n",
    "#### Selecting Regions - Greedy, Top-Down, Recursive Partitioning\n",
    "- You ask question and partition the space and then iteratively keep asking new question, partitioning the space\n",
    "- Is latitude > 30\n",
    "  - Yes\n",
    "    - Is Month < 3\n",
    "      - Yes\n",
    "      - No\n",
    "  - No\n",
    "  \n",
    "- We are looking for a split function\n",
    "- Region $R_{p}$\n",
    "  - Looking for a split $S_{p}$\n",
    "    > $S_{p}(j,t) = (\\{ X|X_{j} \\lt t, X \\in R_{p}\\}, \\{ X|X_{j} \\ge t, X \\in R_{p}\\} ) = (R_{1}, R_{2})$\n",
    "    - where j is the feature number and t is the threshold \n",
    "    \n",
    "#### How to choose splits\n",
    "- isolate space of positives and negatives in this case\n",
    "- Define L(R): loss on R\n",
    "- Given C class, define $\\hat{p_{i}}$ to be the __porportion of examples__ in R that are of class C\n",
    "- Define misclassification loss of any region as \n",
    "> $L_{misclass}(R) = 1 - \\max\\limits_{C} \\hat{p}_{C}$\n",
    "  - what we are saying here is for any region that we have subdivided, we want to predict the most common class there, which is the maximum of $\\hat{p}_{C}$. The remaining is the probability of misclassification errors.\n",
    "- We want to pick a split that maximizes the decrease of loss as much as possible over parent $R_{parent}$ and children regions $R_{1}, R_{2}$\n",
    "> $\\max\\limits_{j,t} L(R_{p}) - (L(R_{1}) + L(R_{2}))$\n",
    "\n",
    "#### Why is misclassification loss the right loss \n",
    "\n",
    "\n",
    "<img src=\"images/10_misclassificationLoss.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n",
    "- We might argue that the decision boundary on right scenario is better than left, because in the right we are isolating out more positives \n",
    "\n",
    "- Loss of R1 and R2 region = 100 on right scenario\n",
    "- Loss of R1' and R2' region = 100 on left scenario\n",
    "- The loss of both parent Rp is also 100\n",
    "\n",
    "- We can see that the misclassification loss is not sensitive enough\n",
    "  - its not sensitive enough or the loss is not informative enough because the parent level loss is same as child level loss\n",
    "\n",
    "- Instead we can define __cross entropy loss__\n",
    "> $L_{cross}(R) = - \\sum\\limits_{c}\\hat{p}_{c} log_{2}\\hat{p}_{c}$\n",
    "  - we are summing over the classes the proportion of elements in that class times the log of proportion in that class\n",
    "  - if we know everything about one class, we dont need to communicate, as we know everything that it's a 100% chance that it is of one class\n",
    "  - if we have a even split, then we need to communicate lot more information about the class\n",
    "  \n",
    "- Cross entropy came from information theory where it is used for transmitting bits, where you can transmit bits of information, which is why it came up as log base 2\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062dcc11",
   "metadata": {},
   "source": [
    "#### Misclassification loss vs Cross-entropy loss\n",
    "- Let the plot be between $\\hat{p}$ - the proportion of positives in the set vs the loss  \n",
    "- the cross-entropy loss is a strictly concave curve\n",
    "- Let $L(R_{1})$ and $L(R_{2})$ be the child loss plotted on the curve\n",
    "- Let there be equal number of examples in both $R_{1}$ and $R_{2}$, are equally weighted \n",
    "- the overall loss between the two is the average loss between the two, which is $\\frac{L(R_{1}) + L(R_{2})}{2}$ \n",
    "- the parent node loss is the projected loss on the curve $L(R_{p})$\n",
    "- the projection height is the change in loss \n",
    "\n",
    "- as we see below, \\hat{p} parent is the average of child proportions\n",
    "\n",
    "<img src=\"images/10_crossEntropyLoss.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n",
    "- the cross-entropy diagram\n",
    "\n",
    "<img src=\"images/10_crossEntropyDiagram.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecbab7",
   "metadata": {},
   "source": [
    "- the misrepresenstation loss\n",
    "  - if we end up with child node loss on the same side of the curve, there is no change in loss and hence no information gain based on this kind of representation\n",
    "  - this is not strictly concave curve\n",
    "\n",
    "<img src=\"images/10_misrepresentationDiagram.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n",
    "- the decision splits curves that are successfully used are strictly concave curve\n",
    "\n",
    "- Gini curve\n",
    "  > $\\sum\\limits_{c}\\hat{p}_{c}(1-\\hat{p}_{c})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b47ff",
   "metadata": {},
   "source": [
    "#### Regression Tree - Extension for decision tree\n",
    "- So far we used decision tree for classification\n",
    "- Decision trees can also be used for regression trees\n",
    "- Example: Amount of snowfall\n",
    "  - Instead of predicting class, you predict mean of the \n",
    "  \n",
    "- For Region $R_{m}$, the prediction will be\n",
    "> Predict $\\hat{y}_{m} = \\frac{\\sum\\limits_{i \\in R_{m}}Y_{i}}{|R_{m}|}$\n",
    "  - sum all the values within the region and average them\n",
    "  \n",
    "\n",
    "<img src=\"images/10_regressionTrees.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n",
    "\n",
    "The loss will be\n",
    "> $L_{squared} = \\frac{\\sum\\limits_{i \\in R_{m}} (y_{i} - \\hat{y}_{m})^{2} }{|R_{m}|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ffd53",
   "metadata": {},
   "source": [
    "#### Categorical Variables\n",
    "- can ask questions on any form of subset, is location in northern hemisphere?\n",
    "- $location \\in \\{N\\}$\n",
    "- if there are q categories, the possible number of splits would be $2^{q}$, which very quickly becomes intractable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3eb13",
   "metadata": {},
   "source": [
    "#### Regularization of DTs\n",
    "- if you carry on the process of splits, you can split region for each datapoint and that will be case of overfitting\n",
    "- Decision trees are high variance models\n",
    "- So we need to regularize the decision tree models\n",
    "- Heuristics for regularization \n",
    "  - If you have a minimum leaf size, stop\n",
    "  - max depth\n",
    "  - max number of nodes\n",
    "  - min decrease in loss\n",
    "    - Before split, the loss is: $L(R_{p})$\n",
    "    - After split, the loss is: $L(R_{1}) + L(R_{2})$\n",
    "    - if after split, the loss is not great enough, we might conclude that it didn't gain  us anything\n",
    "      - but there might be some correlation between variables \n",
    "  - pruning\n",
    "    - you grow up your full tree and check which nodes to prune out\n",
    "    - you have a validation set that you use and you evaluate what your misclassification error is on the validation set, for each example for each leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdeda9",
   "metadata": {},
   "source": [
    "#### Runtime\n",
    "- n train examples\n",
    "- f features\n",
    "- d depth of tree\n",
    "\n",
    "##### Test time O(d)\n",
    "d < log n\n",
    "\n",
    "##### Train time\n",
    "- Each point is part of O(d) nodes\n",
    "- Cost of point at each node is O(f)\n",
    "  - for binary features, the cost will be f\n",
    "  - for quantitative features, sort and scan linearly, the cost will be f, as well\n",
    "- Total cost is O(nfd)\n",
    "  - where data matrix size is nf\n",
    "  - and depth is log n\n",
    "  - so cost is fairly fast training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6cf04",
   "metadata": {},
   "source": [
    "#### Downside of DT\n",
    "- it does not have additive structure\n",
    "- in the example below we get a very rough estimation of decision boundary\n",
    "- decision trees have problems where the features are interacting additively with one another\n",
    "\n",
    "<img src=\"images/10_noAdditiveStructure.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a8844",
   "metadata": {},
   "source": [
    "#### DT - Recap\n",
    "- Pos\n",
    "  - Easy to explain\n",
    "  - Interpretable\n",
    "  - can deal with categorical variable\n",
    "  - generally fast\n",
    "\n",
    "- Neg\n",
    "  - high variance problems - generally leads to overfitting\n",
    "  - Not additive\n",
    "  - Low predictive accuracy\n",
    "\n",
    "- We can make it lot better with ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c929b21",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "- take $X_{i}'s$ which are random variables that are independent identically distributed (i.i.d.) \n",
    "> $Var(X_{i}) = \\sigma^{2}$\n",
    "> $Var(\\bar{X}) = Var\\left(\\frac{1}{n}\\sum\\limits_{i}X_{i}\\right) = \\frac{\\sigma^{2}}{n}$ \n",
    "- which means each independent rv is decreasing the variance of your model\n",
    "\n",
    "- If we drop the independence assumption, so now $X_{i}'s$ are only i.d. X's are correlated by $\\rho$\n",
    "- So the variance of mean will be:\n",
    "> $Var(\\bar{X}) = \\rho \\sigma^{2} + \\frac{1-\\rho}{n} \\sigma^{2}$ \n",
    "  - if they are fully correlated ($\\rho = 1$), it becomes $Var(\\bar{X}) = \\sigma^{2}$\n",
    "  - if there is no correlation($\\rho = 0$), it becomes $Var(\\bar{X}) = \\frac{\\sigma^{2}}{n} $\n",
    "  - there would be interest in models with large n so the second term goes down. Also have models that are decorrelated so the first term goes down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb197e",
   "metadata": {},
   "source": [
    "#### Ways to ensemble\n",
    "- different algorithms, not really helpful\n",
    "- use different training sets, not really helpful\n",
    "- Bagging - Random Forest\n",
    "- Boosting - Adaboost, xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74041016",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "- Bootstrap aggregation\n",
    "  - bootstrapping is a method used in statistics to measure uncertainty\n",
    "- Say that a true population is P\n",
    "- Training set $S \\sim P$\n",
    "- Assume population is the training sample P = S\n",
    "- Bootstrap samples Z \\sim S\n",
    "  - Z is sampled from S. We take a training sample S with cardinality N. We sample N times from S with replacement, because we are assuming that S is a population and we are sampling from a population\n",
    "  - Take model and then train on all these separate bootstrap samples\n",
    "  \n",
    "<br>  \n",
    "\n",
    "#### Bootstrap aggregation\n",
    "  - we will train separate models separately and then average their outputs\n",
    "  - Say we have bootstrap samples $Z_{1},...,Z_{M}$\n",
    "  - We train model $G_{m}$ on $Z_{m}$ and define \n",
    "  > Aggregate Predictor $G(m) = \\frac{\\sum\\limits_{m=1}{M}G_{m}(x)}{M}$\n",
    "  - This process is called bagging\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d30b9",
   "metadata": {},
   "source": [
    "#### Bias-Variance Analysis\n",
    "> $Var(\\bar{X}) = \\rho \\sigma^{2} + \\frac{1-\\rho}{n} \\sigma^{2}$ \n",
    "- Bootstrapping is driving down $\\rho$\n",
    "- But what about the second term\n",
    "  - With the increase in bootstrap samples, the M term increases, driving down the second term\n",
    "- A nice property about bootstrapping is that increasing the number of bootstrap models does not cause overfit than before. \n",
    "- More M causes less variance\n",
    "- But the bias of the model increases\n",
    "  - because of the random subsampling from S, it causes model to be less complex as we are drawing less data, and increases the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7caca21",
   "metadata": {},
   "source": [
    "#### Decision Trees + Bagging\n",
    "- DT have high variance, low bias\n",
    "- this makes DT ideal fit for bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ea1e8",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "- RF is a version of decision trees and bagging\n",
    "- the random forest introduces even more randomization into each individual decision tree\n",
    "- 1st - Earlier we learnt, bootstrapping drives down $\\rho$\n",
    "- 2nd - But if we can further decorrelate the random variables, we can drive down the variance even further\n",
    "- At each split for RF, we consider only a fraction of your total features\n",
    "- 1st - Decreasing $\\rho$ in $Var(\\bar{X})$ \n",
    "- 2nd - Say in a classification problem, we have found a very strong predictor that gives very good performance on its own (in ski example - the latitude split), and we use that predictor first at the first split. That causes all your models to be very highly correlated. So we should try to decorrelate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2281e",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "- In bagging we tried to reduce variance\n",
    "- Boosting is opposite. In boosting we try to reduce bias\n",
    "- Is additive \n",
    "- In bagging, we took average of number of variables\n",
    "- In boosting, we train one model and then add it into the ensemble and then keep adding in as prediction\n",
    "- Decision stump - ask one question at a time\n",
    "  - the reason behind this is: we are decreasing bias by restricting the tree depth to be only 1\n",
    "  - this causes the bias to increase and decrease the variance \n",
    "- Say we make a split and make some misclassifications. \n",
    "- we identify those mistakes and increase the weights\n",
    "- in the next iteration, it works on the modified sets - because of more weights on misclassfied samples, split might pick this weighted decision boundary \n",
    "\n",
    "<img src=\"images/10_boosting.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Raphael Townshend}}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87b216",
   "metadata": {},
   "source": [
    "#### Adaboost\n",
    "- Determine for classifier $G_{m}$ a weight $\\alpha_{m}$ proportional, which is log odds\n",
    "> $log\\left( \\frac{1-err_{m}}{err_{m}}\\right)$\n",
    "- Total classifier\n",
    "> $G(x) = \\sum\\limits_{m}\\alpha_{m}G_{m}$\n",
    "- each $G_{m}$ is trained on re-weighted training set\n",
    "\n",
    "- Similar mechanism is used to derive algorithm like XGBoost or gradient boosting machines that allow us to reweight the examples we are getting right or wrong in dynamic fashion and then adding them in additive fashion to your model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
