{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1306f8",
   "metadata": {},
   "source": [
    ">>> Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29270c64",
   "metadata": {},
   "source": [
    "#### Outline\n",
    "- Perceptron\n",
    "- Exponential Family\n",
    "- Generalized Linear Models(GLM)\n",
    "- Softmax Regression(Multiclass classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13901ebc",
   "metadata": {},
   "source": [
    "### Logistic Regression (Recap)\n",
    "- Logistic Regression uses sigmoid function\n",
    "- ranges from $-\\infty$ to $\\infty$, with values ranging from 0 to 1, which is probability\n",
    "  > $$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "  - At z=0, g(z) = 0.5\n",
    "  - As z tends to $-\\infty$, g converges to 0\n",
    "  - As z tends to $\\infty$, g converges to 1\n",
    "  \n",
    "- variant of this is perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3737d5",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "- is not used widely because it does not have a probabilistic interpretation\n",
    "- is taught for historical reasons\n",
    "- logistic regression is a softer version of perceptron\n",
    "\n",
    "> $$\\begin{equation}\n",
    "  g(z) =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if z $\\ge$ 0}\\\\\n",
    "      0 & \\text{if z $\\lt$ 0}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n",
    "- hypothesis function\n",
    "> $h_{\\theta}(x) = g(\\theta^{T}x)$\n",
    "\n",
    "<img src=\"images/04_perceptronEq.png\" width=400 height=400 />   $\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$\n",
    "\n",
    "> $\\theta_{j} := \\theta_{j} + \\alpha(y^{(i)} - h_{\\theta}(x^{(i)}))x_{j}^{(i)}$\n",
    "- In this equation $(y^{(i)} - h_{\\theta}(x^{(i)}))$ is scalar, because $y^{(i)}$ is either 0/1 and so will be $h_{\\theta}(x^{(i)})$\n",
    "- So the result can be either\n",
    "$\\begin{equation}\n",
    "      =\n",
    "    \\begin{cases}\n",
    "      0  & \\text{if algorithm got it right}\\\\\n",
    "      +1 & \\text{if wrong $y^{(i)} = 1$}\\\\\n",
    "      -1 & \\text{if wrong $y^{(i)} = 0$}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$\n",
    "  - A result of 0 means, if the example is already classified, you do nothing\n",
    "  - A result of +1/-1 means, the example is misclassified, and you either add/subtract a small component of the example ($\\alpha x_{j}^{(i)}$) \n",
    "  - This will shift the decision boundary correctly\n",
    "  \n",
    "\n",
    "<img src=\"images/04_perceptronExample.png\" width=400 height=400 />   $\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7b84c",
   "metadata": {},
   "source": [
    "### Exponential Family\n",
    "- its a class of probability distribution\n",
    "- they are closely related to GLM\n",
    "<br>\n",
    "\n",
    "- PDF:\n",
    "  > $P(y;\\eta) = b(y) \\text{exp}[\\eta^{T}T(y) - a(\\eta)]$  \n",
    "    - y: data - use it to model output of data\n",
    "    - $\\eta$: natural parameter\n",
    "    - $T(y)$: sufficient statistics, cannot involve $\\eta$\n",
    "    - b(y): base measure, cannot involve $\\eta$\n",
    "    - $a(\\eta)$: log partition function\n",
    "  > $P(y;\\eta) = \\frac{b(y) e^{(\\eta^{T}T(y))}}{e^{a(\\eta)}}$ \n",
    "    - $a(\\eta)$: is also called normalizing constant of probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7bb55",
   "metadata": {},
   "source": [
    "#### Types of exponential family\n",
    "##### Bernoulli distribution\n",
    "- Bernoulli distribution belongs to the exponential family\n",
    "  - PDF \n",
    "  >$P(y; \\theta) $  \n",
    "  >$= \\phi^{y}(1-\\phi)^{1-y}$  \n",
    "  >$ = exp(log(\\phi^{y}(1-\\phi)^{1-y}))$   \n",
    "  >$ = exp[log(\\frac{\\phi}{1-\\phi})y + log(1-\\phi)]$  \n",
    "  - where\n",
    "    >$b(y) = 1$  \n",
    "    >$T(y) = y$  \n",
    "    >$\\eta = log(\\frac{\\phi}{1-\\phi}) \\Rightarrow \\phi = \\frac{1}{1+e^{-\\eta}}$  \n",
    "    >$a(\\eta) = -log(1-\\phi) = -log(1-\\frac{1}{1+e^{-\\eta} }) = log(1+e^{\\eta})$  \n",
    "    - We are linking the canonical parameters to natural parameters here\n",
    "\n",
    "##### Gaussian distribution\n",
    "- Gaussian distribution - with fixed variance\n",
    "  - Assume $\\sigma^{2} = 1$\n",
    "  - PDF \n",
    "  >$P(y; \\mu)$  \n",
    "  >$= \\frac{1}{\\sqrt(2\\pi)}exp(-\\frac{(y-\\mu)^{2}}{2})$  \n",
    "  >$ = \\frac{1}{\\sqrt(2\\pi)} e^{-\\frac{y^{2}}{2}}exp(\\mu y - \\frac{1}{2}\\mu ^{2}) $  \n",
    "  - where  \n",
    "    >$b(y) = \\frac{1}{\\sqrt(2\\pi)} e^{-\\frac{y^{2}}{2}}$  \n",
    "    >$T(y) = y$  \n",
    "    >$\\eta = \\mu$  \n",
    "        >$a(\\eta) = \\frac{\\mu^{2}}{2} = \\frac{\\eta^{2}}{2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b142d7",
   "metadata": {},
   "source": [
    "##### Other distributions:\n",
    "- How do you decide which distribution to use?\n",
    "  - The task in reality tells/influences you which distribution to use \n",
    "  - Real: Gaussian\n",
    "    - regression - predict house prices\n",
    "  - Binary: Bernoulli\n",
    "    - classification\n",
    "  - Integer Count: Poisson\n",
    "    - number of visitors to a web page\n",
    "  - Real positive: Gamma, Exponential\n",
    "  - Prob distribution over other distribution: Beta, Dirichlet (used mostly in Bayesian statistics)\n",
    "\n",
    "#### Properties of Exponential family:\n",
    "* If you perform maximum likelihood MLE wrt $\\eta \\Rightarrow$ concave \n",
    "  * NLL is convex (negative log likelihood)\n",
    "* $E[y;\\eta] = \\frac{\\partial}{\\partial \\eta} a(\\eta)$\n",
    "* $Var[y;\\eta] = \\frac{\\partial^{2}}{\\partial \\eta^{2}} a(\\eta)$\n",
    "  * Generally to calculate distribution properties (mean and variance), you need to integrate, in exponential family you differentiate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa2207",
   "metadata": {},
   "source": [
    "### Generalized Linear Models (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06322a9",
   "metadata": {},
   "source": [
    "Natural extension of exponential family - to include covariance/input features. Powerful models can be made by using this.  \n",
    "<br>\n",
    "Assumption/Design choices (to move from exponential family to GLM):  \n",
    "i) $y | x; \\theta \\sim $ Exponential family$(\\eta) $  \n",
    "ii) $\\eta = \\theta^{T}x $ where $ \\theta \\in \\mathbb R^{n}, x \\in \\mathbb R^{n}$  \n",
    "iii) At test time: Output E[y|x; $\\theta$]  \n",
    "  - $h_{\\theta}(x) = E[y|x; \\theta]$ - is the hypothesis function\n",
    "    - if we plugin exponential family as Gaussian, the hypothesis will turn out to be Gaussian hypothesis of linear regression\n",
    "    - if we plugin exponential family as Bernoulli, the hypothesis will turn out to be Bernoulli hypothesis of logistic regression\n",
    "<br>\n",
    "\n",
    "- One way to visualize this is (as in figure below) \n",
    "  - there is a model (linear model here)\n",
    "    - given x, there is a learnable parameter $\\theta$, and $\\theta^{T}x$ will give you a parameter $\\eta$\n",
    "  - there is a distribution\n",
    "    - the distribution is a member of exponential family and parameter for this distribution is output of linear model\n",
    "    - we choose exponential family based on the data that we have (classification problem, regression problem, or other)\n",
    "    - we will choose appropriate b, a and T based on distribution of your choice\n",
    "  - expectation \n",
    "    - During test time\n",
    "      - $E[y; \\eta] = E[y; \\theta^{T}x] = h_{\\theta}(x)$ - this is the hypothesis function\n",
    "      - Caveat:\n",
    "        - the parameter that we are learning during gradient descent is $\\theta$\n",
    "        - we dont learn anything of the exponential family eg., $\\mu, \\sigma^{2}, \\eta$\n",
    "        - we learn $\\theta$, that is part of model and not part of distribution\n",
    "    - During train time\n",
    "      - we perform gradient ascent/descent on the log probability with y where natural parameter was reparameterized with the linear model \n",
    "      - the gradient ascent is done by taking gradients on $\\theta$\n",
    "\n",
    "- Question\n",
    "  - Are we training $\\theta$ to predict the parameter of exponential family distribution whose mean is our prediction for y\n",
    "    - True\n",
    "\n",
    "<img src=\"images/04_GLM_model.png\" width=600 height=400 />   \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$  \n",
    "\n",
    "- This is the reason, or how GLMs are an extension of exponential families. You reparameterize the parameters with the linear model and you get a GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a717f91",
   "metadata": {},
   "source": [
    "#### GLM training\n",
    "- At train time, we perform maximum likelihood over the log probability of y with respect to $\\theta$\n",
    "- Learning update rule\n",
    "  - plugin appropriate $h_\\theta(x)$ depending on the choice of distribution and you can start learning\n",
    "  - the __learning update rule__ is the __same__ for all GLMs, for classification or regression, just the $h_{\\theta}$ varies. \n",
    "\n",
    "> $ \\theta _{j} := \\theta _{j} - \\alpha (h_{\\theta}(x^{(i)}) - y^{(i)}).x_{j}^{(i)} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ac159",
   "metadata": {},
   "source": [
    "#### Terminology\n",
    "- $\\eta$ - natural parameter\n",
    "- $\\mu = E[y_{i}; \\eta] = g(\\eta)$ - natural parameter to the mean of the function - canonical response function\n",
    "- $\\eta = g^{-1}(\\mu)$ - canonical link function\n",
    "- $g(\\eta) = \\frac{\\partial}{\\partial \\eta}a(\\eta)$\n",
    "<br>\n",
    "\n",
    "- There are 3 types of parameterization being used here:  \n",
    "i) Model parameter - $\\theta$ - this is the only parameter that is learned  \n",
    "ii) Natural parameter - $\\eta$  \n",
    "iii) Canonical parameter \n",
    "  - $\\phi$ - Bernoulli\n",
    "  - $\\mu, \\sigma^{2}$ - Gaussian\n",
    "  - $\\lambda$ - Poisson\n",
    "  \n",
    "<br>  \n",
    "\n",
    "- How are they linked  \n",
    "  - Model parameter and Natural parameter are linked by design choice ($\\theta^{T}x$)  \n",
    "  - g links natural parameter to canonical parameter \n",
    "  - $g^{-1}$ links canonical parameter to natural parameter  \n",
    "  \n",
    "<br>\n",
    "\n",
    "- Logistic Regression\n",
    "  > $h_{\\theta}(x) = E[y|x;\\theta] = \\phi = \\frac{1}{1+e^{-\\eta}} = \\frac{1}{1+e^{-\\theta^{T}x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41fd0b",
   "metadata": {},
   "source": [
    "- Are GLM used for classification/regression  \n",
    "  - depends on choice of distribution  \n",
    "  - GLM just a general way of model data which can be binary, real, exponential or others  \n",
    "  \n",
    "<br>\n",
    "\n",
    "- Assumptions\n",
    "  - Regression\n",
    "    - At every given x, there is a y given x which is Gaussian and is parameterized by $\\theta^{T}x$ as mean\n",
    "    - The assumption is there was a Gaussian distribution and you sampled the value from this Gaussian distribution\n",
    "    - We assume that the data was generated as above and we will work backward to find $\\theta$, which will give us boundary condition\n",
    "    \n",
    "<img src=\"images/04_glm_regression.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1086d",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "- Cross entropy minimization\n",
    "- Multiclass classification\n",
    "- can work over thousand of classes\n",
    "- Type of GLM\n",
    "- one vector per class\n",
    "- generalization of logistic regression, with different set of parameters per class\n",
    "- Goal is to:\n",
    "  - Start from this data, learn a model that can given a new data point, make a prediction of its class\n",
    "- Notation\n",
    "  - k - # of classes\n",
    "  - $x^{(i)} \\in \\mathbb R^{n}$\n",
    "  - Label: $y = [\\{0,1\\}^{k}]$\n",
    "    - For example: [0, 0, 0, 1, 0] - assuming there are 5 class here\n",
    "    - label is a vector which indicates which class the x corresponds to\n",
    "    - each element in the vector corresponds to a vector\n",
    "    - there will be only 1 in the label vector\n",
    "- Each class has its own set of parameters\n",
    "  - $\\theta_{class} \\in \\mathbb R^{n}$\n",
    "\n",
    "<img src=\"images/04_softmax.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8562f",
   "metadata": {},
   "source": [
    "- Example\n",
    "  * For a given x, $\\theta_{i}^{T}x$ (logit space) will have a range of $-\\infty$ to $\\infty$\n",
    "  * goal is to get probability distribution over classes\n",
    "    * inorder to do that, we exponentiate the logits ($exp(\\theta_{i}^{T}x)$), which makes everything positive\n",
    "    * then we normalize this, by the sum of all $\\frac{e^{\\theta_{i}^{T}x}}{\\sum_{all\\space classes} e^{\\theta_{i}^{T}x}}$\n",
    "    * this gives us a probability distribution $\\hat{p}(y)$ over all the classes\n",
    "    * minimize the distance between true label and learned label distribution using cross entropy\n",
    "    \n",
    "<img src=\"images/04_softmax1.png\" width=600 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng/Anand Avati}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816ed78",
   "metadata": {},
   "source": [
    "* minimize the distance between these two distributions or minimize the cross entropy ($p, \\hat p $)\n",
    "  > Cross entropy($p, \\hat p$)  \n",
    "  > $ = -\\sum\\limits_{y \\in \\{\\Delta, \\square, \\circ \\} } p(y) \\text{log}\\hat p(y)$   \n",
    "  > $ = - log\\space\\hat p(y_{\\Delta})$ - associated class here is triangle   \n",
    "  > $ = - log \\frac{e^{\\theta_{\\Delta}^{T}x}}{\\sum_{c \\in \\{\\Delta, \\square, \\circ \\}} e^{\\theta_{c}^{T}x}}$  \n",
    "* treat this as a loss and apply gradient descent wrt parameter   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
