{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02e700b",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525acc13",
   "metadata": {},
   "source": [
    "## Lecture 15\n",
    "\n",
    "#### Outline\n",
    "- EM Convergence\n",
    "- Gaussian properties\n",
    "- Factor Analysis\n",
    "- Gaussian marginals and \n",
    "- EM steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b222f4",
   "metadata": {},
   "source": [
    "### Recap EM algorithm\n",
    "\n",
    "#### E-step\n",
    "> $w_{j}^{(i)} = Q_{i}(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\theta)$\n",
    "\n",
    "#### M-step\n",
    "> $\\theta = \\text{arg}\\max\\limits_{\\theta}\\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\theta)}{Q_{i}(z^{(i)})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e96d92",
   "metadata": {},
   "source": [
    "### Mixture of Gaussian Models\n",
    "\n",
    "- Suppose there is a latent(hidden/unobserved) random variable z; and $x^{(i)}, z^{(i)}$ are modeled as a joint distribution \n",
    "> $p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)})p(z^{(i)})$, where $z \\in \\{1,2,..,k\\}$\n",
    "- where $z^{(i)} \\sim $ Multinomial($\\phi$) $\\Rightarrow p(z^{(i)}=j) = \\phi_{j}$ and $x^{(i)}|z^{(i)} = j \\sim N(\\mu_{j}, \\Sigma_{j})$\n",
    "\n",
    "\n",
    "#### E-step - implementation\n",
    "- $w_{j}^{(i)}$ is the strength with which $x^{(i)}$ is assigned to Gaussian j\n",
    "> $w_{j}^{(i)} = Q:(z^{(i)}=j) = p(z^{(i)}|x^{(i)}; \\phi, \\mu, \\Sigma)$\n",
    "- where $Q:(z^{(i)}=j) \\equiv p(z^{(i)}=j)$\n",
    "\n",
    "#### M-step - implementation\n",
    "> $\\max\\limits_{\\phi, \\mu, \\Sigma} \\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\phi, \\mu, \\Sigma)}{Q_{i}(z^{(i)})}$\n",
    "> $ = \\sum\\limits_{i}\\sum\\limits_{j} w_{j}^{(i)}\\log\\frac{ \\frac{1}{(2\\pi)^{n/2}|\\Sigma_{j}|^{1/2}} exp\\left( -\\frac{1}{2} (x^{(i)} - \\mu_{j})^{T}\\Sigma_{j}^{-1}(x^{(i)} - \\mu_{j}) \\right) \\phi_{j}}{w_{j}^{(i)}}$\n",
    "\n",
    "\n",
    "<img src=\"images/15_stepM.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291d74b",
   "metadata": {},
   "source": [
    "- in order to maximize above equation\n",
    "> $\\nabla_{\\mu_{j}}(...) \\buildrel \\rm set \\over = 0$\n",
    "> $\\Rightarrow \\mu_{j} = \\frac{\\sum\\limits_{i}w_{j}^{(i)}x^{(i)}}{\\sum\\limits_{i}w_{j}^{(i)}}$\n",
    "- Similarly maximize\n",
    "> $\\nabla_{\\phi_{j}}(...) \\buildrel \\rm set \\over = 0$\n",
    "- and \n",
    "> $\\nabla_{\\Sigma_{j}}(...) \\buildrel \\rm set \\over = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc275783",
   "metadata": {},
   "source": [
    "### Application of EM - Factor Analysis Model\n",
    "- In Gaussian mixture models, the $z^{(i)}$ was discrete. \n",
    "- In Factor Analysis models,  the $z^{(i)}$ will be continuous and $z^{(i)}$ will be Gaussian.\n",
    "  - every thing works as before, if we simply change the discrete sum to integral\n",
    "\n",
    "\n",
    "<img src=\"images/15_factorAnalysisModelvsEM.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d833201",
   "metadata": {},
   "source": [
    "#### Another view of EM\n",
    "> $J(\\theta, Q) = \\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\theta)}{Q_{i}(z^{(i)})}$\n",
    "- We earlier proved $l(\\theta) \\ge J(\\theta, Q)$ is true for any $\\theta, Q$ by Jensen's inequality\n",
    "- J for any choice of $\\theta$ and Q is a lower bound for the log likelihood of $\\theta$\n",
    "- Equivalent view of EM algorithm:\n",
    "  - E step: Maximize J wrt Q\n",
    "  - M step: Maximize J wrt $\\theta$\n",
    "  - So in E step, we are picking the choice of Q that maximizes J, by setting J equal to $l()$. In M step, we maximize this wrt $\\theta$ and push the value of $l()$ even higher. \n",
    "  - This algorithm is also called __coordinate ascent__, as it is a function of 2 variables and we optimize wrt to first, then second and go back and forth, and optimize wrt one at a time.\n",
    "  - plot value of J over l to see if it is converging or not  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23b66d",
   "metadata": {},
   "source": [
    "### Factor Analysis\n",
    "- For mixture of Gaussian, say there are n=2 Gaussians and m=100 training examples \n",
    "- Here $m \\gg n$, lot more examples are there than dimensions\n",
    "\n",
    "<br>\n",
    "\n",
    "- We cannot use mixture of Gaussians, if $m \\approx n$ or $m \\ll n$\n",
    "- Say we have 100 dimensional data (n=100) and 30 training examples (m=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf14eb0",
   "metadata": {},
   "source": [
    "#### Model as a single Gaussian\n",
    "- $x \\sim N(\\mu, \\Sigma)$\n",
    "- MLE: \n",
    "> $\\mu = \\frac{1}{m}\\sum\\limits_{i}x^{(i)}$  \n",
    "> $\\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$  \n",
    "- If $m \\ll n$, then $\\Sigma_{i}$ will be singular or non-invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43598375",
   "metadata": {},
   "source": [
    "- Gaussian density looks like this:\n",
    ">  $\\frac{1}{(2\\pi)^{n/2}|\\Sigma_{j}|^{1/2}} \\exp\\left( -\\frac{1}{2} (x^{(i)} - \\mu_{j})^{T}\\Sigma_{j}^{-1}(x^{(i)} - \\mu_{j}) \\right)$\n",
    "  - when covariance matrix is singular - $|\\Sigma_{j}|$ will be 0 and $\\Sigma_{j}^{-1}$ will be 0.\n",
    "- If we look into the contour diagram of such Gaussians, we observe that it is squished into infinitely skinny line form, with all mass distributed across the line. \n",
    "- The problem arises when there is any point which is slightly off, will have no probability mass because Gaussian is squished infinitely thin on that line.\n",
    "\n",
    "<img src=\"images/15_singleGaussianContours1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dabc8f",
   "metadata": {},
   "source": [
    "- This problem first came up in a research paper of psychology, where there are 100 psychological attributes of 30 persons\n",
    "- If standard Gaussian model does not work, what are the alternatives?\n",
    "\n",
    "#### Option 1\n",
    "- Constrain $\\Sigma$ to be diagonal, i.e., make off-diagonal entries as 0\n",
    "  - this has weaker assumptions\n",
    "  - this brings all the model contours onto axis. It does not model off-axis examples\n",
    "  - assumes that all examples are uncorrelated and independent\n",
    "  - this model takes care of if the matrix is singular, but we cant use this if they are correlated \n",
    "  \n",
    "<img src=\"images/15_gaussianOption11.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260eb0da",
   "metadata": {},
   "source": [
    "#### Option 2\n",
    "\n",
    "- Constrain $\\Sigma$ to be diagonal, i.e., make off-diagonal entries as \n",
    "0 and  $\\Sigma = \\sigma^{2}I$ \n",
    "  - this has stronger assumptions\n",
    "  \n",
    "<img src=\"images/15_gaussianOption2.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "- this is also not a good option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0735b8a",
   "metadata": {},
   "source": [
    "#### Factor Analysis Model\n",
    "- very mathy - check notes - add intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f2dd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a3a3c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b38dd90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1cd5fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff626ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
