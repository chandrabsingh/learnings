{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e43f2fa",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23349f7f",
   "metadata": {},
   "source": [
    "## Lecture 15\n",
    "\n",
    "#### Outline\n",
    "- EM Convergence\n",
    "- Gaussian properties\n",
    "- Factor Analysis\n",
    "- Gaussian marginals and \n",
    "- EM steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c190408",
   "metadata": {},
   "source": [
    "### Recap EM algorithm\n",
    "\n",
    "#### E-step\n",
    "> $w_{j}^{(i)} = Q_{i}(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\theta)$\n",
    "\n",
    "#### M-step\n",
    "> $\\theta = \\text{arg}\\max\\limits_{\\theta}\\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\theta)}{Q_{i}(z^{(i)})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ce1cf",
   "metadata": {},
   "source": [
    "### Mixture of Gaussian Models\n",
    "\n",
    "- Suppose there is a latent(hidden/unobserved) random variable z; and $x^{(i)}, z^{(i)}$ are modeled as a joint distribution \n",
    "> $p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)})p(z^{(i)})$, where $z \\in \\{1,2,..,k\\}$\n",
    "- where $z^{(i)} \\sim $ Multinomial($\\phi$) $\\Rightarrow p(z^{(i)}=j) = \\phi_{j}$ and $x^{(i)}|z^{(i)} = j \\sim N(\\mu_{j}, \\Sigma_{j})$\n",
    "\n",
    "\n",
    "#### E-step - implementation\n",
    "- $w_{j}^{(i)}$ is the strength with which $x^{(i)}$ is assigned to Gaussian j\n",
    "> $w_{j}^{(i)} = Q:(z^{(i)}=j) = p(z^{(i)}|x^{(i)}; \\phi, \\mu, \\Sigma)$\n",
    "- where $Q:(z^{(i)}=j) \\equiv p(z^{(i)}=j)$\n",
    "\n",
    "#### M-step - implementation\n",
    "> $\\max\\limits_{\\phi, \\mu, \\Sigma} \\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\phi, \\mu, \\Sigma)}{Q_{i}(z^{(i)})}$\n",
    "> $ = \\sum\\limits_{i}\\sum\\limits_{j} w_{j}^{(i)}\\log\\frac{ \\frac{1}{(2\\pi)^{n/2}|\\Sigma_{j}|^{1/2}} exp\\left( -\\frac{1}{2} (x^{(i)} - \\mu_{j})^{T}\\Sigma_{j}^{-1}(x^{(i)} - \\mu_{j}) \\right) \\phi_{j}}{w_{j}^{(i)}}$\n",
    "\n",
    "\n",
    "<img src=\"images/15_stepM.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f555e",
   "metadata": {},
   "source": [
    "- in order to maximize above equation\n",
    "> $\\nabla_{\\mu_{j}}(...) \\buildrel \\rm set \\over = 0$\n",
    "> $\\Rightarrow \\mu_{j} = \\frac{\\sum\\limits_{i}w_{j}^{(i)}x^{(i)}}{\\sum\\limits_{i}w_{j}^{(i)}}$\n",
    "- Similarly maximize\n",
    "> $\\nabla_{\\phi_{j}}(...) \\buildrel \\rm set \\over = 0$\n",
    "- and \n",
    "> $\\nabla_{\\Sigma_{j}}(...) \\buildrel \\rm set \\over = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab712013",
   "metadata": {},
   "source": [
    "### Application of EM - Factor Analysis Model\n",
    "- In Gaussian mixture models, the $z^{(i)}$ was discrete. \n",
    "- In Factor Analysis models,  the $z^{(i)}$ will be continuous and $z^{(i)}$ will be Gaussian.\n",
    "  - every thing works as before, if we simply change the discrete sum to integral\n",
    "\n",
    "\n",
    "<img src=\"images/15_factorAnalysisModelvsEM.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c2ab8",
   "metadata": {},
   "source": [
    "#### Another view of EM\n",
    "> $J(\\theta, Q) = \\sum\\limits_{i}\\sum\\limits_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)}; \\theta)}{Q_{i}(z^{(i)})}$\n",
    "- We earlier proved $l(\\theta) \\ge J(\\theta, Q)$ is true for any $\\theta, Q$ by Jensen's inequality\n",
    "- J for any choice of $\\theta$ and Q is a lower bound for the log likelihood of $\\theta$\n",
    "- Equivalent view of EM algorithm:\n",
    "  - E step: Maximize J wrt Q\n",
    "  - M step: Maximize J wrt $\\theta$\n",
    "  - So in E step, we are picking the choice of Q that maximizes J, by setting J equal to $l()$. In M step, we maximize this wrt $\\theta$ and push the value of $l()$ even higher. \n",
    "  - This algorithm is also called __coordinate ascent__, as it is a function of 2 variables and we optimize wrt to first, then second and go back and forth, and optimize wrt one at a time.\n",
    "  - plot value of J over l to see if it is converging or not  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33a7e8",
   "metadata": {},
   "source": [
    "### Factor Analysis\n",
    "- For mixture of Gaussian, say there are n=2 Gaussians and m=100 training examples \n",
    "- Here $m \\gg n$, lot more examples are there than dimensions\n",
    "\n",
    "<br>\n",
    "\n",
    "- We cannot use mixture of Gaussians, if $m \\approx n$ or $m \\ll n$\n",
    "- Say we have 100 dimensional data (n=100) and 30 training examples (m=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b87bd",
   "metadata": {},
   "source": [
    "#### Model as a single Gaussian\n",
    "- $x \\sim N(\\mu, \\Sigma)$\n",
    "- MLE: \n",
    "> $\\mu = \\frac{1}{m}\\sum\\limits_{i}x^{(i)}$  \n",
    "> $\\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)} - \\mu)(x^{(i)} - \\mu)^{T}$  \n",
    "- If $m \\ll n$, then $\\Sigma_{i}$ will be singular or non-invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160e816",
   "metadata": {},
   "source": [
    "- Gaussian density looks like this:\n",
    ">  $\\frac{1}{(2\\pi)^{n/2}|\\Sigma_{j}|^{1/2}} \\exp\\left( -\\frac{1}{2} (x^{(i)} - \\mu_{j})^{T}\\Sigma_{j}^{-1}(x^{(i)} - \\mu_{j}) \\right)$\n",
    "  - when covariance matrix is singular - $|\\Sigma_{j}|$ will be 0 and $\\Sigma_{j}^{-1}$ will be 0.\n",
    "- If we look into the contour diagram of such Gaussians, we observe that it is squished into infinitely skinny line form, with all mass distributed across the line. \n",
    "- The problem arises when there is any point which is slightly off, will have no probability mass because Gaussian is squished infinitely thin on that line.\n",
    "\n",
    "<img src=\"images/15_singleGaussianContours1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872f2c5",
   "metadata": {},
   "source": [
    "- This problem first came up in a research paper of psychology, where there are 100 psychological attributes of 30 persons\n",
    "- If standard Gaussian model does not work, what are the alternatives?\n",
    "\n",
    "#### Option 1\n",
    "- Constrain $\\Sigma$ to be diagonal, i.e., make off-diagonal entries as 0\n",
    "  - this has weaker assumptions\n",
    "  - this brings all the model contours onto axis. It does not model off-axis examples\n",
    "  - assumes that all examples are uncorrelated and independent\n",
    "  - this model takes care of if the matrix is singular, but we cant use this if they are correlated \n",
    "  \n",
    "<img src=\"images/15_gaussianOption11.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db38066",
   "metadata": {},
   "source": [
    "#### Option 2\n",
    "\n",
    "- Constrain $\\Sigma$ to be diagonal, i.e., make off-diagonal entries as \n",
    "0 and  $\\Sigma = \\sigma^{2}I$ \n",
    "  - this has stronger assumptions\n",
    "  \n",
    "<img src=\"images/15_gaussianOption2.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "- this is also not a good option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb1125",
   "metadata": {},
   "source": [
    "#### Factor Analysis Model\n",
    "- very mathy - check notes - add intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe37587",
   "metadata": {},
   "source": [
    "#### Define factor analysis model\n",
    "  \n",
    "- Snapshot1\n",
    "\n",
    "<img src=\"images/15_factorAnalysis111.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "- Snapshot2\n",
    "<img src=\"images/15_factorAnalysis121.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n",
    "- Snapshot 1 - Intuition\n",
    "  - z is hidden - framework - same as Gaussian\n",
    "  - definition of factor analysis model\n",
    "    - z is drawn from a Gaussian density N(0,1) where $z \\in \\mathbb R^{d}$ and $d \\lt n$\n",
    "    - from a high dimensional n = 100, we are projecting it onto a lower dimensional subspace d = 3, with m=30 training examples\n",
    "    - Assumption: $x = \\mu + \\Lambda z + \\epsilon$, with $\\epsilon \\sim N(0, \\Psi)$\n",
    "- Snapshot 2 - Intuition\n",
    "  - Parameters:\n",
    "    - $\\mu \\in \\mathbb R^{nx1}, \\Lambda \\in \\mathbb R^{nxd}, \\Psi \\in \\mathbb R^{nxn}$ diagonal covariance matrix\n",
    "    - Equivalent way of writing \n",
    "    > $x = \\mu + \\Lambda z + \\epsilon$\n",
    "    - will be\n",
    "    > $x|z \\sim N(\\mu + \\Lambda z, \\Psi)$\n",
    "    - given $z \\sim N(0,I)$, x is computed as $\\mu + \\Lambda z$ and a Gaussian noise of covariance $\\Psi$ from $\\epsilon$\n",
    "    \n",
    "    \n",
    "- Snapshot3\n",
    "<img src=\"images/15_factorAnalysis1.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "- Snapshot3/4 - Intuition\n",
    "  - a straight line on top - this is be a typical sample drawn from a standard Gaussian $z^{(i)} \\sim N(0,1)$\n",
    "  - we map $\\Lambda z + \\mu$ is on a $\\mathbb R^{2}$, even though all points lie on a straight line\n",
    "  - $\\Psi$ is thee diagonal covariance matrix \n",
    "    - in this particular diagonal covariance matrix, $x_{2}$ has a bigger variance than $x_{1}$, which means the density is taller than wide. The aspect ratio of contour will be $\\frac{1}{\\sqrt(2)}$\n",
    "  - $x = \\Lambda z + \\mu + \\epsilon$\n",
    "    - this implies we will take all of these points and have a little Gaussian contour, we know the density/shape\n",
    "    - we sample one point from each of these Gaussian contours and sample a point from that Gaussian\n",
    "    - red cross are the typical sample drawn from these model\n",
    "    - z's are latent random variables, which we see after using EM \n",
    "  - $d=1, n=2 \\Rightarrow $ - one way to think of these data is \n",
    "    - we have n=2 dimensional data, but the \n",
    "    - data lies on a d=1D subspace\n",
    "\n",
    "- Snapshot4\n",
    "\n",
    "<img src=\"images/15_factorAnalysis2.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807faf7",
   "metadata": {},
   "source": [
    "- Snapshot5\n",
    "  - compute $\\Lambda z + \\mu$, where $\\Lambda$ is 3x2 and $\\mu$ is 3x1\n",
    "\n",
    "<img src=\"images/15_factorAnalysis5.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "- __A factor analysis can take very high dimensional data, say 100 dimensional data and model the data as roughly lying on 3 dimensional subspace with a little bit of noise off that low dimensional subspace__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c824a93",
   "metadata": {},
   "source": [
    "#### Multivariate Gaussian\n",
    "- details in notes\n",
    "\n",
    "- x is a vector and writing it in a partitioned way, $x_{1}$ has r components and $x_{2}$ has s components\n",
    "- $x_{1} \\in \\mathbb R^{r}, x_{2} \\in \\mathbb R^{s}, x \\in \\mathbb R^{r+s}$ \n",
    "- Similarly partition $\\mu$ and $\\Sigma$\n",
    "\n",
    "<img src=\"images/15_multivariateGaussian1.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adccf6",
   "metadata": {},
   "source": [
    "- To derive factor analysis, we need to compute marginal and conditional distribution of Gaussian\n",
    "- using Marginal distribution, we get $p(x_{1})$\n",
    "> $x_{1} \\sim N(\\mu_{1}, \\Sigma_{11})$\n",
    "\n",
    "\n",
    "<img src=\"images/15_marginalDistr.png\" width=600 height=600>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df4590",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "\n",
    "- check notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
