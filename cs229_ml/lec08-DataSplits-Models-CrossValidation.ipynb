{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21725ba",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f0dbe",
   "metadata": {},
   "source": [
    "## Lecture Notes\n",
    "\n",
    "#### Outline\n",
    "- Bias/Variance\n",
    "- Regularization\n",
    "- Train/dev/test splits\n",
    "- Model selection and Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd6711",
   "metadata": {},
   "source": [
    "### Bias/Variance\n",
    "- underfit the data \n",
    "  - high bias\n",
    "    - this data has strong bias that the data could be fit linearly\n",
    "- overfit the data\n",
    "  - high variance\n",
    "    - the prediction will have very high variance with slight modification in random draws of data\n",
    "\n",
    "- Variance and bias gives an understanding as how to improve the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba73c35",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "- this is used very often\n",
    "- the optimization objective for linear regression looks like:\n",
    "> $\\min_{\\theta}\\sum\\limits_{i=1}^{m}\\Vert y^{(i)} - \\theta^{T}x^{(i)} \\Vert^{2} $  \n",
    "- to add regularization, we add an extra term\n",
    "  - by adding the regularization, we have added an incentive term for the algorithm to make the $\\theta$ parameter smaller\n",
    "> $\\min\\limits_{\\theta}\\sum\\limits_{i=1}^{m}\\Vert y^{(i)} - \\theta^{T}x^{(i)} \\Vert^{2} + \\frac{\\lambda}{2} \\Vert \\theta \\Vert^{2}$  \n",
    "  - if $\\lambda$ is set to 0, we will be overfitting\n",
    "  - if $\\lambda$ is set to a big number, then we will be forcing parameters to be too close to 0, we will be underfitting, with a very simple function\n",
    "- the optimization cost function for logistic regression looks like:\n",
    "> arg $\\max\\limits_{\\theta}\\sum\\limits_{i=1}^{n}log\\space p(y^{(i)}|x{(i)};\\theta)$  \n",
    "- to add regularization, we add an extra term\n",
    "> arg $\\max\\limits_{\\theta}\\sum\\limits_{i=1}^{n}log\\space p(y^{(i)}|x^{(i)};\\theta) - \\frac{\\lambda}{2} \\Vert \\theta \\Vert^{2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64d509",
   "metadata": {},
   "source": [
    "#### Bayesian statistics and regularization\n",
    "\n",
    "- Let S be the training set $S = \\{(x^{(i)}, y^{(i)})\\} _{i=1}^{m}$\n",
    "- Given a training set, we want to find the most likely value of $\\theta$, by Bayes rule\n",
    "> $P(\\theta|s) = \\frac{P(s|\\theta)P(\\theta)}{p(s)}$  \n",
    "- To pick most likely value of $\\theta$, given the data we saw\n",
    "> arg $\\max_{\\theta} P(\\theta|s) = \\text{arg }\\max_{\\theta} P(s|\\theta)P(\\theta)$  \n",
    "  - where the denominator is constant  \n",
    "- For logistic regression, the equation becomes  \n",
    "> arg $\\max_{\\theta} \\left(\\prod\\limits_{i=1}^{m} P(y^{(i)} | x^{(i)}; \\theta)\\right) P(\\theta) $\n",
    "- If you assume $P(\\theta)$ is Gaussian $\\theta \\sim \\mathbb N(0, \\tau^{2}I)$, prior distribution of $\\theta$  \n",
    "> $P(\\theta) = \\frac{1}{\\sqrt{2\\pi}(\\tau^{2}I)^{1/2}} exp\\left(-\\frac{1}{2}\\theta^{T}(\\tau^{2}I)^{-1}\\theta  \\right)$  \n",
    "- The above is the prior distribution for $\\theta$, and if we plug this in the estimate of $\\theta$, take max and apply log, we will get the same regularization solution as above\n",
    "\n",
    "- All of the above is based on frequentist interpretation\n",
    "  - Frequentist  \n",
    "    > arg $\\max\\limits_{\\theta} P(S|\\theta)$ - MLE - Maximum likelihood  \n",
    "  - Bayesian  \n",
    "    - based on prior distribution - after we have seen the data. \n",
    "    - look at the data, compute the Bayesian posterior distribution of $\\theta$ and pick a value of $\\theta$ that's most likely  \n",
    "    > arg $\\max\\limits_{\\theta} P(\\theta|S)$ - MAP - Maximum a posteriori  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f410c",
   "metadata": {},
   "source": [
    "#### Error vs Model complexity\n",
    "\n",
    "- Assuming we dont consider regularization\n",
    "- plot a curve with model complexity on x-axis (with high degree polynomial on right side of curve) and training error on y-axis\n",
    "- we observe that training error improves or reduces with higher degree of complexity or more degree of polynomial\n",
    "- we also observed that the ability of algorithm to generalize goes down and then starts to go back up with increase in model complexity (generalization error)\n",
    "  - this curve is also true with regularization\n",
    "    - if $\\lambda$(=infinite) is way too big, it will underfit\n",
    "    - if $\\lambda$(=zero) is way too small, it will overfit\n",
    "- Let us try to find different procedures for finding this point in the middle\n",
    "\n",
    "<img src=\"images/08_generalizationError.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c76190",
   "metadata": {},
   "source": [
    "### Train/Dev/Test datasets\n",
    "- Given a dataset\n",
    "- we split data into train/dev/test sets\n",
    "- say we have 10000 examples\n",
    "- we are trying to find what is the polynomial we are trying to fit, or we are trying to choose $\\lambda$, or we are trying to choose $\\tau$ band-width parameter in locally weighted regression, or we are trying to choose value $C$ in SVM\n",
    "  - in all these problems, we have question of bias/variance trade-offs\n",
    "- Split dataset $S$ into $S_{train}$, $S_{dev}$, $S_{test}$\n",
    "  - Train each $model_{i}$ (option for different degree of polynomial) on \"$S_{train}$\"\n",
    "  - Get some hypothesis $h_{i}$\n",
    "  - Measure the error on \"$S_{dev}$\"\n",
    "  - Pick the one with lowest error on \"$S_{dev}$\"\n",
    "    - If you measure error on \"$S_{train}$\", we will end up choosing a complex polynomial to fit\n",
    "  - To publish a paper or report unbiased report, evaluate your algorithm on a separate \"$S_{test}$\" set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dad01c",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "\n",
    "- Holdout cross validation set\n",
    "<br>\n",
    "\n",
    "- Optimize performance on the dev set\n",
    "- Then to know how well is the algorithm performing, then evaluate the model on the test set\n",
    "- Be careful not to do is - Dont make decision based on the test set\n",
    "  - Because then your scientific data to the test set is no longer an unbiased estimate\n",
    "\n",
    "<img src=\"images/08_crossValidationError.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b20b87",
   "metadata": {},
   "source": [
    "#### k-fold Cross Validation\n",
    "- Small datasets - You have 100 examples\n",
    "- It is a waste of data if you apply 70-30 rule?\n",
    "  - $S_{train}$ = 70, $S_{dev}$ = 30\n",
    "- Procedure if you have a small dataset\n",
    "    - Say k=5, divide 100 examples into 5 subsets, 20 examples in each subset\n",
    "      - For d = 1,...,5 (for each degree of polynomial say 5)\n",
    "        - For i=1,..,k\n",
    "          - Train (fit parameters) on k-1 pieces\n",
    "          - Test on the remaining 1 piece\n",
    "        - Average\n",
    "      - pick the degree of polynomial that did best among all the runs\n",
    "      - we have now 5 classifiers\n",
    "      - say if we choose 2nd order polynomial\n",
    "      - refit the model once on all 100% of the data\n",
    "- Typically k=10 is used\n",
    "\n",
    "- Even smaller \n",
    "  - Leave-one-out CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f5c5c",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- If you suspect that out of 10000 features only 50 are highly relevant\n",
    "- Preventive Maintenance for truck - say there are 10000 reasons why the truck may go down, but only 10 of them might be most relevant\n",
    "  - in such cases, feature selection might be the thing to go for\n",
    "- Many a times, one way to reduce overfitting is to try to find a small subset of features that are most useful for the task\n",
    "  - this takes judgement\n",
    "  - this cannot be applied to computer vision, as subset of pixels might be relevant\n",
    "  - but this can be applied to other type of problems, where looking at small subset of relevant features help in reducing overfitting\n",
    "  \n",
    "<br>\n",
    "\n",
    "- Feature selection is a special case of model selection\n",
    "<br>\n",
    "\n",
    "- Algorithm\n",
    "  - Start with empty set of feature F = $\\phi$\n",
    "  - Repeat \n",
    "    - Try adding each feature i to F and see which single feature addition most improves the dev set performance\n",
    "    - Go ahead and add that feature to F\n",
    "    \n",
    "- This can be computationally expensive\n",
    "- Another such method is backward search\n",
    "  - in this we start will all the features and remove one feature at a time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
