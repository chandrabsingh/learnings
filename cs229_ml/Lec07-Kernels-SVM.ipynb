{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6004f2a",
   "metadata": {},
   "source": [
    ">>> Work in Progress (Following are the lecture notes of Prof Andrew Ng - CS229 - Stanford. This is my interpretation of his excellent teaching and I take full responsibility of any misinterpretation/misinformation provided herein.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58143b6a",
   "metadata": {},
   "source": [
    "## Lecture Notes\n",
    "\n",
    "#### Outline\n",
    "- SVM\n",
    "  - Optimization problem\n",
    "  - Representer theorem\n",
    "  - Kernels\n",
    "  - Examples of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d093fe7",
   "metadata": {},
   "source": [
    "### Derivation of optimal margin classifier\n",
    "- In the last lecture, we learned what the optimal margin classifier does is to choose w,b to maximize $\\gamma$\n",
    "> $\\max_{\\gamma, w, b} \\text{s.t.} \\frac{(y^{(i)})(w^{T}x^{(i)} + b)}{\\Vert w \\Vert} \\ge \\gamma$ for i=1,...,m\n",
    "  - subject to for every single training example  must have the geometric margin greater than or equal to gamma\n",
    "  - this causes to maximize the worst-case geometric margin\n",
    "    - what this optimization problem is trying to find w and b to drive up and choose gamma as big as possible so that every training example has geometric margin even bigger than gamma  \n",
    "- functional margin is the numerator in the equation above\n",
    "  - we can scale w, b up/down by any number and the decision boundary remains to be the same \n",
    "  - Trick:\n",
    "    - if we choose $\\Vert w \\Vert = \\frac{1}{\\gamma}$\n",
    "      - the optimization objective becomes: \n",
    "      > $max \\frac{1}{\\Vert w \\Vert}$  \n",
    "      > s.t. $(y^{(i)})(w^{T}x^{(i)} + b)\\gamma \\ge \\gamma$\n",
    "      - which becomes\n",
    "      > $\\min\\limits_{w,b} \\frac{1}{2}\\Vert w \\Vert^{2}$  \n",
    "      > s.t. $(y^{(i)})(w^{T}x^{(i)} + b) \\ge 1$ for i=1,..m\n",
    "- This will give you the optimal margin classifier\n",
    "\n",
    "### Derivation of SVM\n",
    "- We are assuming that the dimension of training example is 100 or so. Later we will learn how to solve this classifier when the dimension is infinite. $x \\in \\mathfrak R^{100}$\n",
    "- We are assuming that the weights w can be represented as a linear combination of the training examples $w = \\sum\\limits_{i=1}^{m} \\alpha_{i}x^{(i)}$\n",
    "- __Representer theorem__ shows that you can make this assumption without losing any performance using __primal dual optimization__\n",
    "  - Why - Intuition 1:\n",
    "    - Using induction and taking the use case of gradient descent, we can see that $\\theta$ is a linear combination of the training examples $x^{(i)}$\n",
    "    - $\\theta := \\theta - \\alpha(h_{\\theta}x^{(i)} - y^{(i)})x^{(i)}$\n",
    "  - Why - Intuition 2:\n",
    "    - The vector w is always orthogonal to the decision boundary. Or the vector w lies in the span of the training examples\n",
    "    - The way to picture this is that w sets the direction of the decision boundary and b sets the relative position. \n",
    "\n",
    "<img src=\"images/07_representerTheorem1.png\" width=200 height=200>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "\n",
    "<img src=\"images/07_representerTheorem2.png\" width=300 height=300>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01e866",
   "metadata": {},
   "source": [
    "- Lets say the training examples are in 3 dimension of which the training examples have no coordinates $x_{3} = 0$\n",
    "- Vector w is represented in the span of features $x_{1} and x_{2}$\n",
    "\n",
    "<img src=\"images/07_representerTheorem3.png\" width=300 height=300>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "- _Full derivation is in lecture notes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b59f81",
   "metadata": {},
   "source": [
    "- Lets assume $w = \\sum\\limits_{i=1}^{m}\\alpha_{i}y^{(i)}x^{(i)}$\n",
    "\n",
    "- The optimization problem is\n",
    "> $\\min\\limits_{w,b} \\frac{1}{2}\\Vert w \\Vert^{2}$  \n",
    "> s.t. $ y^{(i)}(w^{T}x^{(i)} + b) \\ge 1$ for i=1,..m\n",
    "\n",
    "- As $\\Vert w \\Vert^{2} = w^{T}w$\n",
    "- Substituting w in equation above, the optimization objective becomes\n",
    "> $\\min\\limits_{w,b} \\frac{1}{2}\\Vert w \\Vert^{2}$  \n",
    "> = $\\min\\limits_{w,b} \\frac{1}{2} (\\sum\\limits_{i=1}^{m}\\alpha_{i}y^{(i)}x^{(i)})^{T} (\\sum\\limits_{j=1}^{m}\\alpha_{j}y^{(j)}x^{(j)})$  \n",
    "> = $\\min\\limits_{w,b} \\frac{1}{2} \\sum\\limits_{i=1}^{m} \\sum\\limits_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)} {x^{(i)}}^{T} x^{(j)}$  \n",
    "> = $\\min\\limits_{w,b} \\frac{1}{2} \\sum\\limits_{i=1}^{m} \\sum\\limits_{j=1}^{m} \\alpha_{i} \\alpha_{j} y^{(i)} y^{(j)} \\langle{x^{(i)}}, x^{(j)} \\rangle$\n",
    "\n",
    "- The inner product of $\\langle x^{(i)}, x^{(j)} \\rangle  = {x^{(i)}}^{T} x^{(j)}$\n",
    "  - this is the key step towards deriving kernels\n",
    "  \n",
    "- Substituting w in equation above, the constraint becomes\n",
    "> $ y^{(i)}(w^{T}x^{(i)} + b) \\ge 1$  \n",
    "> = $ y^{(i)}((\\sum\\limits_{j=1}^{m}\\alpha_{j}y^{(j)}x^{(j)})^{T} x^{(i)} + b) \\ge 1$  \n",
    "> = $ y^{(i)}(\\sum\\limits_{j=1}^{m}\\alpha_{j}y^{(j)} \\langle x^{(j)}, x^{(i)} \\rangle + b) \\ge 1$  \n",
    "\n",
    "- From the equations above, we see that the feature vectors appear in this inner product only. So the key is if we can compute this inner product very efficiently, we will get good results with infinite dimensional feature vector manipulation.\n",
    "- By using this inner product, we wont need to loop over infinite dimensional elements in an array\n",
    "\n",
    "- These optimization algorithm is now written in terms of $\\alpha$. So now we need to optimize $\\alpha$\n",
    "\n",
    "- SVM optimization problem (as above) can be further simplified to \"dual optimization problem\" using convex optimization theory:\n",
    "> $max_{\\alpha} w(\\alpha) = \\sum\\limits_{i=1}^{n}\\alpha_{i} - \\frac{1}{2} \\sum\\limits_{i,j=1}^{n} y^{(i)}y^{(j)} \\alpha_{i} \\alpha_{j} \\langle x^{(i)}, x^{(j)} \\rangle$  \n",
    "> $\\space$ s.t. $\\alpha_{i} \\ge 0, i=1,..n$  \n",
    "> $\\space$ and $\\sum\\limits_{i=1}^{n} \\alpha_{i}y^{(i)} = 0$  \n",
    "\n",
    "- The way we make prediction is\n",
    "  - Solve for $\\alpha_{i}'s$ and\n",
    "  - to make prediction, we need to compute\n",
    "  > $h_{w,b}(x) = g(w^{T}x + b)$  \n",
    "  > = $g((\\sum\\limits_{i=1}^{m}\\alpha_{i}y^{(i)}x^{(i)})^{T}x + b)$  \n",
    "  > = $g(\\sum\\limits_{i=1}^{m}\\alpha_{i}y^{(i)} \\langle x^{(i)}, x \\rangle + b)$\n",
    "\n",
    "- We see that the entire algorithm both the optimization objective that we need to deal with during training and how we make predictions, is expressed only in terms of inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d85a95",
   "metadata": {},
   "source": [
    "### Kernel trick\n",
    "\n",
    "1) Write your algorithm in terms instead of $\\langle x^{(i)}, x^{(j)}\\rangle$ as $\\langle x, z\\rangle$  \n",
    "2) Let there be some mapping from $x \\rightarrow \\phi(x)$ from $\\mathbb R ^{1}\\text{ or }\\mathbb R ^{2} $ to $\\mathbb R ^{\\infty} $  \n",
    "3) Find way to compute $K(x,z) = \\phi(x)^{T}\\phi(z)$, where K is kernel function, which we can use to compute the inner product  \n",
    "4) Replace $\\langle x, z\\rangle$ with $K(x,z)$, because by doing this we are swapping out x for $\\phi(x)$, which is computationally expensive. __Because we have written the whole algorithm just in terms of inner products, we dont need to explicitly compute $\\phi(x)$, we can simply compute these kernels K__\n",
    "\n",
    "<img src=\"images/07_kernelTrick.png\" width=600 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640f0b1",
   "metadata": {},
   "source": [
    "- Say X is 3 dimensional feature vector $\\mathbb R^{n}$ and we write $\\phi(x)$ with combination of all the features with duplicates, which will make it of $\\mathbb R^{n^{2}}$ dimensions. Similar will be $\\phi(z)$. So we have gone from 1000 features to 1 million features.\n",
    "- The computation time for this is $O(n^{2})$\n",
    "\n",
    "<img src=\"images/07_kernelFeatureVector.png\" width=600 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b66d4",
   "metadata": {},
   "source": [
    "- A better way would be if we can prove: $K(x,z) = \\phi(x)^{T}\\phi(z)$ is equal to $(x^{T}z)^{2}$\n",
    "- both x and z are $\\mathbb R^{n}$\n",
    "- so the computation time for this will be $\\mathbb O(n)$\n",
    "\n",
    "> $(x^{T}z)^{2}$  \n",
    "> = $(\\sum\\limits_{i=1}^{n}x_{i}z_{i})(\\sum\\limits_{j=1}^{n}x_{j}z_{j})$  \n",
    "> = $\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} x_{i}z_{i}x_{j}z_{j}$  \n",
    "> = $\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} (x_{i}x_{j}) (z_{i}z_{j})$  \n",
    "> = $\\phi(x)^{T}\\phi(z)$\n",
    "<img src=\"images/07_kernelFeatureVector2.png\" width=400 height=400>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "- So instead of needing to manipulate $n^{2}$ dimensional vectors, we now need to compute only $n$ element dimension vector\n",
    "\n",
    "<br>\n",
    "\n",
    "- To fit in the constant part into the kernel, we get\n",
    "> $K(x,z) = (x^{T}z + c)^{2}$  \n",
    "- so in the feature vector $\\phi(x)$ we add constant element which becomes\n",
    "\n",
    "<img src=\"images/07_kernelFeatureVector3.png\" width=200 height=200>  \n",
    "$\\tiny{\\text{YouTube-Stanford-CS229-Andrew Ng}}$  \n",
    "\n",
    "<br>\n",
    "\n",
    "- The computation time for a kernel function will still be $O(n)$ \n",
    "> $K(x,z) = (x^{T}z + c)^{d}$  \n",
    "\n",
    "- $\\phi(x)$ has all $\\binom{n+d}{d}$ feature of monomial up to order d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e1f4e",
   "metadata": {},
   "source": [
    "- So SVM uses the optimal margin classifier we derived earlier and applies kernel trick to it\n",
    "- So even with this infinite dimensional feature space, the computation time scales only linearly with the order of n, as the number of input feature dimension x rather than as a function of infinite dimensional feature space. \n",
    "\n",
    "- Why is this a good idea?\n",
    "  - We took the training set, \n",
    "  - mapped it to much higher dimensional feature space, \n",
    "  - then found a linear decision boundary (hyperplane) in that higher dimensional space, \n",
    "  - and then when we look in original feature space, \n",
    "  - where we found it to have a very non-linear decision boundary.\n",
    "  - __The non-linear decision boundary we see in lower dimensional space is a linear decision boundary in higher dimensional space.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4d1e3",
   "metadata": {},
   "source": [
    "### How to make kernels\n",
    "- If x, z are \"similar\", then K(x,z) = \\phi(x)\\phi(z) is \"large\"\n",
    "- If x, z are \"dissimilar\", then K(x,z) = \\phi(x)\\phi(z) is \"small\"\n",
    "  - this is because the inner product of two similar vectors should be large and the inner product of two dissimilar vectors should be small\n",
    "  \n",
    "- If we consider another function with the same property as above, \n",
    "> $K(x,z) = exp\\left(-\\frac{\\Vert x-z \\Vert}{2\\sigma^{2}}\\right)$\n",
    "- So in this function, if x and z are close to each other, the difference is close to 0 and so exp(0) is close to 1\n",
    "  - can we use this function as a kernel function\n",
    "\n",
    "<br>\n",
    "\n",
    "- Does there exist $\\phi$ s.t. $ K(x,z) = \\phi(x)^{T}\\phi(z) $\n",
    "- Applying constraints on this kernel function\n",
    "  - it must satisfy $ K(x,x) = \\phi(x)^{T}\\phi(x) \\ge 0 $, as the inner product must be non-negative, otherwise its not a valid kernel function\n",
    "  - More generally, a proof that outlines when is something a valid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00ba5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4406cb17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e60b40b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458415ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc5514c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b56f31c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed967e6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
